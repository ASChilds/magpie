Instructions For Spark
----------------------

0) If necessary, download your favorite version of Spark off of Apache
   and install it into a location where it's accessible on all cluster
   nodes.  Usually this is on a NFS home directory.

   On older versions of Spark, you may need to set
   SPARK_HADOOP_VERSION and run 'sbt/sbt assembly' to prepare Spark
   for execution.  If you are not using the default Java
   implementation installed in your system, you may need to edit
   sbt/sbt to use the proper Java version you desire (this is the case
   with 0.9.1, not the case in future versions).

   See below in 'Spark Patching' about patches that may be necessary
   for Spark depending on your environment and Spark version.

   See 'Convenience Scripts' in README about
   misc/magpie-apache-download-and-setup.sh, which may make the
   downloading and patching easier.

1) Select an appropriate submission script for running your job.  You
   can find them in the directory submission-scripts/, with Slurm
   Sbatch scripts using srun in script-sbatch-srun, Moab Msub+Slurm
   scripts using srun in script-msub-slurm-srun, Moab Msub+Torque
   scripts using pdsh in script-msub-torque-pdsh, and LSF scripts
   using mpirun in script-lsf-mpirun.

   You'll likely want to start with the base spark script
   (e.g. magpie.sbatch-srun-spark) or spark w/ hdfs
   (e.g. magpie.sbatch-srun-spark-with-hdfs) for your
   scheduler/resource manager.  If you wish to configure more, you can
   choose to start with the base script (e.g. magpie.sbatch-srun)
   which contains all configuration options.

   It should be noted that you can run Spark without HDFS.  You can
   access files normally through "file://<path>".

2) Setup your job essentials at the top of the submission script.  As
   an example, the following are the essentials for running with Moab.

   #MSUB -l nodes : Set how many nodes you want in your job

   #MSUB -l walltime : Set the time for this job to run

   #MSUB -l partition : Set the job partition

   #MSUB -q <my batch queue> : Set to batch queue

   MOAB_JOBNAME : Set your job name.

   MAGPIE_SCRIPTS_HOME : Set where your scripts are

   MAGPIE_LOCAL_DIR : For scratch space files

   MAGPIE_JOB_TYPE : This should be set to 'spark'

   JAVA_HOME : B/c you need to ...

3) Setup the essentials for Spark.

   SPARK_SETUP : Set to yes

   SPARK_VERSION : Set appropriately.

   SPARK_HOME : Where your Spark code is.  Typically in an NFS
   mount.

   SPARK_LOCAL_DIR : A small place for conf files and log files local
   to each node.  Typically /tmp directory.  

   SPARK_LOCAL_SCRATCH_DIR : A scratch directory for Spark to use.  If
   a local SSD is available, it would be preferable to set this to
   that path and set SPARK_LOCAL_SCRATCH_DIR_TYPE to "local".

   SPARK_LOCAL_SCRATCH_DIR_TYPE : Indicates if SPARK_LOCAL_SCRATCH_DIR
   if a network file path or local store path.

4) Select how your job will run by setting SPARK_MODE.  The first time
   you'll probably want to run w/ 'sparkpi' mode just to try things
   out and make things look setup correctly.

   After this, you may want to run with 'interactive' mode to play
   around and figure things out.  In the job output you will see
   output similar to the following:

      ssh node70
      setenv SPARK_CONF_DIR "/tmp/username/spark/ajobname/1081559/conf"
      cd /home/username/spark-1.2.0-bin-hadoop2.4

   These instructions will inform you how to login to the master node
   of your allocation and how to initialize your session.  Once in
   your session.  You can do as you please.  For example, you can run
   a job using spark-class (bin/spark-class ...).  There will also be
   instructions in your job output on how to tear the session down
   cleanly if you wish to end your job early.

   Once you have figured out how you wish to run your job, you will
   likely want to run with 'script' mode.  Create a script that will
   run your job/calculation automatically, set it in
   SPARK_SCRIPT_PATH, and then run your job.  You can find an example
   job script in examples/spark-example-job-script.

   See "Exported Environment Variables" in README for information on
   common exported environment variables that may be useful in
   scripts.

   See below in "Spark Exported Environment Variables", for
   information on Spark specific exported environment variables that
   may be useful in scripts.

5) Spark does not require HDFS, but many choose to use it.  If you do,
   setup Hadoop w/ HDFS in your submission script.  See README.hadoop
   for Hadoop setup instructions.  Simply use the prefix "hdfs://" or
   "file://" appropriately for the filesystem you will access files
   from.

   You may wish to run with SPARK_MODE set to 'sparkwordcount' to test
   the HDFS setup.

6) Submit your job into the cluster by running "sbatch -k
   ./magpie.sbatchfile" for Slurm, "msub ./magpie.msubfile" for
   Moab, or "bsub < ./magpie.lsffile" for LSF.  
   Add any other options you see fit.

7) Look at your job output file to see your output.  There will also
   be some notes/instructions/tips in the output file for viewing the
   status of your job in a web browser, environment variables you wish
   to set if interacting with it, etc.

   See "General Advanced Usage" in README for additional tips.

   See below in "Spark Advanced Usage" for additional Spark tips.

Spark Exported Environment Variables
------------------------------------

The following environment variables are exported when your job is run
and may be useful in scripts in your run or in pre/post run scripts.

SPARK_MASTER_NODE : the master node of the Spark allocation.  Often
		    used for launching Spark jobs
		    (e.g. spark://${SPARK_MASTER_NODE}:${SPARK_MASTER_PORT})

SPARK_MASTER_PORT : the master port for running Spark jobs.  Often
		    used for launching Spark jobs
		    (e.g. spark://${SPARK_MASTER_NODE}:${SPARK_MASTER_PORT})

SPARK_SLAVE_COUNT : number of compute/data nodes in your allocation
                    for Spark.  May be useful for adjusting run time
                    options such as reducer count.

SPARK_SLAVE_CORE_COUNT : Total cores on slave nodes in the allocation.
       		         May be useful for adjusting run time options
       		         such as reducer count.

SPARK_CONF_DIR : the directory that Spark configuration files local
                 to the node are stored.

SPARK_LOG_DIR : the directory Spark log files are stored

See "Hadoop Exported Environment Variables" in README.hadoop, for
Hadoop environment variables that may be useful.

Example Job Output for Spark running SparkPi
--------------------------------------------

The following is an example job output of Magpie running Spark and
running SparkPi.  This is run over HDFS over Lustre.  Sections of
extraneous text have been left out.

While this output is specific to using Magpie with Spark, the output
when using Hadoop, Storm, Hbase, etc. is not all that different.

1) First we get some details of the job

*******************************************************
* Magpie General Job Info
*
* Job Nodelist: apex[69-77]
* Job Nodecount: 9
* Job Timelimit in Minutes: 60
* Job Name: sparkpitest
* Job ID: 1081573
*
*******************************************************

2) Next, Spark begins to launch and startup daemons on all cluster nodes.

Starting spark
starting org.apache.spark.deploy.master.Master, logging to /tmp/achu/spark/test/1081573/log/spark-achu-org.apache.spark.deploy.master.Master-1-apex69.out
apex71: starting org.apache.spark.deploy.worker.Worker, logging to /tmp/achu/spark/test/1081573/log/spark-achu-org.apache.spark.deploy.worker.Worker-1-apex71.out
apex72: starting org.apache.spark.deploy.worker.Worker, logging to /tmp/achu/spark/test/1081573/log/spark-achu-org.apache.spark.deploy.worker.Worker-1-apex72.out
apex77: starting org.apache.spark.deploy.worker.Worker, logging to /tmp/achu/spark/test/1081573/log/spark-achu-org.apache.spark.deploy.worker.Worker-1-apex77.out
apex76: starting org.apache.spark.deploy.worker.Worker, logging to /tmp/achu/spark/test/1081573/log/spark-achu-org.apache.spark.deploy.worker.Worker-1-apex76.out
apex74: starting org.apache.spark.deploy.worker.Worker, logging to /tmp/achu/spark/test/1081573/log/spark-achu-org.apache.spark.deploy.worker.Worker-1-apex74.out
apex75: starting org.apache.spark.deploy.worker.Worker, logging to /tmp/achu/spark/test/1081573/log/spark-achu-org.apache.spark.deploy.worker.Worker-1-apex75.out
apex73: starting org.apache.spark.deploy.worker.Worker, logging to /tmp/achu/spark/test/1081573/log/spark-achu-org.apache.spark.deploy.worker.Worker-1-apex73.out
apex70: starting org.apache.spark.deploy.worker.Worker, logging to /tmp/achu/spark/test/1081573/log/spark-achu-org.apache.spark.deploy.worker.Worker-1-apex70.out
Waiting 30 seconds to allow Spark daemons to setup

3) Next, we see output with details of the Spark setup.  You'll find
   addresses indicating web services you can access to get detailed
   job information.  You'll also find information about how to login
   to access Spark directly and how to shut down the job early if you
   so desire.

*******************************************************
*
* Spark Information
*
* You can view your Spark status by launching a web browser and pointing to ...
*
* Spark Master: http://apex69:8080
* Spark Worker: http://<WORKERNODE>:8081
* Spark Application Dashboard: http://apex69:4040
*
* The Spark Master for running jobs is
*
* spark://apex69:7077
*
* To access Spark directly, you'll want to:
*   ssh apex69
*   setenv SPARK_CONF_DIR "/tmp/achu/spark/test/1081573/conf"
*   cd /home/achu/hadoop/spark-1.2.0-bin-hadoop2.4
*
* Then you can do as you please.  For example to run a job:
*
*   bin/spark-class <class> spark://apex69:7077
*
* To end/cleanup your session, kill the daemons via:
*
*   ssh apex69
*   setenv SPARK_CONF_DIR "/tmp/achu/spark/test/1081573/conf"
*   cd /home/achu/hadoop/spark-1.2.0-bin-hadoop2.4
*   sbin/stop-all.sh
*
* Some additional environment variables you may sometimes wish to set
*
*   setenv JAVA_HOME "/usr/lib/jvm/jre-1.6.0-sun.x86_64/"
*   setenv SPARK_HOME "/home/achu/hadoop/spark-1.2.0-bin-hadoop2.4"
*
*******************************************************

4) Then the SparkPi job is run

Running bin/run-example org.apache.spark.examples.SparkPi 8
15/02/20 10:33:23 INFO SecurityManager: Changing view acls to: achu
15/02/20 10:33:23 INFO SecurityManager: Changing modify acls to: achu
15/02/20 10:33:23 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(achu); users with modify permissions: Set(achu)
15/02/20 10:33:23 INFO Slf4jLogger: Slf4jLogger started
15/02/20 10:33:24 INFO Remoting: Starting remoting
15/02/20 10:33:24 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@apex69.llnl.gov:59131]
15/02/20 10:33:24 INFO Utils: Successfully started service 'sparkDriver' on port 59131.
15/02/20 10:33:24 INFO SparkEnv: Registering MapOutputTracker
15/02/20 10:33:24 INFO SparkEnv: Registering BlockManagerMaster
15/02/20 10:33:24 INFO DiskBlockManager: Created local directory at /p/lscratchg/achu/sparkscratch/node-0/spark-local-20150220103324-88d6
15/02/20 10:33:24 INFO MemoryStore: MemoryStore started with capacity 25.9 GB
15/02/20 10:33:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
15/02/20 10:33:25 INFO HttpFileServer: HTTP File server directory is /tmp/spark-b62dfb05-a931-4c9f-bbde-f420b605e4e4
15/02/20 10:33:25 INFO HttpServer: Starting HTTP Server
15/02/20 10:33:25 INFO Utils: Successfully started service 'HTTP file server' on port 43436.
15/02/20 10:33:25 INFO Utils: Successfully started service 'SparkUI' on port 4040.
15/02/20 10:33:25 INFO SparkUI: Started SparkUI at http://apex69.llnl.gov:4040
15/02/20 10:33:26 INFO SparkContext: Added JAR file:/home/achu/hadoop/spark-1.2.0-bin-hadoop2.4/lib/spark-examples-1.2.0-hadoop2.4.0.jar at http://192.168.123.69:43436/jars/spark-examples-1.2.0-hadoop2.4.0.jar with timestamp 1424457206218
15/02/20 10:33:26 INFO AppClient$ClientActor: Connecting to master spark://apex69:7077...
15/02/20 10:33:26 INFO SparkDeploySchedulerBackend: Connected to Spark cluster with app ID app-20150220103326-0000
15/02/20 10:33:26 INFO AppClient$ClientActor: Executor added: app-20150220103326-0000/0 on worker-20150220103252-apex73.llnl.gov-37145 (apex73.llnl.gov:37145) with 16 cores
15/02/20 10:33:26 INFO SparkDeploySchedulerBackend: Granted executor ID app-20150220103326-0000/0 on hostPort apex73.llnl.gov:37145 with 16 cores, 50.0 GB RAM
15/02/20 10:33:26 INFO AppClient$ClientActor: Executor added: app-20150220103326-0000/1 on worker-20150220103252-apex71.llnl.gov-33291 (apex71.llnl.gov:33291) with 16 cores
15/02/20 10:33:26 INFO SparkDeploySchedulerBackend: Granted executor ID app-20150220103326-0000/1 on hostPort apex71.llnl.gov:33291 with 16 cores, 50.0 GB RAM
15/02/20 10:33:26 INFO AppClient$ClientActor: Executor added: app-20150220103326-0000/2 on worker-20150220103252-apex75.llnl.gov-33121 (apex75.llnl.gov:33121) with 16 cores
15/02/20 10:33:26 INFO SparkDeploySchedulerBackend: Granted executor ID app-20150220103326-0000/2 on hostPort apex75.llnl.gov:33121 with 16 cores, 50.0 GB RAM
15/02/20 10:33:26 INFO AppClient$ClientActor: Executor added: app-20150220103326-0000/3 on worker-20150220103252-apex74.llnl.gov-60926 (apex74.llnl.gov:60926) with 16 cores
15/02/20 10:33:26 INFO SparkDeploySchedulerBackend: Granted executor ID app-20150220103326-0000/3 on hostPort apex74.llnl.gov:60926 with 16 cores, 50.0 GB RAM
15/02/20 10:33:26 INFO AppClient$ClientActor: Executor added: app-20150220103326-0000/4 on worker-20150220103252-apex77.llnl.gov-49445 (apex77.llnl.gov:49445) with 16 cores
15/02/20 10:33:26 INFO SparkDeploySchedulerBackend: Granted executor ID app-20150220103326-0000/4 on hostPort apex77.llnl.gov:49445 with 16 cores, 50.0 GB RAM
15/02/20 10:33:26 INFO AppClient$ClientActor: Executor added: app-20150220103326-0000/5 on worker-20150220103252-apex76.llnl.gov-52178 (apex76.llnl.gov:52178) with 16 cores
15/02/20 10:33:26 INFO SparkDeploySchedulerBackend: Granted executor ID app-20150220103326-0000/5 on hostPort apex76.llnl.gov:52178 with 16 cores, 50.0 GB RAM
15/02/20 10:33:26 INFO AppClient$ClientActor: Executor added: app-20150220103326-0000/6 on worker-20150220103252-apex70.llnl.gov-53289 (apex70.llnl.gov:53289) with 16 cores
15/02/20 10:33:26 INFO SparkDeploySchedulerBackend: Granted executor ID app-20150220103326-0000/6 on hostPort apex70.llnl.gov:53289 with 16 cores, 50.0 GB RAM
15/02/20 10:33:26 INFO AppClient$ClientActor: Executor added: app-20150220103326-0000/7 on worker-20150220103252-apex72.llnl.gov-51360 (apex72.llnl.gov:51360) with 16 cores
15/02/20 10:33:26 INFO SparkDeploySchedulerBackend: Granted executor ID app-20150220103326-0000/7 on hostPort apex72.llnl.gov:51360 with 16 cores, 50.0 GB RAM
15/02/20 10:33:26 INFO AppClient$ClientActor: Executor updated: app-20150220103326-0000/0 is now RUNNING
15/02/20 10:33:26 INFO AppClient$ClientActor: Executor updated: app-20150220103326-0000/1 is now RUNNING
15/02/20 10:33:26 INFO AppClient$ClientActor: Executor updated: app-20150220103326-0000/2 is now RUNNING
15/02/20 10:33:26 INFO AppClient$ClientActor: Executor updated: app-20150220103326-0000/3 is now RUNNING
15/02/20 10:33:26 INFO AppClient$ClientActor: Executor updated: app-20150220103326-0000/4 is now RUNNING
15/02/20 10:33:26 INFO AppClient$ClientActor: Executor updated: app-20150220103326-0000/5 is now RUNNING
15/02/20 10:33:26 INFO AppClient$ClientActor: Executor updated: app-20150220103326-0000/6 is now RUNNING
15/02/20 10:33:26 INFO AppClient$ClientActor: Executor updated: app-20150220103326-0000/7 is now RUNNING
15/02/20 10:33:26 INFO AppClient$ClientActor: Executor updated: app-20150220103326-0000/0 is now LOADING
15/02/20 10:33:26 INFO AppClient$ClientActor: Executor updated: app-20150220103326-0000/6 is now LOADING
15/02/20 10:33:26 INFO AppClient$ClientActor: Executor updated: app-20150220103326-0000/1 is now LOADING
15/02/20 10:33:26 INFO AppClient$ClientActor: Executor updated: app-20150220103326-0000/5 is now LOADING
15/02/20 10:33:26 INFO AppClient$ClientActor: Executor updated: app-20150220103326-0000/2 is now LOADING
15/02/20 10:33:26 INFO AppClient$ClientActor: Executor updated: app-20150220103326-0000/4 is now LOADING
15/02/20 10:33:26 INFO AppClient$ClientActor: Executor updated: app-20150220103326-0000/7 is now LOADING
15/02/20 10:33:26 INFO AppClient$ClientActor: Executor updated: app-20150220103326-0000/3 is now LOADING
<snip>
<snip>
<snip>
15/02/20 10:33:30 INFO TaskSetManager: Finished task 5.0 in stage 0.0 (TID 5) in 2001 ms on apex70.llnl.gov (1/8)
15/02/20 10:33:30 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 2005 ms on apex70.llnl.gov (2/8)
15/02/20 10:33:30 INFO TaskSetManager: Finished task 6.0 in stage 0.0 (TID 6) in 2003 ms on apex70.llnl.gov (3/8)
15/02/20 10:33:30 INFO TaskSetManager: Finished task 7.0 in stage 0.0 (TID 7) in 2150 ms on apex70.llnl.gov (4/8)
15/02/20 10:33:30 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2167 ms on apex70.llnl.gov (5/8)
15/02/20 10:33:30 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 2165 ms on apex70.llnl.gov (6/8)
15/02/20 10:33:30 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 2166 ms on apex70.llnl.gov (7/8)
15/02/20 10:33:30 INFO TaskSetManager: Finished task 4.0 in stage 0.0 (TID 4) in 2167 ms on apex70.llnl.gov (8/8)
15/02/20 10:33:30 INFO DAGScheduler: Stage 0 (reduce at SparkPi.scala:35) finished in 3.377 s
15/02/20 10:33:30 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
15/02/20 10:33:30 INFO DAGScheduler: Job 0 finished: reduce at SparkPi.scala:35, took 3.583061 s
Pi is roughly 3.142925
15/02/20 10:33:30 INFO SparkUI: Stopped Spark web UI at http://apex69.llnl.gov:4040
15/02/20 10:33:30 INFO DAGScheduler: Stopping DAGScheduler
15/02/20 10:33:30 INFO SparkDeploySchedulerBackend: Shutting down all executors
15/02/20 10:33:30 INFO SparkDeploySchedulerBackend: Asking each executor to shut down
15/02/20 10:33:31 INFO MapOutputTrackerMasterActor: MapOutputTrackerActor stopped!
15/02/20 10:33:31 INFO MemoryStore: MemoryStore cleared
15/02/20 10:33:31 INFO BlockManager: BlockManager stopped
15/02/20 10:33:31 INFO BlockManagerMaster: BlockManagerMaster stopped
15/02/20 10:33:31 INFO SparkContext: Successfully stopped SparkContext
15/02/20 10:33:31 INFO RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.
15/02/20 10:33:31 INFO RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.

The Pi approximation is 3.142925.

5) With the job complete, Magpie now tears down the session and cleans
   up all daemons.

Stopping spark
apex74: stopping org.apache.spark.deploy.worker.Worker
apex77: stopping org.apache.spark.deploy.worker.Worker
apex75: stopping org.apache.spark.deploy.worker.Worker
apex73: stopping org.apache.spark.deploy.worker.Worker
apex72: stopping org.apache.spark.deploy.worker.Worker
apex70: stopping org.apache.spark.deploy.worker.Worker
apex71: stopping org.apache.spark.deploy.worker.Worker
apex76: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master

Spark Patching
--------------
- Patch to support alternate config file directories is required.

  For versions < 1.0.0, patch can be applied directly to startup
  scripts, not needing a recompilation of source.

  For atleast Spark version 1.0.0 (possibly versions newer than it),
  patch for startup scripts and code is required, a recompilation of
  source is required.

  For Spark version 1.2.0 and newer, a patch for startup scripts is
  required, but recompilation of source is not required.

  Patches for this can be found in the patches/spark/ directory with
  'alternate' in the filename.

- Patch to support non-ssh remote execution may be needed in some
  environments.  Patch can be applied directly to startup scripts, not
  needing a recompilation of source.

  Patches for this can be found in the patches/spark/ directory with
  'alternate-ssh' in the filename.

  The alternate remote execution command must be specified in the
  environment variable MAGPIE_REMOTE_CMD.

- If MAGPIE_NO_LOCAL_DIR support is desired, patches in
  patches/spark/ with 'no-local-dir' in the filename can be found for
  support.  See README.no-local-dir for more details.

Spark Testing
-------------

Spark was added/tested against spark-0.9.1 and spark-1.0.0.  Nominal
testing was done with Spark 0.9.2, 1.2.0, 1.2.1, 1.2.2, 1.3.0. 1.3.1,
1.4.0, 1.4.1, 1.5.0, 1.5.1, 1.5.2, and 1.6.0.

Spark Advanced Usage
---------------------

1) If your cluster has a local SSD on each node, set a path to it via
   the SPARK_LOCAL_SCRATCH_DIR environment variable in your submission
   scripts.  In addition set SPARK_LOCAL_SCRATCH_DIR_TYPE
   appropriately to indicate it is local.  

   Setting this SSD serves two purposes.  One, this local scratch
   directory can be used for spillover from map outputs to aid in
   shuffles.  This local scratch directory can greatly improve shuffle
   performance.

   Second, it can be used for quickly storing/caching RDDs to disk
   using MEMORY_AND_DISK and/or DISK_ONLY persistence levels.

2) Magpie configures the default number of reducers (or
   partitions/parallelism) in a Spark job to the number of compute
   nodes in your allocation.  This is significantly superior to the
   original Spark default of 8 (pre-1.0) and likely superior than the
   current default of all partitions in your data set (post-1.0).
   However, it may not be optimal for many jobs.

   Users should play around with the parallelism in their job to
   improve performance.  The default can be tweaked in the submission
   scripts via the SPARK_DEFAULT_PARALLELISM environment variable.

3) Magpie configures a relatively conservative amount of memory for
   Spark, currently 80% of system memory.  While there should always
   be a buffer to allow the operating system, system daemons, and
   Spark (and potentially Hadoop HDFS) daemons to operate, the 80%
   value may be on the conservative side and users wishing to push it
   higher to 90% or 95% of system memory may see benefits..

   Users can adjust the amount of memory used by each Spark Worker
   through the SPARK_WORKER_MEMORY_PER_NODE environment variable in
   the submission scripts.

4) There are two major memory fraction configuration variables
   SPARK_STORAGE_MEMORY_FRACTION and SPARK_SHUFFLE_MEMORY_FRACTION
   which may have major effects on performance depending on your job.
   
   SPARK_STORAGE_MEMORY_FRACTION controls the percentage of memory
   used for the memory cache while SPARK_SHUFFLE_MEMORY_FRACTION
   controls the percentage used for shuffles.

   You may wish to adjust these for your specific job, as they can
   have a large influence on job performance.  Please see submission
   scripts for more information.

   Note that beginning Spark 1.6.0 memory fractions have been
   deprecated.