Running Big Data Software on Traditional HPC Clusters

Albert Chu
Updated April 8th, 2014
chu11@llnl.gov

What is this project
--------------------

This project contains a number of scripts for running Big Data
software in HPC environments.  Thus far, Hadoop, Hbase, Pig, Spark,
and Zookeeper are supported.  It currently supports running over the
schedulers/resource managers of Slurm and Moab.  It currently supports
running over the parallel file system Lustre and running over any
generic network filesytem.

Some of the features presently supported:

- Run Hadoop, Hbase, or Pig interactively or via scripts.
- Run Mapreduce 1.0 or 2.0 jobs via Hadoop 1.0 or 2.0
- Run against a number of filesystem options, such as HDFS, HDFS over
  Lustre, HDFS over a generic network filesystem, Lustre directly, or
  a generic network filesystem.
- Take advantage of SSDs for local caching if available
- Run the UDA Infiniband optimization plugin for Hadoop.
- Make decent optimizations of Hadoop for your hardware

Basic Idea
----------

The basic idea behind these scripts are to:

1) Allocate nodes on a cluster using your HPC scheduler/resource
   manager.  Slurm and Moab are currently supported.

2) Scripts will setup configuration files so the rank 0 node is
   the "master".  All compute nodes will have configuration files
   created that point to the node designated as the master server.

   The configuration files will be populated with values for your
   filesystem choice and the hardware that exists in your cluster.
   Reasonable attempts are made to determine optimal values (they are
   almost certainly better than the default values).

3) Launch daemons on all nodes.  The rank 0 node will run master
   daemons, such as the Hadoop Namenode or the Hbase Master.  All
   remaining nodes will run appropriate slave daemons, such as the
   Hadoop Datanodes or Hbase RegionServers.

4) Now you have a mini cluster to do whatever you want.

Basic Instructions For Running Hadoop
-------------------------------------

1) Download your favorite version of Hadoop off of Apache and install
   it into a location where it's accessible on all cluster nodes.
   Usually this is on a NFS home directory.

   See below about patches that may be necessary for Hadoop depending
   on your environment and Hadoop version.

2) Select an appropriate job submission script for running your job.
   You can find Slurm Sbatch scripts in the directory script-sbatch or
   Moab Msub scripts in the directory script-msub-slurm.

   You'll likely want to start with the magpie.sbatch-hadoop or
   magpie.msub-hadoop submission script.  If you wish to configure
   more, you can choose to start with the magpie.sbatch or magpie.msub
   file which contains all configuration options.

3) Setup your job essentials at the top of the script.  As an example,
   the following are the essentials for running with Slurm w/ Sbatch.

   #SBATCH --nodes : Set how many nodes you want in your job

   SBATCH_TIMELIMIT : Set the time for this job to run

   #SBATCH --partition : Set the job partition

   MAGPIE_SCRIPTS_HOME : Set where your scripts are.

   MAGPIE_LOCAL_DIR : For scratch space files

   MAGPIE_JOB_TYPE : Set to "hadoop" most likely.

   JAVA_HOME : B/c you need to ...

4) Now setup the essentials for Hadoop.

   HADOOP_SETUP : Set to yes

   HADOOP_SETUP_TYPE : Are you running Mapreduce version 1 or 2

   HADOOP_VERSION : Make sure your build matches HADOOP_SETUP_TYPE
   (i.e. don't say you want MapReduce 1 and point to Hadoop 2.0 build)

   HADOOP_HOME : Where your hadoop code is.  Typically in an NFS mount.

   HADOOP_LOCAL_DIR : A small place for conf files and log files local
   to each node.  Typically /tmp directory.

   HADOOP_MODE : The first time you'll probably want to run w/
   'terasort' mode just to try things out.  Later you may want to run
   w/ 'script' or 'interactive' mode, as that is the more likely way
   to run.

   HADOOP_FILESYSTEM_MODE : Most will likely want "hdfsoverlustre" or
   "hdfsovernetworkfs".  See below for details on HDFS over Lustre and
   HDFS over NetworkFS.

   HADOOP_HDFSOVERLUSTRE_PATH or equivalent: For HDFS over Lustre, you
   need to set this.  If not using HDFS over Lustre, set the
   appropriate path for your filesystem mode choice.

5) If you are happy with the configuration files provided by this
   project, you can use them.  If not, change them.  If you copy them
   to a new directory, adjust HADOOP_CONF_FILES in your submission
   script as needed.

6) Run "sbatch -k ./magpie.sbatchfile" or "msub ./magpie.msubfile"
   ... and any other options you see fit.

7) Look at your job output file to see your output.  There will also
   be some notes/instructions/tips in the output file for viewing the
   status of your job, how to interact, etc..

Basic Instructions For Running Hadoop w/ UDA
--------------------------------------------

UDA is an Infiniband plugin to alter the suffle/sort mechanism in
Hadoop to take advantage of RDMA.  It can be found at:

https://code.google.com/p/uda-plugin/

It is currently supported in Hadoop 2.3.0 and newer.  UDA can be used
with older versions of Hadoop but will require patches against those
builds.  Magpie currently supports UDA in versions 2.2.0 and up.
Minor modifications to the scripts should allow support in older
versions.

To configure UDA to be used with Hadoop, you need to configure:

HADOOP_UDA_SETUP : Set to yes

HADOOP_UDA_JAR : Set to the path for the uda jar file, such as
                 uda-hadoop-2.x.jar for Hadoop 2.2.0.

HADOOP_UDA_LIBPATH : Set to location of libuda.so.  Necessary if your
                     libuda.so isn't in a common location (e.g. /usr/lib).

If you are building UDA yourself, be aware that the jar may have
difficulty finding the libuda.so file.  Some appropriately placed
symlinks in your filesystem should solve that problem.  Please refer
to error logs to see the paths you may be missing.

Remember, that when running your job, you may also need to link to the
UDA jar file to your job run, such as "--libjars /mypath/uda-hadoop-2.x.jar"

Basics of HDFS over Lustre/NetworkFS
------------------------------------

Instead of using local disk, we designate a Lustre/network file system
directory to "emulate" local disk for each compute node.  For example,
lets say you have 4 compute nodes.  If we create the following paths
in Lustre,

/lustre/myusername/node-0
/lustre/myusername/node-1
/lustre/myusername/node-2
/lustre/myusername/node-3

We can give each of these paths to one of the compute nodes, which
they can treat like a local disk.  HDFS operates on top of these
directories just as though there were a local disk on the server.

Q: Does that mean I have to constantly rebuild HDFS everytime I start
  a job?

A: No, using node ranks, "disk-paths" can be consistently assigned to
   nodes so that all your HDFS files from a previous run can exist on
   a later run.  The next time you run your job, it doesn't matter
   what server you're running on, b/c your schedule/resource manager
   will assign that node it's appropriate rank.  The node will
   subsequently load HDFS from its appropriate directory.

Q: But that'll mean I have to consistently use the same number of
   cluster nodes?

A: Generally speaking no, but you can hit issues if you don't.  Just
   imagine how HDFS issues if you were on a traditional Hadoop cluster
   and added or removed nodes.

   Generally speaking, increasing the number of nodes you use for a
   job is fine.  Data you currently have in HDFS is still there and
   readable, but it is not viewed as "local" according to HDFS and
   more network transfers will have to happen.  You may wish to
   rebalance the HDFS blocks though.  The script
   hadoop-rebalance-hdfs-over-lustre-if-increasing-nodes-script.sh can
   be used instead.

   (Special Note: The start-balancer.sh that is
   normally used probably will not work.  All of the paths are in
   Lustre/NetworkFS, so the "free space" on each "local" path is identical,
   messing up calculations for balancing (i.e. no "local disk" is
   more/less utilized than another).  

   Decreasing nodes is a bit more dangerous, as data can "disappear"
   just like if it were on a traditional Hadoop cluster.  If you try
   to scale down the number of nodes, you should go through the
   process of "decomissioning" nodes like on a real cluster, otherwise
   you may lose data.  You can decomission nodes through the
   hadoop-hdfs-over-lustre-nodes-decomission-script.sh script.

Q: What should HDFS replication be?

A: The scripts in this package default to HDFS replication of 3 when
   HDFS over Lustre is done.  If HDFS replication is > 1, it can
   improve performance of your job reading in HDFS input b/c there
   will be fewer network transfer of data (i.e. Hadoop may need to
   transfer "non-local" data to another node).  In addition, if a
   datanode were to die (i.e. a node crashes) Hadoop has the ability
   to survive the crash just like in a traditional Hadoop cluster.

   The trade-off is space and HDFS writes.  With HDFS replication of
   1, you can save space, but replication also increases the time for
   writes.

Pro vs Con of HDFS over Lustre/NetworkFS vs. Posix FS (e.g. rawnetowrkfs, etc.)
-------------------------------------------------------------------------------

Here are some pros vs. cons of using a network filesystem directly vs
HDFS over Lustre/NetworkFS.

HDFS over Lustre/NetworkFS:

Pro: Portability w/ code that runs against a "traditional" Hadoop
cluster.  If it runs on a "traditional" Hadoop cluster w/ local disk,
it should run fine w/ HDFS over Lustre/NetworkFS.

Con: Must always run job w/ Hadoop & HDFS running as a job.

Con: Must "import" and "export" data from HDFS using job runs, cannot
read/write directly.  On some clusters, this may involve a double copy
of data. e.g. first need to copy data into the cluster, then run job to
copy data into HDFS over Lustre/NetworkFS.

Con: Possible difficulty changing job size on clusters.

Con: If HDFS replication > 1, more space used up.

Posix FS directly:

Pro: You can read/write files to Lustre without Hadoop/HDFS running.

Pro: Less space used up.

Pro: Can adjust job size easily.

Con: Portability issues w/ code that usually runs on HDFS.  As an
example, HDFS has no concept of a working directory while Posix
filesystems do.  Code will have to be adjusted for this.

Con: User must "manage" and organize their files directly, not gaining
the block advantages of HDFS.  If not handled well, this can lead to
performance issues.  For example, a Hadoop job that creates a 1
terabyte file under HDFS is creating a file made up of many small HDFS
blocks.  The same job may create a single 1 terabyte file under access
to the Posic FS directly.

Magpie Network File System Option
---------------------------------

As noted above, there are some portability or "gotchas" when using
Rawnetworkfs.  In attempt to get around some of those issues, the
"Magpie Network FS" patch is available as a plugin for Hadoop 2.0.  It
is a file system plugin that is a small layer above the Raw Local
FileSystem.

The patch runs very similarly to rawnetworkfs except that it handles
the following extra cases.

1) It handles the case indicated in

https://issues.apache.org/jira/browse/MAPREDUCE-5528

2) If you specify a relative path, the Magpie Network FS option will
read/write to the base path of HADOOP_MAGPIENETWORKFS_PATH.  The
current working directory also defaults to HADOOP_MAGPIENETWORKFS_PATH
regardless of where you are in the file system tree.

For example, if the user specifies the path "mypath/file", with
rawnetworkfs it would normally be written to the current working
directory the user is running in.  The user would be required to
specify the absolute path to ensure files are written to the networked
file system (e.g. /mynetworkmount/mypath/file).

With this patch, the relative path "mypath/file" would be written to 
${HADOOP_MAGPIENETWORKFS_PATH}/mypath/file.

This can help w/ portability of code/scripts that usually assume HDFS,
which does not have a current working directory.

A patch for Magpie Network FS can be found in the patches directory.

Basic Instructions For Using Pig
--------------------------------

1) Download your favorite version of Pig off of Apache and install it
   into a location where it's accessible on all cluster nodes.
   Usually this is on a NFS home directory.

   Make sure that the version of Pig you install is compatible with
   the version of Hadoop you are using.

   In some cases, a re-compile of Pig may be necessary.  For example,
   by default Pig 0.12.0 works against the 0.20.0 (i.e. Hadoop 1.0)
   branch of Hadoop.  You may need to modify the Pig build.xml to work
   with the 0.23.0 branch (i.e. Hadoop 2.0).

2) Select an appropriate job submission script for running your job.
   You can find Slurm Sbatch scripts in the directory script-sbatch or
   Moab Msub scripts in the directory script-msub-slurm.

   You'll likely want to start with the magpie.sbatch-hadoop-and-pig
   or magpie.msub-hadoop-and-pig submission script.  If you wish to
   configure more, you can choose to start with the magpie.sbatch or
   magpie.msub file which contains all configuration options.

3) Setup the essentials for Pig.  If you are letting Magpie do some
   setup, here are the essentials:

   MAGPIE_JOB_TYPE : Set to "pig".

   PIG_SETUP : Set to yes

   PIG_VERSION : Set appropriately.

   PIG_HOME : Where your pig code is.  Typically in an NFS mount.

   PIG_LOCAL_DIR : A small place for conf files and log files local to
   each node.  Typically /tmp directory.

   PIG_MODE : The first time you'll probably want to run w/ "testpig"
   mode just to try things out.  Later you may want to run w/ 'script'
   or 'interactive' mode, as that is the more likely way to run.

4) If you are happy with the configuration files provided by this
   project, you can use them.  If not, change them.  If you copy them
   to a new directory, adjust PIG_CONF_FILES in your submission script
   as needed.

5) Run "sbatch -k ./magpie.sbatchfile" or "msub ./magpie.msubfile"
   ... and any other options you see fit.

6) Look at your job output file to see your output.  There will also
   be some notes/instructions/tips in the output file for viewing the
   status of your job, how to interact, etc..

Basic Instructions For Running Zookeeper
----------------------------------------

1) Download your favorite version of Zookeeper off of Apache and install
   it into a location where it's accessible on all cluster nodes.
   Usually this is on a NFS home directory.  

2) Select an appropriate job submission script for running your job.
   You can find Slurm Sbatch scripts in the directory script-sbatch or
   Moab Msub scripts in the directory script-msub-slurm.

   If the Zookeeper section isn't in it, just copy the Zookeeper
   section from magpie.sbatch or magpie.msub into it.

3) Setup the essentials for Zookeeper.  Here are the essentials:

   ZOOKEEPER_SETUP : Set to yes

   ZOOKEEPER_VERSION : Set appropriately.

   ZOOKEEPER_HOME : Where your zookeeper code is.  Typically in an NFS
   mount.

   ZOOKEEPER_REPLICATION_COUNT : Number of nodes in your Zookeeper
   quorom.

   ZOOKEEPER_MODE : This will almost certainly be "launch". 

   ZOOKEEPER_FILESYSTEM_MODE : most will likely want "networkfs" so
   data files can be stored to Lustre.  If you have local disks such
   as SSDs, you can use "local" instead.

   ZOOKEEPER_DATA_DIR : The base path for where you will store
   Zookeeper data.

   ZOOKEEPER_LOCAL_DIR : A small place for conf files and log files
   local to each node.  Typically /tmp directory.

4) If you are happy with the configuration files provided by this
   project, you can use them.  If not, change them.  If you copy them
   to a new directory, adjust ZOOKEEPER_CONF_FILES in your submission
   script as needed.

5) Run "sbatch -k ./magpie.sbatchfile" or "msub ./magpie.msubfile"
   appropriately.  Now Zookeeper will be started along with your job.

Basic Instructions For Running Hbase
------------------------------------

1) Download your favorite version of Hbase off of Apache and install
   it into a location where it's accessible on all cluster nodes.
   Usually this is on a NFS home directory.  

   See below about patches that may be necessary for Hbase depending
   on your environment and Hbase version.

2) Select an appropriate job submission script for running your job.
   You can find Slurm Sbatch scripts in the directory script-sbatch or
   Moab Msub scripts in the directory script-msub-slurm.

   You'll likely want to start with the magpie.sbatch-hbase-with-hdfs
   or magpie.msub-hbase-with-hdfs submission script.  

3) Setup the essentials for Hbase.  Here are the essentials:

   HBASE_SETUP : Set to yes

   HBASE_VERSION : Set appropriately.

   HBASE_HOME : Where your Hbase code is.  Typically in an NFS
   mount.

   HBASE_LOCAL_DIR : A small place for conf files and log files local
   to each node.  Typically /tmp directory.

   HBASE_MODE : The first time you'll probably want to run w/
   'performanceeval' mode just to try things out.  Later you may want
   to run w/ 'script' or 'interactive' mode, as that is the more
   likely way to run.

4) Hbase requires HDFS, so ensure the Hadoop w/ HDFS is configured and
   also in your submission script.  MapReduce is not needed with Hbase
   but can be setup along with it.  See above for Hadoop setup
   instructions.

5) Hbase requires Zookeeper, so setup the essentials for Zookeeper.
   See above for Zookeeper setup instructions.

6) If you are happy with the configuration files provided by this
   project, you can use them.  If not, change them.  If you copy them
   to a new directory, adjust HBASE_CONF_FILES in your submission
   script as needed.

6) Run "sbatch -k ./magpie.sbatchfile" or "msub ./magpie.msubfile
   ... and any other options you see fit.

7) Look at your job output file to see your output.  There will also
   be some notes/instructions/tips in the output file for viewing the
   status of your job, how to interact, etc..

Hbase Notes
-----------

If you increase the size of your node allocation when running Hbase on
HDFS over Lustre or HDFS over NetworkFS, data/regions will not be
balanced over all of the new nodes.  Think of this similarly to how
data would not be distributed evenly if you added new nodes into a
traditional Hbase/Hadoop cluster.  Over time Hbase will rebalance data
over the new nodes.

Basic Instructions For Spark
----------------------------

1) Download your favorite version of Spark off of Apache and install
   it into a location where it's accessible on all cluster nodes.
   Usually this is on a NFS home directory.  You may need to set
   SPARK_HADOOP_VERSION and run 'sbt/sbt assembly' to prepare Spark
   for execution.  If you are not using the default Java
   implementation installed in your system, you may need to edit
   sbt/sbt to use the proper Java version you desire (this is the case
   with 0.9.1, not the case in future versions).

   See below about patches that may be necessary for Spark depending
   on your environment and Spark version.

2) Select an appropriate job submission script for running your job.
   You can find Slurm Sbatch scripts in the directory script-sbatch or
   Moab Msub scripts in the directory script-msub-slurm.

   You'll likely want to start with the magpie.sbatch-spark-with-hdfs
   or magpie.msub-spark-with-hdfs submission script.

   It should be noted that you can run Spark without HDFS.  You can
   access files normally through "file://<path>".

3) Setup the essentials for Spark.  Here are the essentials:

   SPARK_SETUP : Set to yes

   SPARK_VERSION : Set appropriately.

   SPARK_HOME : Where your Spark code is.  Typically in an NFS
   mount.

   SPARK_LOCAL_DIR : A small place for conf files and log files local
   to each node.  Typically /tmp directory.

   SPARK_MODE : The first time you'll probably want to run w/
   'sparkpi' mode just to try things out.  Later you may want
   to run w/ 'script' or 'interactive' mode, as that is the more
   likely way to run.

4) If you are happy with the configuration files provided by this
   project, you can use them.  If not, change them.  If you copy them
   to a new directory, adjust SPARK_CONF_FILES in your submission
   script as needed.

5) Run "sbatch -k ./magpie.sbatchfile" or "msub ./magpie.msubfile
   ... and any other options you see fit.

6) Look at your job output file to see your output.  There will also
   be some notes/instructions/tips in the output file for viewing the
   status of your job, how to interact, etc..

Exported Environment Variables
------------------------------

The following environment variables are exported by magpie-run and may
be useful in scripts in your run or in pre/post run scripts.

MAGPIE_CLUSTER_NODERANK : the rank of the node you are on.  It's often
                          convenient to do something like

if [ $MAGPIE_CLUSTER_NODERANK == 0 ]
then 
   ....
fi

To only do something on one node of your allocation.

MAGPIE_NODE_COUNT : Number of nodes in this allocation.

MAGPIE_NODELIST : Nodes in your allocation.

HADOOP_MASTER_NODE : the master node of the Hadoop allocation

HADOOP_SLAVE_COUNT : number of compute/data nodes in your allocation
                     for Hadoop.

HADOOP_CONF_DIR : the directory that Hadoop configuration files local
                  to the node are stored.

HADOOP_LOG_DIR : the directory Hadoop log files are stored

PIG_CONF_DIR : the directory that Pig configuration files
               local to the node are stored.

ZOOKEEPER_CONF_DIR : the directory that Zookeeper configuration files
          	     local to the node are stored.

ZOOKEEPER_LOG_DIR : the directory Zookeeper log files are stored

HBASE_CONF_DIR : the directory that Hbase configuration files local
                 to the node are stored.

HBASE_LOG_DIR : the directory Hbase log files are stored

HBASE_MASTER_NODE : the master node of the Hbase allocation

HBASE_REGIONSERVER_COUNT : number of region servers in your allocation
                           for Hbase.

SPARK_MASTER_NODE : the master node of the Spark allocation

SPARK_MASTER_PORT : the master port for running Spark jobs

SPARK_SLAVE_COUNT : number of compute/data nodes in your allocation
                    for Spark.

SPARK_CONF_DIR : the directory that Spark configuration files local
                 to the node are stored.

SPARK_LOG_DIR : the directory Spark log files are stored

Convenience Scripts
-------------------

A number of convenience scripts are included in the scripts/
directory, both for possible usefulness and as examples.  Notable ones
worth mentioning:

magpie-gather-config-files-and-logs-script.sh - This script will get
all of the conf files and log files from Hadoop, Hbase, Pig, and/or
Zookeeper and store it in a location for post-analysis of your job.
It's convenient for debugging.

magpie-output-config-files-script.sh - This script will output all of
the conf files from your job.  It's convenient for debugging.

hadoop-hdfs-over-lustre-nodes-decomission-script.sh - See HDFS over
Lustre section above for details.

hadoop-rebalance-hdfs-over-lustre-if-increasing-nodes-script.sh - See
HDFS over Lustre section above for details.

hadoop-hdfs-fsck-cleanup-corrupted-blocks-script.sh - Cleanup/remove
corrupted blocks in HDFS.

hbase-major-compaction.sh - Perform a major compaction on all Hbase
tables.

Patching
--------

Generally speaking, no modifications to any projects are necessary.
However tweaks may be necessary depending on your environment or
special features listed above.

Patch - passwordless SSH
------------------------

Many projects in the big data space require passwordless ssh to launch
daemons on remote nodes.  In some environments, passwordless ssh is
disabled, therefore requiring a modification to allow you to use
non-ssh mechanisms for launching daemons.

I have submitted a patch for Hadoop at this JIRA:

https://issues.apache.org/jira/browse/HADOOP-9109

For those who use mrsh (https://github.com/chaos/mrsh), applying one
of the appropriate patches in the 'patches' directory will allow you
to specify mrsh for launching remote daemons instead of ssh using the
MAGPIE_REMOTE_CMD environment variable.

A similar patch is also available for Hbase and Spark.

Patch - low entropy in Hadoop 1.0
---------------------------------

See this Jira:

https://issues.apache.org/jira/browse/HDFS-1835

Hadoop 1.0 appears to have more trouble on diskless systems, as
diskless systems have less entropy in them.  So you may wish to apply
the patch in the above jira if things don't seem to be working.  I
noticed datanode log error on my diskless cluster:

2013-09-19 10:45:37,620 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnRegistration = DatanodeRegistration(apex114.llnl.gov:50010, storageID=, infoPort=50075, ipcPort=50020)

Notice the storageID is blank, that's b/c the random number
calculation failed.  Subsequently daemons aren't started, datanodes
can't connect to the namenode, etc., badness all around.

If you have root privileges, starting up the rngd daemon is another
way to solve this problem without resorting to patching.

Patch - Terasort over Raw Network FS
------------------------------------

I believe there is a bug in the Terasort example, leading to issues
with running Terasort against a parallel file system directly.  I
submitted a patch in this Jira.

https://issues.apache.org/jira/browse/MAPREDUCE-5528

The patch is included in the patches directory.

Patch - Alternate Config Dir for Spark
--------------------------------------

Unlike Hadoop and Hbase, for some reason Spark's startup scripts do
not allow flexibility with SPARK_CONF_DIR as of Spark 0.9.1.  Patches
to allow an alternate config directory is available in the patches
directory.  

Alternate conf dir support in the sbin directory is being monitored via:

https://issues.apache.org/jira/browse/SPARK-693

and

https://issues.apache.org/jira/browse/SPARK-1559

Dependencies
------------

This project includes a script called 'magpie-expand-nodes' which is
used for hostrange expansion within the scripts.  It is a hack pieced
together from other scripts.

The preferred mechanism is to use the hostlist command in the
lua-hostlist project.  You can find lua-hostlist at 
https://github.com/grondo/lua-hostlist

The main magpie-run script will use 'magpie-expand-nodes' if it cannot
find 'hostlist' in its path.

Testing
-------

While I did some testing w/ Hadoop 1.2.1, b/c of the problem mentioned
above (HDFS-1835), I did most development and testing against Hadoop
2.1.0-beta.  I also regularly tested against Hadoop 2.2.0.

Pig support was added/tested against version 0.12.0.

Zookeeper support was added/tested against version 3.4.5.

UDA support was added/tested against uda-plugin 3.3.2-0 with Hadoop
2.2.0 and an appropriate patch for MAPREDUCE-5329.

Hbase was tested against hbase-0.96.1.1-hadoop2.  It has only been
tested against HDFS based file systems.

Spark was tested against spark-0.9.1.

Contributions
-------------

Feel free to send me patches for new environment variables, new
adjustments, new optimization possibilities, alternate defaults that
you feel are better, etc.

Any patches you submit to me for fixes will be appreciated.  I am by
no means a bash expert ... in fact I'm quite bad at it.

Credit
------

Credit must be given to Kevin Regimbal @ PNNL.  Initial experiments
were done using heavily modified versions of scripts Kevin developed
for running Hadoop w/ Slurm & Lustre.  A number of the ideas from
Kevin's scripts continue in spirit in these scripts.
