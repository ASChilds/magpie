Running Hadoop on Clusters w/ Slurm & Lustre

Albert Chu
Updated October 21st, 2013
chu11@llnl.gov

What is this project
--------------------

This project contains a number of scripts for running Hadoop jobs in
HPC environments using Slurm and running jobs on top of Lustre.

This project allows you to:

- Run Hadoop interactively or via scripts.
- Run Mapreduce 1.0 or 2.0 jobs (i.e. Hadoop 1.0 or 2.0)
- Run against a number of filesystem options, such as HDFS, HDFS over
  Lustre, or Lustre directly
- Take advantage of SSDs for local caching if available
- Make decent optimizations of Hadoop for your hardware

Credit
------

First, credit must be given to Kevin Regimbal @ PNNL.  Initial
experiments were done using heavily modified versions of scripts Kevin
developed for running Hadoop w/ Slurm & Lustre.  A number of the
ideas from Kevin's scripts are still in these scripts.

Basic Idea
----------

The basic idea behind these scripts are to:

1) Allocate nodes on a cluster using slurm

2) Scripts will setup configuration files so the Slurm/MPI rank 0 node
   is the "master".  All compute nodes will have configuration files
   created that point to the node designated as the jobtracker/yarn
   server.

   The Hadoop configuration files will be populated with values for
   your filesystem choice and the hardware that exists in your
   cluster.  Reasonable attempts are made to determine optimal values
   (and they are almost certainly better than the default values for
   Hadoop).

3) Launch Hadoop daemons on all nodes.  The Slurm/MPI rank 0 node will
   run the JobTracker/NameNode (or Yarn Server in Hadoop 2.0).  All
   remaining nodes will run DataNodes/Tasktrackers (or NodeManager in
   Hadoop 2.0).

4) Now you have a mini Hadoop cluster to do whatever you want.

Basic Instructions For Running Hadoop
-------------------------------------

1) Download your favorite version of Hadoop off of Apache and install
   it into a location where it's accessible on all cluster nodes.
   Usually this is on a NFS home directory.  Adjust HADOOP_VERSION and
   HADOOP_HOME in magpie.sbatch appropriately for the install.

2) Open up magpie.sbatch and setup Slurm essentials for your job.
   Here are the essentials for the setup:

   #SBATCH --nodes : Set how many nodes you want in your job

   SBATCH_TIMELIMIT : Set the time for this job to run

   #SBATCH --partition : Set the job partition

   MAGPIE_SCRIPTS_HOME - Set where your scripts are.

3) Now setup the essentials for Hadoop.  Here are the essentials:

   HADOOP_MODE : The first time you'll probably want to run w/
   'terasort' mode just to try things out.  Later you may want to run
   w/ 'script' or 'interactive' mode, as that is the more likely way
   to run.

   HADOOP_FILESYSTEM_MODE : most will likely want "hdfsoverlustre".
   See below for details on HDFS over Lustre.

   HADOOP_SETUP_TYPE : Are you running Mapreduce version 1 or 2

   HADOOP_VERSION : Make sure your build matches HADOOP_SETUP_TYPE
   (i.e. don't say you want MapReduce 1 and point to Hadoop 2.0 build)

   HADOOP_HOME : Where your hadoop code is.  Typically in an NFS mount.

   HADOOP_LOCAL_DIR : A small place for conf files and log files local
   to each node.  Typically /tmp directory.

   HADOOP_HDFSOVERLUSTRE_PATH : For hdfs over lustre, need to set
   this.  If not using HDFS over Lustre, set the appropriate path for
   your filesystem mode choice.

   JAVA_HOME : B/c you need to ...

4) If you are happy with the configuration files provided by this
   project, you can use them.  If not, change them.  If you copy them
   to a new directory, adjust HADOOP_CONF_FILES in magpie.sbatch as
   needed.

5) Run "sbatch -k ./magpie.sbatch" ... and any other options you see
   fit.

6) Look at your slurm output file to see your output.  There will also
   be some notes/instructions/tips in the slurm output file for
   viewing the status of your job, how to interact, etc..

Basics of HDFS over Lustre
--------------------------

Instead of using local disk, designate a lustre directory to "emulate"
local disk for each compute node.  For example, lets say you have 4
compute nodes.  If we create the following paths in Lustre,

/lustre/myusername/node-0
/lustre/myusername/node-1
/lustre/myusername/node-2
/lustre/myusername/node-3

We can give each of these paths to one of the compute nodes, which
they can treat like a local disk.

Q: Does that mean I have to constantly rebuild HDFS everytime I start
  a job?

A: No, using Slurm/MPI ranks, "disk-paths" can be consistently
   assigned to nodes so that all your HDFS files from a previous run
   can exist on a later run.

Q: But that'll mean I have to consistently use the same number of
   cluster nodes?

A: Generally speaking no, but you can hit issues if you don't.  Just
   imagine how HDFS issues if you were on a traditional Hadoop cluster
   and added or removed nodes.

   Generally speaking, increasing the number of nodes you use for a
   job is fine.  Data you currently have in HDFS is still there and
   readable, but it is not viewed as "local" according to HDFS and
   more network transfers will have to happen.  You may wish to
   rebalance the HDFS blocks though.  The start-balancer.sh that is
   normally used probably will not work.  All of the paths are in
   Lustre, so the "free space" on each "local" path is identical,
   messing up calculations for balancing (i.e. no "local disk" is
   more/less utilized than another).  The script
   hadoop-rebalance-hdfs-over-lustre-if-increasing-nodes-script.sh can
   be used instead.

   Decreasing nodes is a bit more dangerous, as data can "disappear"
   just like if it were on a traditional Hadoop cluster.  If you try
   to scale down the number of nodes, you should go through the
   process of "decomissioning" nodes like on a real cluster, otherwise
   you may lose data.  You can decomission nodes through the
   hadoop-hdfs-over-lustre-nodes-decomission-script.sh script.

Q: What should HDFS replication be?

A: The scripts in this package default to HDFS replication of 3 when
   HDFS over Lustre is done.  If HDFS replication is > 1, it can
   improve performance of your job reading in HDFS input b/c there
   will be fewer network transfer of data (i.e. Hadoop may need to
   transfer "non-local" data to another node).  In addition, if a
   datanode were to die (i.e. a node crashes) Hadoop has the ability
   to survive the crash just like in a traditional Hadoop cluster.

   The trade-off is space and HDFS writes.  With HDFS replication of
   1, you can save space, but replication also increases the time for
   writes.

Advanced
--------

There are many advanced options and other scripting options.  Please
see magpie.sbatch for details.

The scripts make decent guesstimates on what would be best for
performance, but it always depends on your job and your hardware.
Many options in magpie.sbatch are available to help you adjust your
performance.

Exported Environment Variables
------------------------------

The following environment variables are exported by the sbatch
magpie-run script and may be useful in scripts.

MAGPIE_CLUSTER_NODERANK : the rank of the node you are on.  It's often
                          convenient to do something like

if [ $MAGPIE_CLUSTER_NODERANK == 0 ]
then 
   ....
fi

To only do something on one node of your allocation.

HADOOP_SLAVE_COUNT : number of compute/data nodes in your allocation

HADOOP_CONF_DIR : the directory that configuration files local to the
                  node are stored.

HADOOP_LOG_DIR : the directory log files are stored

Pro vs Con of running rawnetworkfs vs HDFS over Lustre
------------------------------------------------------

Here are some pros vs. cons of using Lustre directly (via
rawnetworkfs) vs HDFS over Lustre.

Rawnetworkfs/Lustre directly:

Pro: You can read/write files to Lustre without Hadoop/HDFS running.

Con: User must "manage" and organize their files with Lustre directly,
not gaining the block advantages of HDFS.  If not handled well, this
can lead to performance issues w/ Lustre directly.

Con: Possible portability issues w/ code that usually runs on HDFS.

HDFS over Lustre:

Pro: Portability w/ code that runs against a "traditional" Hadoop
cluster.  "gotchas" are less likely to happen.

Con: Must always run job w/ Hadoop & HDFS running.

Con: Must "import" and "export" data from HDFS using job runs, cannot
read/write directly.  On some clusters, this may involve a double copy
of data. e.g. first need to copy data into Lustre, then run job to
copy data into HDFS.

Con: Possible difficulty changing job size on clusters.

Con: If HDFS replication > 1, more space used up.

Convenience Scripts
-------------------

A number of convenience scripts are included in the scripts/
directory, both for possible usefulness and as examples.  Notable ones
worth mentioning:

hadoop-gather-config-files-and-logs-script.sh - This script will get
all of the conf files and log files from your Hadoop job and store it
in a location for post-analysis of your job.  It's convenient for
debugging.

hadoop-hdfs-over-lustre-nodes-decomission-script.sh - See HDFS over
Lustre section above for details.

hadoop-rebalance-hdfs-over-lustre-if-increasing-nodes-script.sh - See
HDFS over Lustre section above for details.

Patching Hadoop
---------------

Generally speaking, no modifications to Hadoop are necessary, however
tweaks may be necessary depending on your environment and some special
cases are listed below.  In some environments, passwordless ssh is
disabled, therefore requiring a modification to Hadoop to allow you to
use non-ssh mechanisms for launching daemons.

I have submitted a patch for adjusting this at this JIRA:

https://issues.apache.org/jira/browse/HADOOP-9109

For those who use mrsh (https://github.com/chaos/mrsh), applying one
of the appropriate patches in the 'patches' directory will allow you
to specify mrsh for launching remote daemons instead of ssh using the
MAGPIE_REMOTE_CMD environment variable.

Special Note on Hadoop 1.0
--------------------------

See this Jira:

https://issues.apache.org/jira/browse/HDFS-1835

Hadoop 1.0 appears to have more trouble on diskless systems, as
diskless systems have less entropy in them.  So you may wish to apply
the patch in the above jira if things don't seem to be working.  I
noticed datanode log error on my diskless cluster:

2013-09-19 10:45:37,620 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: dnRegistration = DatanodeRegistration(apex114.llnl.gov:50010, storageID=, infoPort=50075, ipcPort=50020)

Notice the storageID is blank, that's b/c the random number
calculation failed.  Subsequently daemons aren't started, datanodes
can't connect to the namenode, etc., badness all around.

If you have root privileges, starting up the rngd daemon is another
way to solve this problem without resorting to patching.

Special Note about Terasort over Raw Network FS
-----------------------------------------------

I believe there is a bug in the Terasort example, leading to issues
with running Terasort against a parallel file system directly.  I
submitted a patch in this Jira.

https://issues.apache.org/jira/browse/MAPREDUCE-5528

Magpie Network File System Option
---------------------------------

As noted above, there are some portability or "gotchas" when using
Rawnetworkfs.  In attempt to get around some of those issues, the
"Magpie Network FS" patch is available as a plugin for Hadoop 2.0
(perhaps 1.0 in the future).  It is a file system plugin that is a
small layer about the Raw Local FileSystem.

The patch runs very similarly to rawnetworkfs except that it handles
the following extra cases.

1) It handles the case indicated in

https://issues.apache.org/jira/browse/MAPREDUCE-5528

2) If you specify a relative path, the Magpie Network FS option will
read/write to the base path of HADOOP_MAGPIENETWORKFS_PATH.  The
current working directory also defaults to HADOOP_MAGPIENETWORKFS_PATH
regardless of where you are in the file system tree.

For example, if the user specifies the path "mypath/file", with
rawnetworkfs it would normally be written to the current working
directory the user is running in.  The user would be required to
specify the absolute path to ensure files are written to the networked
file system (e.g. /mynetworkmount/mypath/file).

With this patch, the relative path "mypath/file" would be written to 
${HADOOP_MAGPIENETWORKFS_PATH}/mypath/file.

This can help w/ portability of code/scripts that usually assume HDFS,
which does not have a current working directory.

A patch for Magpie Network FS can be found in the patches directory.

Testing
-------

While I did some testing w/ Hadoop 1.2.1, b/c of the problem mentioned
above (HDFS-1835), I did most development and testing against Hadoop
2.1.0-beta.

Dependency
----------

This project includes a script called 'magpie-expand-nodes' which is
used for hostrange expansion within the scripts.  It is a hack pieced
together from other scripts.

The preferred mechanism is to use the hostlist command in the
lua-hostlist project.  You can find lua-hostlist here : < FILL IN >

The main magpie-run script will use 'magpie-expand-nodes' if it cannot
find 'hostlist' in its path.

Contributions
-------------

Feel free to send me patches for new environment variables, new
adjustments, new optimization possibilities, alternate defaults that
you feel are better, etc.

Any patches you submit to me for fixes will be appreciated.  I am by
no means a bash expert ... in fact I'm quite bad at it.
