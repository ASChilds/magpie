############################################################################
# Spark Core Configurations
############################################################################

# Should Spark be run
#
# Specify yes or no.  Defaults to no.
# 
export SPARK_SETUP=no

# Version
#
export SPARK_VERSION="0.9.1"

# Path to your Spark build/binaries
#
# This should be accessible on all nodes in your allocation. Typically
# this is in an NFS mount.
#
# Ensure the build matches the Hadoop/HDFS version this will run against.
#
export SPARK_HOME="/home/username/spark-${SPARK_VERSION}"

# Path local to each cluster node, typically something in /tmp.
# This will store local conf files and log files for your job. 
#
export SPARK_LOCAL_DIR="/tmp/username/spark"

# Directory where Spark configuration templates are stored
#
# If not specified, assumed to be $MAGPIE_SCRIPTS_HOME/conf
#
# export SPARK_CONF_FILES="${MAGPIE_SCRIPTS_HOME}/conf"

# Worker Cores per Node
#
# If not specified, a reasonable estimate will be calculated based on
# number of CPUs on the system.
#
# If also running Hbase or Hadoop MapReduce, be aware of the number of
# tasks and the amount of memory that may be needed by other software.
#
# export SPARK_WORKER_CORES_PER_NODE=8

# Worker Memory
#
# Specified in M.  If not specified, a reasonable estimate will be
# calculated based on total memory available and number of CPUs on the
# system.
#
# If also running Hbase or Hadoop MapReduce, be aware of the number of
# tasks and the amount of memory that may be needed by other software.
#
# export SPARK_WORKER_MEMORY_PER_NODE=16000

# Worker Directory
#
# Directory to run applications in, which will include both logs and
# scratch space for local jars.  If not specified, defaults to
# SPARK_HOME/work.
#
# Generally speaking, this is probably fine if SPARK_HOME/work is in
# an NFS mount, but under some situation, such as having many large
# jar files, using an NFS mount may not be wise.
#
# export SPARK_WORKER_DIRECTORY=/tmp/username/spark

# SPARK_JOB_MEMORY
#
# Memory for spark jobs.  Defaults to being set equal to
# SPARK_WORKER_MEMORY_PER_NODE, but users may wish to lower it if
# multiple Spark jobs will be submitted at the same time.
#
# export SPARK_JOB_MEMORY="2048"

# Daemon Heap Max
#
# Heap maximum for Spark daemons, specified in megs.
#
# If not specified, defaults to 1000
#
# May need to be increased if you are scaling large, get OutofMemory
# errors, or perhaps have a lot of cores on a node.
#
# export SPARK_DAEMON_HEAP_MAX=2000

# Environment Extra
#
# Specify extra environment information that should be passed into
# Spark.  This file will simply be appended into the spark-env.sh.
#
# By default, a reasonable estimate for max user processes and open
# file descriptors will be calculated and put into spark-env.sh.
# However, it's always possible they may need to be set
# differently. Everyone's cluster/situation can be slightly different.
#
# See the example example-environment-extra for examples on
# what you can/should do with adding extra environment settings.
#
# export SPARK_ENVIRONMENT_EXTRA_PATH="${MAGPIE_SCRIPTS_HOME}/spark-my-environment"

############################################################################
# Spark Job/Run Configurations
############################################################################

# Set how Spark should run
#
# "sparkpi" - run sparkpi example. Useful for making sure things are
#             setup the way you like.
#
# "script" - execute a script that lists all of your Spark jobs.  Be
#            sure to set SPARK_SCRIPT_PATH to your script.
#
# "interactive" - manually interact to submit jobs, peruse Spark, etc.
#                 In this mode you'll login to the cluster node that
#                 is your 'master' node and interact with Spark
#                 directly (e.g. bin/spark-shell ...)
#
# "setuponly" - Like 'interactive' but only setup conf files. useful
#               if user wants to setup daemons themselves, etc.
#
export SPARK_MODE="sparkpi"

# SPARK_DEFAULT_PARALLELISM
#
# Default number of tasks to use across the cluster for distributed
# shuffle operations (groupByKey, reduceByKey, etc) when not set by
# user. If not specified, defaults to # compute nodes (i.e. 1 per node)
#
# export SPARK_DEFAULT_PARALLELISM=8

# SPARK_JOB_CLASSPATH
#
# May be necessary to set to get certain code/scripts to work.
#
# e.g. to run a Spark example, you may need to set
#
# export SPARK_JOB_CLASSPATH="examples/target/scala-2.10/spark-examples-assembly-0.9.1.jar"
#
# export SPARK_JOB_CLASSPATH=""

# SPARK_JOB_LIBRARY_PATH
#
# May be necessary to set to get certain code/scripts to work.
#
# export SPARK_JOB_LIBRARY_PATH=""

# SPARK_JOB_JAVA_OPTS  
#
# May be necessary to set options to set Spark options
#
# e.g. -Dspark.default.parallelism=16
#
# Magpie will set several options on its own, however, these options
# will be appended last, ensuring they override anything that Magpie
# will set by default.
#
# export SPARK_JOB_JAVA_OPTS=""

# SPARK_LOCAL_SCRATCH_DIR
#
# By default, if Hadoop is setup with a file system, the Spark local
# scratch directory, where scratch data is placed, will automatically
# be calculated and configured.  If Hadoop is not setup, the following
# must be specified.
#
# If you have local SSDs stored on the nodes of your system, it may be
# in your interest to set this to a local drive and adjust
# SPARK_LOCAL_SCRATCH_DIR_TYPE below to 'local'.
#
# export SPARK_LOCAL_SCRATCH_DIR="/myscratchlocation"

# Set how SPARK_LOCAL_SCRATCH_DIR should be setup.
#
# "networkfs" - SPARK_LOCAL_SCRATCH_DIR points to a network filesystem
#               (such as Lustre).  Local paths will be
#               setup.
#
# "local" - SPARK_LOCAL_SCRATCH_DIR points to a local drive.
#
# export SPARK_LOCAL_SCRATCH_DIR_TYPE="networkfs"

############################################################################
# Spark SparkPi Configuration
############################################################################

# SparkPi Slices
#
# Number of "slices" to parallelize in Pi estimation.  Generally
# speaking, more should lead to more accurate estimates.
#
# If not specified, equals number of nodes.
# 
# export SPARK_SPARKPI_SLICES=4

############################################################################
# Spark Script Configurations
############################################################################

# Specify script to execute for "script" mode
#
# See spark-example-job-script for example of what to put in the script.
#
# export SPARK_SCRIPT_PATH="${MAGPIE_SCRIPTS_HOME}/my-job-script"
