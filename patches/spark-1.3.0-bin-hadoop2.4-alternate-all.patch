diff -pruN ./spark-1.3.0-bin-hadoop2.4-orig/bin/spark-submit ./spark-1.3.0-bin-hadoop2.4/bin/spark-submit
--- ./spark-1.3.0-bin-hadoop2.4-orig/bin/spark-submit	2015-03-05 16:31:24.000000000 -0800
+++ ./spark-1.3.0-bin-hadoop2.4/bin/spark-submit	2015-03-25 10:50:31.750217000 -0700
@@ -47,7 +47,12 @@ done
 if [ -z "$SPARK_CONF_DIR" ]; then
   export SPARK_CONF_DIR="$SPARK_HOME/conf"
 fi
-DEFAULT_PROPERTIES_FILE="$SPARK_CONF_DIR/spark-defaults.conf"
+if [ "${SPARK_CONF_DIR}X" == "X" ]
+then
+       DEFAULT_PROPERTIES_FILE="$SPARK_HOME/conf/spark-defaults.conf"
+else
+       DEFAULT_PROPERTIES_FILE="$SPARK_CONF_DIR/spark-defaults.conf"
+fi
 if [ "$MASTER" == "yarn-cluster" ]; then
   SPARK_SUBMIT_DEPLOY_MODE=cluster
 fi
diff -pruN ./spark-1.3.0-bin-hadoop2.4-orig/sbin/slaves.sh ./spark-1.3.0-bin-hadoop2.4/sbin/slaves.sh
--- ./spark-1.3.0-bin-hadoop2.4-orig/sbin/slaves.sh	2015-03-05 16:31:24.000000000 -0800
+++ ./spark-1.3.0-bin-hadoop2.4/sbin/slaves.sh	2015-03-25 10:50:49.796154000 -0700
@@ -25,6 +25,8 @@
 #     Default is ${SPARK_CONF_DIR}/slaves.
 #   SPARK_CONF_DIR  Alternate conf dir. Default is ${SPARK_HOME}/conf.
 #   SPARK_SLAVE_SLEEP Seconds to sleep between spawning remote commands.
+#   SPARK_SSH_CMD Specify an alternate remote shell command.
+#     Defaults to ssh if not specified.
 #   SPARK_SSH_OPTS Options passed to ssh when running remote commands.
 ##
 
@@ -79,19 +81,19 @@ if [ "$HOSTLIST" = "" ]; then
   fi
 fi
 
-
+RSH_CMD=${SPARK_SSH_CMD:-ssh}
 
 # By default disable strict host key checking
-if [ "$SPARK_SSH_OPTS" = "" ]; then
+if [ "$RSH_CMD" == "ssh" ] && [ "$SPARK_SSH_OPTS" = "" ]; then
   SPARK_SSH_OPTS="-o StrictHostKeyChecking=no"
 fi
 
 for slave in `echo "$HOSTLIST"|sed  "s/#.*$//;/^$/d"`; do
   if [ -n "${SPARK_SSH_FOREGROUND}" ]; then
-    ssh $SPARK_SSH_OPTS "$slave" $"${@// /\\ }" \
+    $RSH_CMD $SPARK_SSH_OPTS "$slave" $"${@// /\\ }" \
       2>&1 | sed "s/^/$slave: /"
   else
-    ssh $SPARK_SSH_OPTS "$slave" $"${@// /\\ }" \
+    $RSH_CMD $SPARK_SSH_OPTS "$slave" $"${@// /\\ }" \
       2>&1 | sed "s/^/$slave: /" &
   fi
   if [ "$SPARK_SLAVE_SLEEP" != "" ]; then
diff -pruN ./spark-1.3.0-bin-hadoop2.4-orig/sbin/spark-daemon.sh ./spark-1.3.0-bin-hadoop2.4/sbin/spark-daemon.sh
--- ./spark-1.3.0-bin-hadoop2.4-orig/sbin/spark-daemon.sh	2015-03-05 16:31:24.000000000 -0800
+++ ./spark-1.3.0-bin-hadoop2.4/sbin/spark-daemon.sh	2015-03-25 10:50:49.823126000 -0700
@@ -138,7 +138,8 @@ case $option in
 
     if [ "$SPARK_MASTER" != "" ]; then
       echo rsync from "$SPARK_MASTER"
-      rsync -a -e ssh --delete --exclude=.svn --exclude='logs/*' --exclude='contrib/hod/logs/*' $SPARK_MASTER/ "$SPARK_HOME"
+      RSH_CMD=${SPARK_SSH_CMD:-ssh}
+      rsync -a -e $RSH_CMD --delete --exclude=.svn --exclude='logs/*' --exclude='contrib/hod/logs/*' $SPARK_MASTER/ "$SPARK_HOME"
     fi
 
     spark_rotate_log "$log"
diff -pruN ./spark-1.3.0-bin-hadoop2.4-orig/sbin/spark-daemon.sh.orig ./spark-1.3.0-bin-hadoop2.4/sbin/spark-daemon.sh.orig
--- ./spark-1.3.0-bin-hadoop2.4-orig/sbin/spark-daemon.sh.orig	1969-12-31 16:00:00.000000000 -0800
+++ ./spark-1.3.0-bin-hadoop2.4/sbin/spark-daemon.sh.orig	2015-03-05 16:31:24.000000000 -0800
@@ -0,0 +1,187 @@
+#!/usr/bin/env bash
+
+#
+# Licensed to the Apache Software Foundation (ASF) under one or more
+# contributor license agreements.  See the NOTICE file distributed with
+# this work for additional information regarding copyright ownership.
+# The ASF licenses this file to You under the Apache License, Version 2.0
+# (the "License"); you may not use this file except in compliance with
+# the License.  You may obtain a copy of the License at
+#
+#    http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+#
+
+# Runs a Spark command as a daemon.
+#
+# Environment Variables
+#
+#   SPARK_CONF_DIR  Alternate conf dir. Default is ${SPARK_HOME}/conf.
+#   SPARK_LOG_DIR   Where log files are stored. ${SPARK_HOME}/logs by default.
+#   SPARK_MASTER    host:path where spark code should be rsync'd from
+#   SPARK_PID_DIR   The pid files are stored. /tmp by default.
+#   SPARK_IDENT_STRING   A string representing this instance of spark. $USER by default
+#   SPARK_NICENESS The scheduling priority for daemons. Defaults to 0.
+##
+
+usage="Usage: spark-daemon.sh [--config <conf-dir>] (start|stop) <spark-command> <spark-instance-number> <args...>"
+
+# if no args specified, show usage
+if [ $# -le 1 ]; then
+  echo $usage
+  exit 1
+fi
+
+sbin="`dirname "$0"`"
+sbin="`cd "$sbin"; pwd`"
+
+. "$sbin/spark-config.sh"
+
+# get arguments
+
+# Check if --config is passed as an argument. It is an optional parameter.
+# Exit if the argument is not a directory.
+
+if [ "$1" == "--config" ]
+then
+  shift
+  conf_dir="$1"
+  if [ ! -d "$conf_dir" ]
+  then
+    echo "ERROR : $conf_dir is not a directory"
+    echo $usage
+    exit 1
+  else
+    export SPARK_CONF_DIR="$conf_dir"
+  fi
+  shift
+fi
+
+option=$1
+shift
+command=$1
+shift
+instance=$1
+shift
+
+spark_rotate_log ()
+{
+    log=$1;
+    num=5;
+    if [ -n "$2" ]; then
+	num=$2
+    fi
+    if [ -f "$log" ]; then # rotate logs
+	while [ $num -gt 1 ]; do
+	    prev=`expr $num - 1`
+	    [ -f "$log.$prev" ] && mv "$log.$prev" "$log.$num"
+	    num=$prev
+	done
+	mv "$log" "$log.$num";
+    fi
+}
+
+. "$SPARK_PREFIX/bin/load-spark-env.sh"
+
+if [ "$SPARK_IDENT_STRING" = "" ]; then
+  export SPARK_IDENT_STRING="$USER"
+fi
+
+
+export SPARK_PRINT_LAUNCH_COMMAND="1"
+
+# get log directory
+if [ "$SPARK_LOG_DIR" = "" ]; then
+  export SPARK_LOG_DIR="$SPARK_HOME/logs"
+fi
+mkdir -p "$SPARK_LOG_DIR"
+touch "$SPARK_LOG_DIR"/.spark_test > /dev/null 2>&1
+TEST_LOG_DIR=$?
+if [ "${TEST_LOG_DIR}" = "0" ]; then
+  rm -f "$SPARK_LOG_DIR"/.spark_test
+else
+  chown "$SPARK_IDENT_STRING" "$SPARK_LOG_DIR"
+fi
+
+if [ "$SPARK_PID_DIR" = "" ]; then
+  SPARK_PID_DIR=/tmp
+fi
+
+# some variables
+log="$SPARK_LOG_DIR/spark-$SPARK_IDENT_STRING-$command-$instance-$HOSTNAME.out"
+pid="$SPARK_PID_DIR/spark-$SPARK_IDENT_STRING-$command-$instance.pid"
+
+# Set default scheduling priority
+if [ "$SPARK_NICENESS" = "" ]; then
+    export SPARK_NICENESS=0
+fi
+
+
+case $option in
+
+  (start|spark-submit)
+
+    mkdir -p "$SPARK_PID_DIR"
+
+    if [ -f $pid ]; then
+      TARGET_ID="$(cat "$pid")"
+      if [[ $(ps -p "$TARGET_ID" -o args=) =~ $command ]]; then
+        echo "$command running as process $TARGET_ID.  Stop it first."
+        exit 1
+      fi
+    fi
+
+    if [ "$SPARK_MASTER" != "" ]; then
+      echo rsync from "$SPARK_MASTER"
+      rsync -a -e ssh --delete --exclude=.svn --exclude='logs/*' --exclude='contrib/hod/logs/*' $SPARK_MASTER/ "$SPARK_HOME"
+    fi
+
+    spark_rotate_log "$log"
+    echo "starting $command, logging to $log"
+    if [ $option == spark-submit ]; then
+      source "$SPARK_HOME"/bin/utils.sh
+      gatherSparkSubmitOpts "$@"
+      nohup nice -n $SPARK_NICENESS "$SPARK_PREFIX"/bin/spark-submit --class $command \
+        "${SUBMISSION_OPTS[@]}" spark-internal "${APPLICATION_OPTS[@]}" >> "$log" 2>&1 < /dev/null &
+    else
+      nohup nice -n $SPARK_NICENESS "$SPARK_PREFIX"/bin/spark-class $command "$@" >> "$log" 2>&1 < /dev/null &
+    fi
+    newpid=$!
+    echo $newpid > $pid
+    sleep 2
+    # Check if the process has died; in that case we'll tail the log so the user can see
+    if [[ ! $(ps -p "$newpid" -o args=) =~ $command ]]; then
+      echo "failed to launch $command:"
+      tail -2 "$log" | sed 's/^/  /'
+      echo "full log in $log"
+    fi
+    ;;
+
+  (stop)
+
+    if [ -f $pid ]; then
+      TARGET_ID="$(cat "$pid")"
+      if [[ $(ps -p "$TARGET_ID" -o comm=) =~ "java" ]]; then
+        echo "stopping $command"
+        kill "$TARGET_ID" && rm -f "$pid"
+      else
+        echo "no $command to stop"
+      fi
+    else
+      echo "no $command to stop"
+    fi
+    ;;
+
+  (*)
+    echo $usage
+    exit 1
+    ;;
+
+esac
+
+
diff -pruN ./spark-1.3.0-bin-hadoop2.4-orig/sbin/spark-daemons.sh ./spark-1.3.0-bin-hadoop2.4/sbin/spark-daemons.sh
--- ./spark-1.3.0-bin-hadoop2.4-orig/sbin/spark-daemons.sh	2015-03-05 16:31:24.000000000 -0800
+++ ./spark-1.3.0-bin-hadoop2.4/sbin/spark-daemons.sh	2015-03-25 10:49:11.725326000 -0700
@@ -30,6 +30,24 @@ fi
 sbin=`dirname "$0"`
 sbin=`cd "$sbin"; pwd`
 
+# Check if --config is passed as an argument. It is an optional parameter.
+# Exit if the argument is not a directory.
+
+if [ "$1" == "--config" ]
+then
+  shift
+  conf_dir=$1
+  if [ ! -d "$conf_dir" ]
+  then
+    echo "ERROR : $conf_dir is not a directory"
+    echo $usage
+    exit 1
+  else
+    export SPARK_CONF_DIR=$conf_dir
+  fi
+  shift
+fi
+
 . "$sbin/spark-config.sh"
 
-exec "$sbin/slaves.sh" cd "$SPARK_HOME" \; "$sbin/spark-daemon.sh" "$@"
+exec "$sbin/slaves.sh" --config $SPARK_CONF_DIR cd "$SPARK_HOME" \; "$sbin/spark-daemon.sh" --config $SPARK_CONF_DIR "$@"
diff -pruN ./spark-1.3.0-bin-hadoop2.4-orig/sbin/start-slave.sh ./spark-1.3.0-bin-hadoop2.4/sbin/start-slave.sh
--- ./spark-1.3.0-bin-hadoop2.4-orig/sbin/start-slave.sh	2015-03-05 16:31:24.000000000 -0800
+++ ./spark-1.3.0-bin-hadoop2.4/sbin/start-slave.sh	2015-03-25 10:49:11.728322000 -0700
@@ -23,4 +23,27 @@
 sbin="`dirname "$0"`"
 sbin="`cd "$sbin"; pwd`"
 
-"$sbin"/spark-daemon.sh start org.apache.spark.deploy.worker.Worker "$@"
+# Check if --config is passed as an argument. It is an optional parameter.
+# Exit if the argument is not a directory.
+
+if [ "$1" == "--config" ]
+then
+  shift
+  conf_dir=$1
+  if [ ! -d "$conf_dir" ]
+  then
+    echo "ERROR : $conf_dir is not a directory"
+    echo $usage
+    exit 1
+  else
+    export SPARK_CONF_DIR=$conf_dir
+  fi
+  shift
+fi
+
+if [ "${SPARK_CONF_DIR}X" != "X" ]
+then
+    "$sbin"/spark-daemon.sh --config $SPARK_CONF_DIR start org.apache.spark.deploy.worker.Worker "$@"
+else
+    "$sbin"/spark-daemon.sh start org.apache.spark.deploy.worker.Worker "$@"
+fi
diff -pruN ./spark-1.3.0-bin-hadoop2.4-orig/sbin/start-slaves.sh ./spark-1.3.0-bin-hadoop2.4/sbin/start-slaves.sh
--- ./spark-1.3.0-bin-hadoop2.4-orig/sbin/start-slaves.sh	2015-03-05 16:31:24.000000000 -0800
+++ ./spark-1.3.0-bin-hadoop2.4/sbin/start-slaves.sh	2015-03-25 10:49:11.742305000 -0700
@@ -58,12 +58,12 @@ fi
 
 # Launch the slaves
 if [ "$SPARK_WORKER_INSTANCES" = "" ]; then
-  exec "$sbin/slaves.sh" cd "$SPARK_HOME" \; "$sbin/start-slave.sh" 1 "spark://$SPARK_MASTER_IP:$SPARK_MASTER_PORT"
+  exec "$sbin/slaves.sh" --config $SPARK_CONF_DIR cd "$SPARK_HOME" \; "$sbin/start-slave.sh" --config $SPARK_CONF_DIR 1 "spark://$SPARK_MASTER_IP:$SPARK_MASTER_PORT"
 else
   if [ "$SPARK_WORKER_WEBUI_PORT" = "" ]; then
     SPARK_WORKER_WEBUI_PORT=8081
   fi
   for ((i=0; i<$SPARK_WORKER_INSTANCES; i++)); do
-    "$sbin/slaves.sh" cd "$SPARK_HOME" \; "$sbin/start-slave.sh" $(( $i + 1 ))  "spark://$SPARK_MASTER_IP:$SPARK_MASTER_PORT" --webui-port $(( $SPARK_WORKER_WEBUI_PORT + $i ))
+    "$sbin/slaves.sh" --config $SPARK_CONF_DIR cd "$SPARK_HOME" \; "$sbin/start-slave.sh" --config $SPARK_CONF_DIR $(( $i + 1 ))  "spark://$SPARK_MASTER_IP:$SPARK_MASTER_PORT" --webui-port $(( $SPARK_WORKER_WEBUI_PORT + $i ))
   done
 fi
diff -pruN ./spark-1.3.0-bin-hadoop2.4-orig/sbin/stop-slaves.sh ./spark-1.3.0-bin-hadoop2.4/sbin/stop-slaves.sh
--- ./spark-1.3.0-bin-hadoop2.4-orig/sbin/stop-slaves.sh	2015-03-05 16:31:24.000000000 -0800
+++ ./spark-1.3.0-bin-hadoop2.4/sbin/stop-slaves.sh	2015-03-25 10:49:11.750299000 -0700
@@ -30,9 +30,9 @@ if [ -e "$sbin"/../tachyon/bin/tachyon ]
 fi
 
 if [ "$SPARK_WORKER_INSTANCES" = "" ]; then
-  "$sbin"/spark-daemons.sh stop org.apache.spark.deploy.worker.Worker 1
+  "$sbin"/spark-daemons.sh --config $SPARK_CONF_DIR stop org.apache.spark.deploy.worker.Worker 1
 else
   for ((i=0; i<$SPARK_WORKER_INSTANCES; i++)); do
-    "$sbin"/spark-daemons.sh stop org.apache.spark.deploy.worker.Worker $(( $i + 1 ))
+    "$sbin"/spark-daemons.sh --config $SPARK_CONF_DIR stop org.apache.spark.deploy.worker.Worker $(( $i + 1 ))
   done
 fi
