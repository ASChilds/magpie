diff -pruN hadoop-3.1.0-orig/bin/hadoop hadoop-3.1.0/bin/hadoop
--- hadoop-3.1.0-orig/bin/hadoop	2018-03-29 17:01:09.000000000 -0700
+++ hadoop-3.1.0/bin/hadoop	2018-07-12 17:12:33.756257000 -0700
@@ -222,6 +222,21 @@ fi
 hadoop_add_client_opts
 
 if [[ ${HADOOP_WORKER_MODE} = true ]]; then
+  if [ "${HADOOP_CONF_DIR}X" != "X" ] && [ "${HADOOP_CONF_DIR_CONFIG_ORIG}X" != "X" ]
+  then
+    export HADOOP_CONF_DIR="${HADOOP_CONF_DIR_CONFIG_ORIG}"
+  fi
+
+  if [ "${HADOOP_LOG_DIR}X" != "X" ] && [ "${HADOOP_LOG_DIR_CONFIG_ORIG}X" != "X" ]
+  then
+    export HADOOP_LOG_DIR="${HADOOP_LOG_DIR_CONFIG_ORIG}"
+  fi
+
+  if [ "${HADOOP_PID_DIR}X" != "X" ] && [ "${HADOOP_PID_DIR_CONFIG_ORIG}X" != "X" ]
+  then
+    export HADOOP_PID_DIR="${HADOOP_PID_DIR_CONFIG_ORIG}"
+  fi
+
   hadoop_common_worker_mode_execute "${HADOOP_COMMON_HOME}/bin/hadoop" "${HADOOP_USER_PARAMS[@]}"
   exit $?
 fi
diff -pruN hadoop-3.1.0-orig/bin/hadoop.orig hadoop-3.1.0/bin/hadoop.orig
--- hadoop-3.1.0-orig/bin/hadoop.orig	1969-12-31 16:00:00.000000000 -0800
+++ hadoop-3.1.0/bin/hadoop.orig	2018-03-29 17:01:09.000000000 -0700
@@ -0,0 +1,232 @@
+#!/usr/bin/env bash
+
+# Licensed to the Apache Software Foundation (ASF) under one or more
+# contributor license agreements.  See the NOTICE file distributed with
+# this work for additional information regarding copyright ownership.
+# The ASF licenses this file to You under the Apache License, Version 2.0
+# (the "License"); you may not use this file except in compliance with
+# the License.  You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+# The name of the script being executed.
+HADOOP_SHELL_EXECNAME="hadoop"
+MYNAME="${BASH_SOURCE-$0}"
+
+## @description  build up the hadoop command's usage text.
+## @audience     public
+## @stability    stable
+## @replaceable  no
+function hadoop_usage
+{
+  hadoop_add_option "buildpaths" "attempt to add class files from build tree"
+  hadoop_add_option "hostnames list[,of,host,names]" "hosts to use in slave mode"
+  hadoop_add_option "loglevel level" "set the log4j level for this command"
+  hadoop_add_option "hosts filename" "list of hosts to use in slave mode"
+  hadoop_add_option "workers" "turn on worker mode"
+
+  hadoop_add_subcommand "checknative" client "check native Hadoop and compression libraries availability"
+  hadoop_add_subcommand "classpath" client "prints the class path needed to get the Hadoop jar and the required libraries"
+  hadoop_add_subcommand "conftest" client "validate configuration XML files"
+  hadoop_add_subcommand "credential" client "interact with credential providers"
+  hadoop_add_subcommand "daemonlog" admin "get/set the log level for each daemon"
+  hadoop_add_subcommand "dtutil" client "operations related to delegation tokens"
+  hadoop_add_subcommand "envvars" client "display computed Hadoop environment variables"
+  hadoop_add_subcommand "fs" client "run a generic filesystem user client"
+  hadoop_add_subcommand "jar <jar>" client "run a jar file. NOTE: please use \"yarn jar\" to launch YARN applications, not this command."
+  hadoop_add_subcommand "jnipath" client "prints the java.library.path"
+  hadoop_add_subcommand "kerbname" client "show auth_to_local principal conversion"
+  hadoop_add_subcommand "key" client "manage keys via the KeyProvider"
+  hadoop_add_subcommand "trace" client "view and modify Hadoop tracing settings"
+  hadoop_add_subcommand "version" client "print the version"
+  hadoop_add_subcommand "kdiag" client "Diagnose Kerberos Problems"
+  hadoop_generate_usage "${HADOOP_SHELL_EXECNAME}" true
+}
+
+## @description  Default command handler for hadoop command
+## @audience     public
+## @stability    stable
+## @replaceable  no
+## @param        CLI arguments
+function hadoopcmd_case
+{
+  subcmd=$1
+  shift
+
+  case ${subcmd} in
+    balancer|datanode|dfs|dfsadmin|dfsgroups|  \
+    namenode|secondarynamenode|fsck|fetchdt|oiv| \
+    portmap|nfs3)
+      hadoop_error "WARNING: Use of this script to execute ${subcmd} is deprecated."
+      subcmd=${subcmd/dfsgroups/groups}
+      hadoop_error "WARNING: Attempting to execute replacement \"hdfs ${subcmd}\" instead."
+      hadoop_error ""
+      #try to locate hdfs and if present, delegate to it.
+      if [[ -f "${HADOOP_HDFS_HOME}/bin/hdfs" ]]; then
+        exec "${HADOOP_HDFS_HOME}/bin/hdfs" \
+          --config "${HADOOP_CONF_DIR}" "${subcmd}"  "$@"
+      elif [[ -f "${HADOOP_HOME}/bin/hdfs" ]]; then
+        exec "${HADOOP_HOME}/bin/hdfs" \
+          --config "${HADOOP_CONF_DIR}" "${subcmd}" "$@"
+      else
+        hadoop_error "HADOOP_HDFS_HOME not found!"
+        exit 1
+      fi
+    ;;
+
+    #mapred commands for backwards compatibility
+    pipes|job|queue|mrgroups|mradmin|jobtracker|tasktracker)
+      hadoop_error "WARNING: Use of this script to execute ${subcmd} is deprecated."
+      subcmd=${subcmd/mrgroups/groups}
+      hadoop_error "WARNING: Attempting to execute replacement \"mapred ${subcmd}\" instead."
+      hadoop_error ""
+      #try to locate mapred and if present, delegate to it.
+      if [[ -f "${HADOOP_MAPRED_HOME}/bin/mapred" ]]; then
+        exec "${HADOOP_MAPRED_HOME}/bin/mapred" \
+        --config "${HADOOP_CONF_DIR}" "${subcmd}" "$@"
+      elif [[ -f "${HADOOP_HOME}/bin/mapred" ]]; then
+        exec "${HADOOP_HOME}/bin/mapred" \
+        --config "${HADOOP_CONF_DIR}" "${subcmd}" "$@"
+      else
+        hadoop_error "HADOOP_MAPRED_HOME not found!"
+        exit 1
+      fi
+    ;;
+    checknative)
+      HADOOP_CLASSNAME=org.apache.hadoop.util.NativeLibraryChecker
+    ;;
+    classpath)
+      hadoop_do_classpath_subcommand HADOOP_CLASSNAME "$@"
+    ;;
+    conftest)
+      HADOOP_CLASSNAME=org.apache.hadoop.util.ConfTest
+    ;;
+    credential)
+      HADOOP_CLASSNAME=org.apache.hadoop.security.alias.CredentialShell
+    ;;
+    daemonlog)
+      HADOOP_CLASSNAME=org.apache.hadoop.log.LogLevel
+    ;;
+    dtutil)
+      HADOOP_CLASSNAME=org.apache.hadoop.security.token.DtUtilShell
+    ;;
+    envvars)
+      echo "JAVA_HOME='${JAVA_HOME}'"
+      echo "HADOOP_COMMON_HOME='${HADOOP_COMMON_HOME}'"
+      echo "HADOOP_COMMON_DIR='${HADOOP_COMMON_DIR}'"
+      echo "HADOOP_COMMON_LIB_JARS_DIR='${HADOOP_COMMON_LIB_JARS_DIR}'"
+      echo "HADOOP_COMMON_LIB_NATIVE_DIR='${HADOOP_COMMON_LIB_NATIVE_DIR}'"
+      echo "HADOOP_CONF_DIR='${HADOOP_CONF_DIR}'"
+      echo "HADOOP_TOOLS_HOME='${HADOOP_TOOLS_HOME}'"
+      echo "HADOOP_TOOLS_DIR='${HADOOP_TOOLS_DIR}'"
+      echo "HADOOP_TOOLS_LIB_JARS_DIR='${HADOOP_TOOLS_LIB_JARS_DIR}'"
+      if [[ -n "${QATESTMODE}" ]]; then
+        echo "MYNAME=${MYNAME}"
+        echo "HADOOP_SHELL_EXECNAME=${HADOOP_SHELL_EXECNAME}"
+      fi
+      exit 0
+    ;;
+    fs)
+      HADOOP_CLASSNAME=org.apache.hadoop.fs.FsShell
+    ;;
+    jar)
+      if [[ -n "${YARN_OPTS}" ]] || [[ -n "${YARN_CLIENT_OPTS}" ]]; then
+        hadoop_error "WARNING: Use \"yarn jar\" to launch YARN applications."
+      fi
+      HADOOP_CLASSNAME=org.apache.hadoop.util.RunJar
+    ;;
+    jnipath)
+      hadoop_finalize
+      echo "${JAVA_LIBRARY_PATH}"
+      exit 0
+    ;;
+    kerbname)
+      HADOOP_CLASSNAME=org.apache.hadoop.security.HadoopKerberosName
+    ;;
+    kdiag)
+      HADOOP_CLASSNAME=org.apache.hadoop.security.KDiag
+    ;;
+    key)
+      HADOOP_CLASSNAME=org.apache.hadoop.crypto.key.KeyShell
+    ;;
+    trace)
+      HADOOP_CLASSNAME=org.apache.hadoop.tracing.TraceAdmin
+    ;;
+    version)
+      HADOOP_CLASSNAME=org.apache.hadoop.util.VersionInfo
+    ;;
+    *)
+      HADOOP_CLASSNAME="${subcmd}"
+      if ! hadoop_validate_classname "${HADOOP_CLASSNAME}"; then
+        hadoop_exit_with_usage 1
+      fi
+    ;;
+  esac
+}
+
+# This script runs the hadoop core commands.
+
+# let's locate libexec...
+if [[ -n "${HADOOP_HOME}" ]]; then
+  HADOOP_DEFAULT_LIBEXEC_DIR="${HADOOP_HOME}/libexec"
+else
+  bin=$(cd -P -- "$(dirname -- "${MYNAME}")" >/dev/null && pwd -P)
+  HADOOP_DEFAULT_LIBEXEC_DIR="${bin}/../libexec"
+fi
+
+HADOOP_LIBEXEC_DIR="${HADOOP_LIBEXEC_DIR:-$HADOOP_DEFAULT_LIBEXEC_DIR}"
+HADOOP_NEW_CONFIG=true
+if [[ -f "${HADOOP_LIBEXEC_DIR}/hadoop-config.sh" ]]; then
+  # shellcheck source=./hadoop-common-project/hadoop-common/src/main/bin/hadoop-config.sh
+  . "${HADOOP_LIBEXEC_DIR}/hadoop-config.sh"
+else
+  echo "ERROR: Cannot execute ${HADOOP_LIBEXEC_DIR}/hadoop-config.sh." 2>&1
+  exit 1
+fi
+
+# now that we have support code, let's abs MYNAME so we can use it later
+MYNAME=$(hadoop_abs "${MYNAME}")
+
+if [[ $# = 0 ]]; then
+  hadoop_exit_with_usage 1
+fi
+
+HADOOP_SUBCMD=$1
+shift
+
+if hadoop_need_reexec hadoop "${HADOOP_SUBCMD}"; then
+  hadoop_uservar_su hadoop "${HADOOP_SUBCMD}" \
+    "${MYNAME}" \
+    "--reexec" \
+    "${HADOOP_USER_PARAMS[@]}"
+  exit $?
+fi
+
+hadoop_verify_user_perm "${HADOOP_SHELL_EXECNAME}" "${HADOOP_SUBCMD}"
+
+HADOOP_SUBCMD_ARGS=("$@")
+
+if declare -f hadoop_subcommand_"${HADOOP_SUBCMD}" >/dev/null 2>&1; then
+  hadoop_debug "Calling dynamically: hadoop_subcommand_${HADOOP_SUBCMD} ${HADOOP_SUBCMD_ARGS[*]}"
+  "hadoop_subcommand_${HADOOP_SUBCMD}" "${HADOOP_SUBCMD_ARGS[@]}"
+else
+  hadoopcmd_case "${HADOOP_SUBCMD}" "${HADOOP_SUBCMD_ARGS[@]}"
+fi
+
+hadoop_add_client_opts
+
+if [[ ${HADOOP_WORKER_MODE} = true ]]; then
+  hadoop_common_worker_mode_execute "${HADOOP_COMMON_HOME}/bin/hadoop" "${HADOOP_USER_PARAMS[@]}"
+  exit $?
+fi
+
+hadoop_subcommand_opts "${HADOOP_SHELL_EXECNAME}" "${HADOOP_SUBCMD}"
+
+# everything is in globals at this point, so call the generic handler
+hadoop_generic_java_subcmd_handler
diff -pruN hadoop-3.1.0-orig/bin/hdfs hadoop-3.1.0/bin/hdfs
--- hadoop-3.1.0-orig/bin/hdfs	2018-03-29 17:03:01.000000000 -0700
+++ hadoop-3.1.0/bin/hdfs	2018-07-12 17:12:33.771253000 -0700
@@ -267,6 +267,21 @@ fi
 hadoop_add_client_opts
 
 if [[ ${HADOOP_WORKER_MODE} = true ]]; then
+  if [ "${HADOOP_CONF_DIR}X" != "X" ] && [ "${HADOOP_CONF_DIR_CONFIG_ORIG}X" != "X" ]
+  then
+    export HADOOP_CONF_DIR="${HADOOP_CONF_DIR_CONFIG_ORIG}"
+  fi
+
+  if [ "${HADOOP_LOG_DIR}X" != "X" ] && [ "${HADOOP_LOG_DIR_CONFIG_ORIG}X" != "X" ]
+  then
+    export HADOOP_LOG_DIR="${HADOOP_LOG_DIR_CONFIG_ORIG}"
+  fi
+
+  if [ "${HADOOP_PID_DIR}X" != "X" ] && [ "${HADOOP_PID_DIR_CONFIG_ORIG}X" != "X" ]
+  then
+    export HADOOP_PID_DIR="${HADOOP_PID_DIR_CONFIG_ORIG}"
+  fi
+
   hadoop_common_worker_mode_execute "${HADOOP_HDFS_HOME}/bin/hdfs" "${HADOOP_USER_PARAMS[@]}"
   exit $?
 fi
diff -pruN hadoop-3.1.0-orig/bin/hdfs.orig hadoop-3.1.0/bin/hdfs.orig
--- hadoop-3.1.0-orig/bin/hdfs.orig	1969-12-31 16:00:00.000000000 -0800
+++ hadoop-3.1.0/bin/hdfs.orig	2018-03-29 17:03:01.000000000 -0700
@@ -0,0 +1,277 @@
+#!/usr/bin/env bash
+
+# Licensed to the Apache Software Foundation (ASF) under one or more
+# contributor license agreements.  See the NOTICE file distributed with
+# this work for additional information regarding copyright ownership.
+# The ASF licenses this file to You under the Apache License, Version 2.0
+# (the "License"); you may not use this file except in compliance with
+# the License.  You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+# The name of the script being executed.
+HADOOP_SHELL_EXECNAME="hdfs"
+MYNAME="${BASH_SOURCE-$0}"
+
+## @description  build up the hdfs command's usage text.
+## @audience     public
+## @stability    stable
+## @replaceable  no
+function hadoop_usage
+{
+  hadoop_add_option "--buildpaths" "attempt to add class files from build tree"
+  hadoop_add_option "--daemon (start|status|stop)" "operate on a daemon"
+  hadoop_add_option "--hostnames list[,of,host,names]" "hosts to use in worker mode"
+  hadoop_add_option "--loglevel level" "set the log4j level for this command"
+  hadoop_add_option "--hosts filename" "list of hosts to use in worker mode"
+  hadoop_add_option "--workers" "turn on worker mode"
+
+  hadoop_add_subcommand "balancer" daemon "run a cluster balancing utility"
+  hadoop_add_subcommand "cacheadmin" admin "configure the HDFS cache"
+  hadoop_add_subcommand "classpath" client "prints the class path needed to get the hadoop jar and the required libraries"
+  hadoop_add_subcommand "crypto" admin "configure HDFS encryption zones"
+  hadoop_add_subcommand "datanode" daemon "run a DFS datanode"
+  hadoop_add_subcommand "debug" admin "run a Debug Admin to execute HDFS debug commands"
+  hadoop_add_subcommand "dfs" client "run a filesystem command on the file system"
+  hadoop_add_subcommand "dfsadmin" admin "run a DFS admin client"
+  hadoop_add_subcommand "dfsrouter" daemon "run the DFS router"
+  hadoop_add_subcommand "dfsrouteradmin" admin "manage Router-based federation"
+  hadoop_add_subcommand "diskbalancer" daemon "Distributes data evenly among disks on a given node"
+  hadoop_add_subcommand "envvars" client "display computed Hadoop environment variables"
+  hadoop_add_subcommand "ec" admin "run a HDFS ErasureCoding CLI"
+  hadoop_add_subcommand "fetchdt" client "fetch a delegation token from the NameNode"
+  hadoop_add_subcommand "fsck" admin "run a DFS filesystem checking utility"
+  hadoop_add_subcommand "getconf" client "get config values from configuration"
+  hadoop_add_subcommand "groups" client "get the groups which users belong to"
+  hadoop_add_subcommand "haadmin" admin "run a DFS HA admin client"
+  hadoop_add_subcommand "jmxget" admin "get JMX exported values from NameNode or DataNode."
+  hadoop_add_subcommand "journalnode" daemon "run the DFS journalnode"
+  hadoop_add_subcommand "lsSnapshottableDir" client "list all snapshottable dirs owned by the current user"
+  hadoop_add_subcommand "mover" daemon "run a utility to move block replicas across storage types"
+  hadoop_add_subcommand "namenode" daemon "run the DFS namenode"
+  hadoop_add_subcommand "nfs3" daemon "run an NFS version 3 gateway"
+  hadoop_add_subcommand "oev" admin "apply the offline edits viewer to an edits file"
+  hadoop_add_subcommand "oiv" admin "apply the offline fsimage viewer to an fsimage"
+  hadoop_add_subcommand "oiv_legacy" admin "apply the offline fsimage viewer to a legacy fsimage"
+  hadoop_add_subcommand "portmap" daemon "run a portmap service"
+  hadoop_add_subcommand "secondarynamenode" daemon "run the DFS secondary namenode"
+  hadoop_add_subcommand "snapshotDiff" client "diff two snapshots of a directory or diff the current directory contents with a snapshot"
+  hadoop_add_subcommand "storagepolicies" admin "list/get/set block storage policies"
+  hadoop_add_subcommand "version" client "print the version"
+  hadoop_add_subcommand "zkfc" daemon "run the ZK Failover Controller daemon"
+  hadoop_generate_usage "${HADOOP_SHELL_EXECNAME}" false
+}
+
+## @description  Default command handler for hadoop command
+## @audience     public
+## @stability    stable
+## @replaceable  no
+## @param        CLI arguments
+function hdfscmd_case
+{
+  subcmd=$1
+  shift
+
+  case ${subcmd} in
+    balancer)
+      HADOOP_SUBCMD_SUPPORTDAEMONIZATION="true"
+      HADOOP_CLASSNAME=org.apache.hadoop.hdfs.server.balancer.Balancer
+    ;;
+    cacheadmin)
+      HADOOP_CLASSNAME=org.apache.hadoop.hdfs.tools.CacheAdmin
+    ;;
+    classpath)
+      hadoop_do_classpath_subcommand HADOOP_CLASSNAME "$@"
+    ;;
+    crypto)
+      HADOOP_CLASSNAME=org.apache.hadoop.hdfs.tools.CryptoAdmin
+    ;;
+    datanode)
+      HADOOP_SUBCMD_SUPPORTDAEMONIZATION="true"
+      HADOOP_SECURE_CLASSNAME="org.apache.hadoop.hdfs.server.datanode.SecureDataNodeStarter"
+      HADOOP_CLASSNAME='org.apache.hadoop.hdfs.server.datanode.DataNode'
+      hadoop_deprecate_envvar HADOOP_SECURE_DN_PID_DIR HADOOP_SECURE_PID_DIR
+      hadoop_deprecate_envvar HADOOP_SECURE_DN_LOG_DIR HADOOP_SECURE_LOG_DIR
+    ;;
+    debug)
+      HADOOP_CLASSNAME='org.apache.hadoop.hdfs.tools.DebugAdmin'
+    ;;
+    dfs)
+      HADOOP_CLASSNAME=org.apache.hadoop.fs.FsShell
+    ;;
+    dfsadmin)
+      HADOOP_CLASSNAME=org.apache.hadoop.hdfs.tools.DFSAdmin
+    ;;
+    dfsrouter)
+      HADOOP_SUBCMD_SUPPORTDAEMONIZATION="true"
+      HADOOP_CLASSNAME='org.apache.hadoop.hdfs.server.federation.router.DFSRouter'
+    ;;
+    dfsrouteradmin)
+      HADOOP_CLASSNAME='org.apache.hadoop.hdfs.tools.federation.RouterAdmin'
+    ;;
+    diskbalancer)
+      HADOOP_CLASSNAME=org.apache.hadoop.hdfs.tools.DiskBalancerCLI
+    ;;
+    envvars)
+      echo "JAVA_HOME='${JAVA_HOME}'"
+      echo "HADOOP_HDFS_HOME='${HADOOP_HDFS_HOME}'"
+      echo "HDFS_DIR='${HDFS_DIR}'"
+      echo "HDFS_LIB_JARS_DIR='${HDFS_LIB_JARS_DIR}'"
+      echo "HADOOP_CONF_DIR='${HADOOP_CONF_DIR}'"
+      echo "HADOOP_TOOLS_HOME='${HADOOP_TOOLS_HOME}'"
+      echo "HADOOP_TOOLS_DIR='${HADOOP_TOOLS_DIR}'"
+      echo "HADOOP_TOOLS_LIB_JARS_DIR='${HADOOP_TOOLS_LIB_JARS_DIR}'"
+      if [[ -n "${QATESTMODE}" ]]; then
+        echo "MYNAME=${MYNAME}"
+        echo "HADOOP_SHELL_EXECNAME=${HADOOP_SHELL_EXECNAME}"
+      fi
+      exit 0
+    ;;
+    ec)
+      HADOOP_CLASSNAME=org.apache.hadoop.hdfs.tools.ECAdmin
+    ;;
+    fetchdt)
+      HADOOP_CLASSNAME=org.apache.hadoop.hdfs.tools.DelegationTokenFetcher
+    ;;
+    fsck)
+      HADOOP_CLASSNAME=org.apache.hadoop.hdfs.tools.DFSck
+    ;;
+    getconf)
+      HADOOP_CLASSNAME=org.apache.hadoop.hdfs.tools.GetConf
+    ;;
+    groups)
+      HADOOP_CLASSNAME=org.apache.hadoop.hdfs.tools.GetGroups
+    ;;
+    haadmin)
+      HADOOP_CLASSNAME=org.apache.hadoop.hdfs.tools.DFSHAAdmin
+    ;;
+    journalnode)
+      HADOOP_SUBCMD_SUPPORTDAEMONIZATION="true"
+      HADOOP_CLASSNAME='org.apache.hadoop.hdfs.qjournal.server.JournalNode'
+    ;;
+    jmxget)
+      HADOOP_CLASSNAME=org.apache.hadoop.hdfs.tools.JMXGet
+    ;;
+    lsSnapshottableDir)
+      HADOOP_CLASSNAME=org.apache.hadoop.hdfs.tools.snapshot.LsSnapshottableDir
+    ;;
+    mover)
+      HADOOP_SUBCMD_SUPPORTDAEMONIZATION="true"
+      HADOOP_CLASSNAME=org.apache.hadoop.hdfs.server.mover.Mover
+    ;;
+    namenode)
+      HADOOP_SUBCMD_SUPPORTDAEMONIZATION="true"
+      HADOOP_CLASSNAME='org.apache.hadoop.hdfs.server.namenode.NameNode'
+      hadoop_add_param HADOOP_OPTS hdfs.audit.logger "-Dhdfs.audit.logger=${HDFS_AUDIT_LOGGER}"
+    ;;
+    nfs3)
+      HADOOP_SUBCMD_SUPPORTDAEMONIZATION="true"
+      HADOOP_SECURE_CLASSNAME=org.apache.hadoop.hdfs.nfs.nfs3.PrivilegedNfsGatewayStarter
+      HADOOP_CLASSNAME=org.apache.hadoop.hdfs.nfs.nfs3.Nfs3
+      hadoop_deprecate_envvar HADOOP_SECURE_NFS3_LOG_DIR HADOOP_SECURE_LOG_DIR
+      hadoop_deprecate_envvar HADOOP_SECURE_NFS3_PID_DIR HADOOP_SECURE_PID_DIR
+    ;;
+    oev)
+      HADOOP_CLASSNAME=org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsViewer
+    ;;
+    oiv)
+      HADOOP_CLASSNAME=org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageViewerPB
+    ;;
+    oiv_legacy)
+      HADOOP_CLASSNAME=org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageViewer
+    ;;
+    portmap)
+      HADOOP_SUBCMD_SUPPORTDAEMONIZATION="true"
+      HADOOP_CLASSNAME=org.apache.hadoop.portmap.Portmap
+    ;;
+    secondarynamenode)
+      HADOOP_SUBCMD_SUPPORTDAEMONIZATION="true"
+      HADOOP_CLASSNAME='org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode'
+      hadoop_add_param HADOOP_OPTS hdfs.audit.logger "-Dhdfs.audit.logger=${HDFS_AUDIT_LOGGER}"
+    ;;
+    snapshotDiff)
+      HADOOP_CLASSNAME=org.apache.hadoop.hdfs.tools.snapshot.SnapshotDiff
+    ;;
+    storagepolicies)
+      HADOOP_CLASSNAME=org.apache.hadoop.hdfs.tools.StoragePolicyAdmin
+    ;;
+    version)
+      HADOOP_CLASSNAME=org.apache.hadoop.util.VersionInfo
+    ;;
+    zkfc)
+      HADOOP_SUBCMD_SUPPORTDAEMONIZATION="true"
+      HADOOP_CLASSNAME='org.apache.hadoop.hdfs.tools.DFSZKFailoverController'
+    ;;
+    *)
+      HADOOP_CLASSNAME="${subcmd}"
+      if ! hadoop_validate_classname "${HADOOP_CLASSNAME}"; then
+        hadoop_exit_with_usage 1
+      fi
+    ;;
+  esac
+}
+
+# let's locate libexec...
+if [[ -n "${HADOOP_HOME}" ]]; then
+  HADOOP_DEFAULT_LIBEXEC_DIR="${HADOOP_HOME}/libexec"
+else
+  bin=$(cd -P -- "$(dirname -- "${MYNAME}")" >/dev/null && pwd -P)
+  HADOOP_DEFAULT_LIBEXEC_DIR="${bin}/../libexec"
+fi
+
+HADOOP_LIBEXEC_DIR="${HADOOP_LIBEXEC_DIR:-$HADOOP_DEFAULT_LIBEXEC_DIR}"
+HADOOP_NEW_CONFIG=true
+if [[ -f "${HADOOP_LIBEXEC_DIR}/hdfs-config.sh" ]]; then
+  # shellcheck source=./hadoop-hdfs-project/hadoop-hdfs/src/main/bin/hdfs-config.sh
+  . "${HADOOP_LIBEXEC_DIR}/hdfs-config.sh"
+else
+  echo "ERROR: Cannot execute ${HADOOP_LIBEXEC_DIR}/hdfs-config.sh." 2>&1
+  exit 1
+fi
+
+# now that we have support code, let's abs MYNAME so we can use it later
+MYNAME=$(hadoop_abs "${MYNAME}")
+
+if [[ $# = 0 ]]; then
+  hadoop_exit_with_usage 1
+fi
+
+HADOOP_SUBCMD=$1
+shift
+
+if hadoop_need_reexec hdfs "${HADOOP_SUBCMD}"; then
+  hadoop_uservar_su hdfs "${HADOOP_SUBCMD}" \
+    "${MYNAME}" \
+    "--reexec" \
+    "${HADOOP_USER_PARAMS[@]}"
+  exit $?
+fi
+
+hadoop_verify_user_perm "${HADOOP_SHELL_EXECNAME}" "${HADOOP_SUBCMD}"
+
+HADOOP_SUBCMD_ARGS=("$@")
+
+if declare -f hdfs_subcommand_"${HADOOP_SUBCMD}" >/dev/null 2>&1; then
+  hadoop_debug "Calling dynamically: hdfs_subcommand_${HADOOP_SUBCMD} ${HADOOP_SUBCMD_ARGS[*]}"
+  "hdfs_subcommand_${HADOOP_SUBCMD}" "${HADOOP_SUBCMD_ARGS[@]}"
+else
+  hdfscmd_case "${HADOOP_SUBCMD}" "${HADOOP_SUBCMD_ARGS[@]}"
+fi
+
+hadoop_add_client_opts
+
+if [[ ${HADOOP_WORKER_MODE} = true ]]; then
+  hadoop_common_worker_mode_execute "${HADOOP_HDFS_HOME}/bin/hdfs" "${HADOOP_USER_PARAMS[@]}"
+  exit $?
+fi
+
+hadoop_subcommand_opts "${HADOOP_SHELL_EXECNAME}" "${HADOOP_SUBCMD}"
+
+# everything is in globals at this point, so call the generic handler
+hadoop_generic_java_subcmd_handler
diff -pruN hadoop-3.1.0-orig/bin/mapred hadoop-3.1.0/bin/mapred
--- hadoop-3.1.0-orig/bin/mapred	2018-03-29 17:14:50.000000000 -0700
+++ hadoop-3.1.0/bin/mapred	2018-07-12 17:12:33.778256000 -0700
@@ -167,6 +167,21 @@ fi
 hadoop_add_client_opts
 
 if [[ ${HADOOP_WORKER_MODE} = true ]]; then
+  if [ "${HADOOP_CONF_DIR}X" != "X" ] && [ "${HADOOP_CONF_DIR_CONFIG_ORIG}X" != "X" ]
+  then
+    export HADOOP_CONF_DIR="${HADOOP_CONF_DIR_CONFIG_ORIG}"
+  fi
+
+  if [ "${HADOOP_LOG_DIR}X" != "X" ] && [ "${HADOOP_LOG_DIR_CONFIG_ORIG}X" != "X" ]
+  then
+    export HADOOP_LOG_DIR="${HADOOP_LOG_DIR_CONFIG_ORIG}"
+  fi
+
+  if [ "${HADOOP_PID_DIR}X" != "X" ] && [ "${HADOOP_PID_DIR_CONFIG_ORIG}X" != "X" ]
+  then
+    export HADOOP_PID_DIR="${HADOOP_PID_DIR_CONFIG_ORIG}"
+  fi
+
   hadoop_common_worker_mode_execute "${HADOOP_MAPRED_HOME}/bin/mapred" "${HADOOP_USER_PARAMS[@]}"
   exit $?
 fi
diff -pruN hadoop-3.1.0-orig/bin/mapred.orig hadoop-3.1.0/bin/mapred.orig
--- hadoop-3.1.0-orig/bin/mapred.orig	1969-12-31 16:00:00.000000000 -0800
+++ hadoop-3.1.0/bin/mapred.orig	2018-03-29 17:14:50.000000000 -0700
@@ -0,0 +1,177 @@
+#!/usr/bin/env bash
+
+# Licensed to the Apache Software Foundation (ASF) under one or more
+# contributor license agreements.  See the NOTICE file distributed with
+# this work for additional information regarding copyright ownership.
+# The ASF licenses this file to You under the Apache License, Version 2.0
+# (the "License"); you may not use this file except in compliance with
+# the License.  You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+# The name of the script being executed.
+HADOOP_SHELL_EXECNAME="mapred"
+MYNAME="${BASH_SOURCE-$0}"
+
+
+## @description  build up the mapred command's usage text.
+## @audience     public
+## @stability    stable
+## @replaceable  no
+function hadoop_usage
+{
+  hadoop_add_subcommand "classpath" client "prints the class path needed for running mapreduce subcommands"
+  hadoop_add_subcommand "envvars" client "display computed Hadoop environment variables"
+  hadoop_add_subcommand "historyserver" daemon "run job history servers as a standalone daemon"
+  hadoop_add_subcommand "hsadmin" admin "job history server admin interface"
+  hadoop_add_subcommand "job" client "manipulate MapReduce jobs"
+  hadoop_add_subcommand "pipes" client "run a Pipes job"
+  hadoop_add_subcommand "queue" client "get information regarding JobQueues"
+  hadoop_add_subcommand "sampler" client "sampler"
+  hadoop_add_subcommand "frameworkuploader" admin "mapreduce framework upload"
+  hadoop_add_subcommand "version" client "print the version"
+  hadoop_add_subcommand "minicluster" client "CLI MiniCluster"
+  hadoop_generate_usage "${HADOOP_SHELL_EXECNAME}" true
+}
+
+## @description  Default command handler for hadoop command
+## @audience     public
+## @stability    stable
+## @replaceable  no
+## @param        CLI arguments
+function mapredcmd_case
+{
+  subcmd=$1
+  shift
+
+  case ${subcmd} in
+    mradmin|jobtracker|tasktracker|groups)
+      hadoop_error "Sorry, the ${subcmd} command is no longer supported."
+      hadoop_error "You may find similar functionality with the \"yarn\" shell command."
+      hadoop_exit_with_usage 1
+    ;;
+    classpath)
+      hadoop_do_classpath_subcommand HADOOP_CLASSNAME "$@"
+    ;;
+    envvars)
+      echo "JAVA_HOME='${JAVA_HOME}'"
+      echo "HADOOP_MAPRED_HOME='${HADOOP_MAPRED_HOME}'"
+      echo "MAPRED_DIR='${MAPRED_DIR}'"
+      echo "MAPRED_LIB_JARS_DIR='${MAPRED_LIB_JARS_DIR}'"
+      echo "HADOOP_CONF_DIR='${HADOOP_CONF_DIR}'"
+      echo "HADOOP_TOOLS_HOME='${HADOOP_TOOLS_HOME}'"
+      echo "HADOOP_TOOLS_DIR='${HADOOP_TOOLS_DIR}'"
+      echo "HADOOP_TOOLS_LIB_JARS_DIR='${HADOOP_TOOLS_LIB_JARS_DIR}'"
+      exit 0
+    ;;
+    historyserver)
+      HADOOP_SUBCMD_SUPPORTDAEMONIZATION="true"
+      HADOOP_CLASSNAME=org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer
+      if [[ -n "${HADOOP_JOB_HISTORYSERVER_HEAPSIZE}" ]]; then
+        HADOOP_HEAPSIZE_MAX="${HADOOP_JOB_HISTORYSERVER_HEAPSIZE}"
+      fi
+      HADOOP_DAEMON_ROOT_LOGGER=${HADOOP_JHS_LOGGER:-$HADOOP_DAEMON_ROOT_LOGGER}
+      if [[  "${HADOOP_DAEMON_MODE}" != "default" ]]; then
+        hadoop_add_param HADOOP_OPTS mapred.jobsummary.logger "-Dmapred.jobsummary.logger=${HADOOP_DAEMON_ROOT_LOGGER}"
+      fi
+    ;;
+    hsadmin)
+      HADOOP_CLASSNAME=org.apache.hadoop.mapreduce.v2.hs.client.HSAdmin
+    ;;
+    job)
+      HADOOP_CLASSNAME=org.apache.hadoop.mapred.JobClient
+    ;;
+    pipes)
+      HADOOP_CLASSNAME=org.apache.hadoop.mapred.pipes.Submitter
+    ;;
+    queue)
+      HADOOP_CLASSNAME=org.apache.hadoop.mapred.JobQueueClient
+    ;;
+    sampler)
+      HADOOP_CLASSNAME=org.apache.hadoop.mapred.lib.InputSampler
+    ;;
+    frameworkuploader)
+      HADOOP_CLASSNAME=org.apache.hadoop.mapred.uploader.FrameworkUploader
+    ;;
+    version)
+      HADOOP_CLASSNAME=org.apache.hadoop.util.VersionInfo
+    ;;
+    minicluster)
+      hadoop_add_classpath "${HADOOP_YARN_HOME}/${YARN_DIR}/timelineservice"'/*'
+      hadoop_add_classpath "${HADOOP_YARN_HOME}/${YARN_DIR}/test"'/*'
+      HADOOP_CLASSNAME=org.apache.hadoop.mapreduce.MiniHadoopClusterManager
+    ;;
+    *)
+      HADOOP_CLASSNAME="${subcmd}"
+      if ! hadoop_validate_classname "${HADOOP_CLASSNAME}"; then
+        hadoop_exit_with_usage 1
+      fi
+    ;;
+  esac
+}
+
+bin=$(cd -P -- "$(dirname -- "${MYNAME}")" >/dev/null && pwd -P)
+
+# let's locate libexec...
+if [[ -n "${HADOOP_HOME}" ]]; then
+  HADOOP_DEFAULT_LIBEXEC_DIR="${HADOOP_HOME}/libexec"
+else
+  HADOOP_DEFAULT_LIBEXEC_DIR="${bin}/../libexec"
+fi
+
+HADOOP_LIBEXEC_DIR="${HADOOP_LIBEXEC_DIR:-$HADOOP_DEFAULT_LIBEXEC_DIR}"
+HADOOP_NEW_CONFIG=true
+if [[ -f "${HADOOP_LIBEXEC_DIR}/mapred-config.sh" ]]; then
+  # shellcheck source=./hadoop-mapreduce-project/bin/mapred-config.sh
+  . "${HADOOP_LIBEXEC_DIR}/mapred-config.sh"
+else
+  echo "ERROR: Cannot execute ${HADOOP_LIBEXEC_DIR}/mapred-config.sh." 2>&1
+  exit 1
+fi
+
+# now that we have support code, let's abs MYNAME so we can use it later
+MYNAME=$(hadoop_abs "${MYNAME}")
+
+if [ $# = 0 ]; then
+  hadoop_exit_with_usage 1
+fi
+
+HADOOP_SUBCMD=$1
+shift
+
+if hadoop_need_reexec mapred "${HADOOP_SUBCMD}"; then
+  hadoop_uservar_su mapred "${HADOOP_SUBCMD}" \
+    "${MYNAME}" \
+    "--reexec" \
+    "${HADOOP_USER_PARAMS[@]}"
+  exit $?
+fi
+
+hadoop_verify_user_perm "${HADOOP_SHELL_EXECNAME}" "${HADOOP_SUBCMD}"
+
+HADOOP_SUBCMD_ARGS=("$@")
+
+if declare -f mapred_subcommand_"${HADOOP_SUBCMD}" >/dev/null 2>&1; then
+  hadoop_debug "Calling dynamically: mapred_subcommand_${HADOOP_SUBCMD} ${HADOOP_SUBCMD_ARGS[*]}"
+  "mapred_subcommand_${HADOOP_SUBCMD}" "${HADOOP_SUBCMD_ARGS[@]}"
+else
+  mapredcmd_case "${HADOOP_SUBCMD}" "${HADOOP_SUBCMD_ARGS[@]}"
+fi
+
+hadoop_add_client_opts
+
+if [[ ${HADOOP_WORKER_MODE} = true ]]; then
+  hadoop_common_worker_mode_execute "${HADOOP_MAPRED_HOME}/bin/mapred" "${HADOOP_USER_PARAMS[@]}"
+  exit $?
+fi
+
+hadoop_subcommand_opts "${HADOOP_SHELL_EXECNAME}" "${HADOOP_SUBCMD}"
+
+# everything is in globals at this point, so call the generic handler
+hadoop_generic_java_subcmd_handler
diff -pruN hadoop-3.1.0-orig/bin/yarn hadoop-3.1.0/bin/yarn
--- hadoop-3.1.0-orig/bin/yarn	2018-03-29 17:13:57.000000000 -0700
+++ hadoop-3.1.0/bin/yarn	2018-07-12 17:12:33.786246000 -0700
@@ -297,6 +297,21 @@ fi
 hadoop_add_client_opts
 
 if [[ ${HADOOP_WORKER_MODE} = true ]]; then
+  if [ "${HADOOP_CONF_DIR}X" != "X" ] && [ "${HADOOP_CONF_DIR_CONFIG_ORIG}X" != "X" ]
+  then
+    export HADOOP_CONF_DIR="${HADOOP_CONF_DIR_CONFIG_ORIG}"
+  fi
+
+  if [ "${HADOOP_LOG_DIR}X" != "X" ] && [ "${HADOOP_LOG_DIR_CONFIG_ORIG}X" != "X" ]
+  then
+    export HADOOP_LOG_DIR="${HADOOP_LOG_DIR_CONFIG_ORIG}"
+  fi
+
+  if [ "${HADOOP_PID_DIR}X" != "X" ] && [ "${HADOOP_PID_DIR_CONFIG_ORIG}X" != "X" ]
+  then
+    export HADOOP_PID_DIR="${HADOOP_PID_DIR_CONFIG_ORIG}"
+  fi
+
   hadoop_common_worker_mode_execute "${HADOOP_YARN_HOME}/bin/yarn" "${HADOOP_USER_PARAMS[@]}"
   exit $?
 fi
diff -pruN hadoop-3.1.0-orig/bin/yarn.orig hadoop-3.1.0/bin/yarn.orig
--- hadoop-3.1.0-orig/bin/yarn.orig	1969-12-31 16:00:00.000000000 -0800
+++ hadoop-3.1.0/bin/yarn.orig	2018-03-29 17:13:57.000000000 -0700
@@ -0,0 +1,307 @@
+#!/usr/bin/env bash
+
+# Licensed to the Apache Software Foundation (ASF) under one or more
+# contributor license agreements.  See the NOTICE file distributed with
+# this work for additional information regarding copyright ownership.
+# The ASF licenses this file to You under the Apache License, Version 2.0
+# (the "License"); you may not use this file except in compliance with
+# the License.  You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+# The name of the script being executed.
+HADOOP_SHELL_EXECNAME="yarn"
+MYNAME="${BASH_SOURCE-$0}"
+
+## @description  build up the yarn command's usage text.
+## @audience     public
+## @stability    stable
+## @replaceable  no
+function hadoop_usage
+{
+  hadoop_add_option "--buildpaths" "attempt to add class files from build tree"
+  hadoop_add_option "--daemon (start|status|stop)" "operate on a daemon"
+  hadoop_add_option "--hostnames list[,of,host,names]" "hosts to use in worker mode"
+  hadoop_add_option "--loglevel level" "set the log4j level for this command"
+  hadoop_add_option "--hosts filename" "list of hosts to use in worker mode"
+  hadoop_add_option "--workers" "turn on worker mode"
+
+  hadoop_add_subcommand "app|application" client "prints application(s) report/kill application/manage long running application"
+  hadoop_add_subcommand "applicationattempt" client "prints applicationattempt(s) report"
+  hadoop_add_subcommand "classpath" client "prints the class path needed to get the hadoop jar and the required libraries"
+  hadoop_add_subcommand "cluster" client "prints cluster information"
+  hadoop_add_subcommand "container" client "prints container(s) report"
+  hadoop_add_subcommand "daemonlog" admin "get/set the log level for each daemon"
+  hadoop_add_subcommand "envvars" client "display computed Hadoop environment variables"
+  hadoop_add_subcommand "jar <jar>" client "run a jar file"
+  hadoop_add_subcommand "logs" client "dump container logs"
+  hadoop_add_subcommand "node" admin "prints node report(s)"
+  hadoop_add_subcommand "nodemanager" daemon "run a nodemanager on each worker"
+  hadoop_add_subcommand "proxyserver" daemon "run the web app proxy server"
+  hadoop_add_subcommand "queue" client "prints queue information"
+  hadoop_add_subcommand "registrydns" daemon "run the registry DNS server"
+  hadoop_add_subcommand "resourcemanager" daemon "run the ResourceManager"
+  hadoop_add_subcommand "rmadmin" admin "admin tools"
+  hadoop_add_subcommand "router" daemon "run the Router daemon"
+  hadoop_add_subcommand "schedulerconf" client "Updates scheduler configuration"
+  hadoop_add_subcommand "scmadmin" admin "SharedCacheManager admin tools"
+  hadoop_add_subcommand "sharedcachemanager" daemon "run the SharedCacheManager daemon"
+  hadoop_add_subcommand "timelinereader" client "run the timeline reader server"
+  hadoop_add_subcommand "timelineserver" daemon "run the timeline server"
+  hadoop_add_subcommand "top" client "view cluster information"
+  hadoop_add_subcommand "version" client "print the version"
+  hadoop_generate_usage "${HADOOP_SHELL_EXECNAME}" true
+}
+
+## @description  Default command handler for yarn command
+## @audience     public
+## @stability    stable
+## @replaceable  no
+## @param        CLI arguments
+function yarncmd_case
+{
+  subcmd=$1
+  shift
+
+  case ${subcmd} in
+    app|application|applicationattempt|container)
+      HADOOP_CLASSNAME=org.apache.hadoop.yarn.client.cli.ApplicationCLI
+      set -- "${subcmd}" "$@"
+      HADOOP_SUBCMD_ARGS=("$@")
+      local sld="${HADOOP_YARN_HOME}/${YARN_DIR},\
+${HADOOP_YARN_HOME}/${YARN_LIB_JARS_DIR},\
+${HADOOP_HDFS_HOME}/${HDFS_DIR},\
+${HADOOP_HDFS_HOME}/${HDFS_LIB_JARS_DIR},\
+${HADOOP_COMMON_HOME}/${HADOOP_COMMON_DIR},\
+${HADOOP_COMMON_HOME}/${HADOOP_COMMON_LIB_JARS_DIR}"
+      hadoop_translate_cygwin_path sld
+      hadoop_add_param HADOOP_OPTS service.libdir "-Dservice.libdir=${sld}"
+    ;;
+    classpath)
+      hadoop_do_classpath_subcommand HADOOP_CLASSNAME "$@"
+    ;;
+    cluster)
+      HADOOP_CLASSNAME=org.apache.hadoop.yarn.client.cli.ClusterCLI
+    ;;
+    daemonlog)
+      HADOOP_CLASSNAME=org.apache.hadoop.log.LogLevel
+    ;;
+    envvars)
+      echo "JAVA_HOME='${JAVA_HOME}'"
+      echo "HADOOP_YARN_HOME='${HADOOP_YARN_HOME}'"
+      echo "YARN_DIR='${YARN_DIR}'"
+      echo "YARN_LIB_JARS_DIR='${YARN_LIB_JARS_DIR}'"
+      echo "HADOOP_CONF_DIR='${HADOOP_CONF_DIR}'"
+      echo "HADOOP_TOOLS_HOME='${HADOOP_TOOLS_HOME}'"
+      echo "HADOOP_TOOLS_DIR='${HADOOP_TOOLS_DIR}'"
+      echo "HADOOP_TOOLS_LIB_JARS_DIR='${HADOOP_TOOLS_LIB_JARS_DIR}'"
+      exit 0
+    ;;
+    jar)
+      HADOOP_CLASSNAME=org.apache.hadoop.util.RunJar
+    ;;
+    historyserver)
+      HADOOP_SUBCMD_SUPPORTDAEMONIZATION="true"
+      echo "DEPRECATED: Use of this command to start the timeline server is deprecated." 1>&2
+      echo "Instead use the timelineserver command for it." 1>&2
+      echo "Starting the History Server anyway..." 1>&2
+      HADOOP_CLASSNAME='org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryServer'
+    ;;
+    logs)
+      HADOOP_CLASSNAME=org.apache.hadoop.yarn.client.cli.LogsCLI
+    ;;
+    node)
+      HADOOP_CLASSNAME=org.apache.hadoop.yarn.client.cli.NodeCLI
+    ;;
+    nodemanager)
+      HADOOP_SUBCMD_SUPPORTDAEMONIZATION="true"
+      hadoop_add_classpath "$HADOOP_YARN_HOME/$YARN_DIR/timelineservice/*"
+      hadoop_add_classpath "$HADOOP_YARN_HOME/$YARN_DIR/timelineservice/lib/*"
+      HADOOP_CLASSNAME='org.apache.hadoop.yarn.server.nodemanager.NodeManager'
+      # Backwards compatibility
+      if [[ -n "${YARN_NODEMANAGER_HEAPSIZE}" ]]; then
+        HADOOP_HEAPSIZE_MAX="${YARN_NODEMANAGER_HEAPSIZE}"
+      fi
+    ;;
+    proxyserver)
+      HADOOP_SUBCMD_SUPPORTDAEMONIZATION="true"
+      HADOOP_CLASSNAME='org.apache.hadoop.yarn.server.webproxy.WebAppProxyServer'
+      # Backwards compatibility
+      if [[ -n "${YARN_PROXYSERVER_HEAPSIZE}" ]]; then
+        HADOOP_HEAPSIZE_MAX="${YARN_PROXYSERVER_HEAPSIZE}"
+      fi
+    ;;
+    queue)
+      HADOOP_CLASSNAME=org.apache.hadoop.yarn.client.cli.QueueCLI
+    ;;
+    registrydns)
+      HADOOP_SUBCMD_SUPPORTDAEMONIZATION="true"
+      HADOOP_SECURE_CLASSNAME='org.apache.hadoop.registry.server.dns.PrivilegedRegistryDNSStarter'
+      HADOOP_CLASSNAME='org.apache.hadoop.registry.server.dns.RegistryDNSServer'
+    ;;
+    resourcemanager)
+      HADOOP_SUBCMD_SUPPORTDAEMONIZATION="true"
+      hadoop_add_classpath "$HADOOP_YARN_HOME/$YARN_DIR/timelineservice/*"
+      hadoop_add_classpath "$HADOOP_YARN_HOME/$YARN_DIR/timelineservice/lib/*"
+      HADOOP_CLASSNAME='org.apache.hadoop.yarn.server.resourcemanager.ResourceManager'
+      # Backwards compatibility
+      if [[ -n "${YARN_RESOURCEMANAGER_HEAPSIZE}" ]]; then
+        HADOOP_HEAPSIZE_MAX="${YARN_RESOURCEMANAGER_HEAPSIZE}"
+      fi
+      local sld="${HADOOP_YARN_HOME}/${YARN_DIR},\
+${HADOOP_YARN_HOME}/${YARN_LIB_JARS_DIR},\
+${HADOOP_HDFS_HOME}/${HDFS_DIR},\
+${HADOOP_HDFS_HOME}/${HDFS_LIB_JARS_DIR},\
+${HADOOP_COMMON_HOME}/${HADOOP_COMMON_DIR},\
+${HADOOP_COMMON_HOME}/${HADOOP_COMMON_LIB_JARS_DIR}"
+      hadoop_translate_cygwin_path sld
+      hadoop_add_param HADOOP_OPTS service.libdir "-Dservice.libdir=${sld}"
+    ;;
+    rmadmin)
+      HADOOP_CLASSNAME='org.apache.hadoop.yarn.client.cli.RMAdminCLI'
+    ;;
+    router)
+      HADOOP_SUBCMD_SUPPORTDAEMONIZATION="true"
+      HADOOP_CLASSNAME='org.apache.hadoop.yarn.server.router.Router'
+    ;;
+    schedulerconf)
+    HADOOP_CLASSNAME='org.apache.hadoop.yarn.client.cli.SchedConfCLI'
+    ;;
+    scmadmin)
+      HADOOP_CLASSNAME='org.apache.hadoop.yarn.client.SCMAdmin'
+    ;;
+    sharedcachemanager)
+      HADOOP_SUBCMD_SUPPORTDAEMONIZATION="true"
+      HADOOP_CLASSNAME='org.apache.hadoop.yarn.server.sharedcachemanager.SharedCacheManager'
+    ;;
+    timelinereader)
+      HADOOP_SUBCMD_SUPPORTDAEMONIZATION="true"
+      hadoop_add_classpath "$HADOOP_YARN_HOME/$YARN_DIR/timelineservice/*"
+      hadoop_add_classpath "$HADOOP_YARN_HOME/$YARN_DIR/timelineservice/lib/*"
+      HADOOP_CLASSNAME='org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderServer'
+    ;;
+    timelineserver)
+      HADOOP_SUBCMD_SUPPORTDAEMONIZATION="true"
+      HADOOP_CLASSNAME='org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryServer'
+      # Backwards compatibility
+      if [[ -n "${YARN_TIMELINESERVER_HEAPSIZE}" ]]; then
+        HADOOP_HEAPSIZE_MAX="${YARN_TIMELINESERVER_HEAPSIZE}"
+      fi
+    ;;
+    version)
+      HADOOP_CLASSNAME=org.apache.hadoop.util.VersionInfo
+    ;;
+    top)
+      doNotSetCols=0
+      doNotSetRows=0
+      for i in "$@"; do
+        if [[ $i == "-cols" ]]; then
+          doNotSetCols=1
+        fi
+        if [[ $i == "-rows" ]]; then
+          doNotSetRows=1
+        fi
+      done
+      if [ $doNotSetCols == 0 ] && [ -n "${TERM}" ]; then
+        cols=$(tput cols)
+        if [ -n "$cols" ]; then
+          args=( $@ )
+          args=("${args[@]}" "-cols" "$cols")
+          set -- "${args[@]}"
+        fi
+      fi
+      if [ $doNotSetRows == 0 ] && [ -n "${TERM}" ]; then
+        rows=$(tput lines)
+        if [ -n "$rows" ]; then
+          args=( $@ )
+          args=("${args[@]}" "-rows" "$rows")
+          set -- "${args[@]}"
+        fi
+      fi
+      HADOOP_CLASSNAME=org.apache.hadoop.yarn.client.cli.TopCLI
+      HADOOP_SUBCMD_ARGS=("$@")
+    ;;
+    *)
+      HADOOP_CLASSNAME="${subcmd}"
+      if ! hadoop_validate_classname "${HADOOP_CLASSNAME}"; then
+        hadoop_exit_with_usage 1
+      fi
+    ;;
+  esac
+}
+
+# let's locate libexec...
+if [[ -n "${HADOOP_HOME}" ]]; then
+  HADOOP_DEFAULT_LIBEXEC_DIR="${HADOOP_HOME}/libexec"
+else
+  bin=$(cd -P -- "$(dirname -- "${MYNAME}")" >/dev/null && pwd -P)
+  HADOOP_DEFAULT_LIBEXEC_DIR="${bin}/../libexec"
+fi
+
+HADOOP_LIBEXEC_DIR="${HADOOP_LIBEXEC_DIR:-$HADOOP_DEFAULT_LIBEXEC_DIR}"
+HADOOP_NEW_CONFIG=true
+if [[ -f "${HADOOP_LIBEXEC_DIR}/yarn-config.sh" ]]; then
+  # shellcheck source=./hadoop-yarn-project/hadoop-yarn/bin/yarn-config.sh
+  . "${HADOOP_LIBEXEC_DIR}/yarn-config.sh"
+else
+  echo "ERROR: Cannot execute ${HADOOP_LIBEXEC_DIR}/yarn-config.sh." 2>&1
+  exit 1
+fi
+
+# now that we have support code, let's abs MYNAME so we can use it later
+MYNAME=$(hadoop_abs "${MYNAME}")
+
+# if no args specified, show usage
+if [[ $# = 0 ]]; then
+  hadoop_exit_with_usage 1
+fi
+
+# get arguments
+HADOOP_SUBCMD=$1
+shift
+
+if hadoop_need_reexec yarn "${HADOOP_SUBCMD}"; then
+  hadoop_uservar_su yarn "${HADOOP_SUBCMD}" \
+    "${MYNAME}" \
+    "--reexec" \
+    "${HADOOP_USER_PARAMS[@]}"
+  exit $?
+fi
+
+hadoop_verify_user_perm "${HADOOP_SHELL_EXECNAME}" "${HADOOP_SUBCMD}"
+
+HADOOP_SUBCMD_ARGS=("$@")
+
+if declare -f yarn_subcommand_"${HADOOP_SUBCMD}" >/dev/null 2>&1; then
+  hadoop_debug "Calling dynamically: yarn_subcommand_${HADOOP_SUBCMD} ${HADOOP_SUBCMD_ARGS[*]}"
+  "yarn_subcommand_${HADOOP_SUBCMD}" "${HADOOP_SUBCMD_ARGS[@]}"
+else
+  yarncmd_case "${HADOOP_SUBCMD}" "${HADOOP_SUBCMD_ARGS[@]}"
+fi
+
+# It's unclear if YARN_CLIENT_OPTS is actually a useful
+# thing to have separate from HADOOP_CLIENT_OPTS.  Someone
+# might use it, so let's not deprecate it and just override
+# HADOOP_CLIENT_OPTS instead before we (potentially) add it
+# to the command line
+if [[ -n "${YARN_CLIENT_OPTS}" ]]; then
+  HADOOP_CLIENT_OPTS=${YARN_CLIENT_OPTS}
+fi
+
+hadoop_add_client_opts
+
+if [[ ${HADOOP_WORKER_MODE} = true ]]; then
+  hadoop_common_worker_mode_execute "${HADOOP_YARN_HOME}/bin/yarn" "${HADOOP_USER_PARAMS[@]}"
+  exit $?
+fi
+
+hadoop_subcommand_opts "${HADOOP_SHELL_EXECNAME}" "${HADOOP_SUBCMD}"
+
+# everything is in globals at this point, so call the generic handler
+hadoop_generic_java_subcmd_handler
diff -pruN hadoop-3.1.0-orig/libexec/hadoop-config.sh hadoop-3.1.0/libexec/hadoop-config.sh
--- hadoop-3.1.0-orig/libexec/hadoop-config.sh	2018-03-29 17:01:09.000000000 -0700
+++ hadoop-3.1.0/libexec/hadoop-config.sh	2018-07-12 17:12:33.794250000 -0700
@@ -104,6 +104,23 @@ HADOOP_USER_PARAMS=("$@")
 hadoop_parse_args "$@"
 shift "${HADOOP_PARSE_COUNTER}"
 
+myhostname=`hostname`
+if echo $HADOOP_CONF_DIR | grep -q MAGPIEHOSTNAMESUBSTITUTION
+then
+  export HADOOP_CONF_DIR_CONFIG_ORIG="${HADOOP_CONF_DIR}"
+  HADOOP_CONF_DIR=$(echo "$HADOOP_CONF_DIR_CONFIG_ORIG" | sed "s/MAGPIEHOSTNAMESUBSTITUTION/$myhostname/g")
+fi
+if echo $HADOOP_LOG_DIR | grep -q MAGPIEHOSTNAMESUBSTITUTION
+then
+  export HADOOP_LOG_DIR_CONFIG_ORIG="${HADOOP_LOG_DIR}"
+  HADOOP_LOG_DIR=$(echo "$HADOOP_LOG_DIR_CONFIG_ORIG" | sed "s/MAGPIEHOSTNAMESUBSTITUTION/$myhostname/g")
+fi
+if echo $HADOOP_PID_DIR | grep -q MAGPIEHOSTNAMESUBSTITUTION
+then
+  export HADOOP_PID_DIR_CONFIG_ORIG="${HADOOP_PID_DIR}"
+  HADOOP_PID_DIR=$(echo "$HADOOP_PID_DIR_CONFIG_ORIG" | sed "s/MAGPIEHOSTNAMESUBSTITUTION/$myhostname/g")
+fi
+
 #
 # Setup the base-line environment
 #
diff -pruN hadoop-3.1.0-orig/libexec/hadoop-functions.sh hadoop-3.1.0/libexec/hadoop-functions.sh
--- hadoop-3.1.0-orig/libexec/hadoop-functions.sh	2018-03-29 17:01:09.000000000 -0700
+++ hadoop-3.1.0/libexec/hadoop-functions.sh	2018-07-12 17:12:33.806255000 -0700
@@ -2506,8 +2506,15 @@ function hadoop_parse_args
         confdir=$1
         shift
         ((HADOOP_PARSE_COUNTER=HADOOP_PARSE_COUNTER+2))
+        if echo $confdir | grep -q MAGPIEHOSTNAMESUBSTITUTION
+        then
+            local myhostname=`hostname`
+            orig_confdir="$confdir"
+            confdir=$(echo "$confdir" | sed "s/MAGPIEHOSTNAMESUBSTITUTION/$myhostname/g")
+        fi
         if [[ -d "${confdir}" ]]; then
-          HADOOP_CONF_DIR="${confdir}"
+          export HADOOP_CONF_DIR_CONFIG_ORIG="${orig_confdir}"
+          export HADOOP_CONF_DIR="${confdir}"
         elif [[ -z "${confdir}" ]]; then
           hadoop_error "ERROR: No parameter provided for --config "
           hadoop_exit_with_usage 1
diff -pruN hadoop-3.1.0-orig/sbin/start-dfs.sh hadoop-3.1.0/sbin/start-dfs.sh
--- hadoop-3.1.0-orig/sbin/start-dfs.sh	2018-03-29 17:03:01.000000000 -0700
+++ hadoop-3.1.0/sbin/start-dfs.sh	2018-07-12 17:13:30.342461000 -0700
@@ -40,6 +40,43 @@ function hadoop_usage
   echo "Usage: start-dfs.sh [-upgrade|-rollback] [-clusterId]"
 }
 
+function set_magpie_generic_vars
+{
+    if [ "${HADOOP_CONF_DIR}X" != "X" ] && [ "${HADOOP_CONF_DIR_ORIG_DFS}X" != "X" ]
+    then
+        export HADOOP_CONF_DIR="${HADOOP_CONF_DIR_ORIG_DFS}"
+    fi
+
+    if [ "${HADOOP_LOG_DIR}X" != "X" ] && [ "${HADOOP_LOG_DIR_ORIG_DFS}X" != "X" ]
+    then
+        export HADOOP_LOG_DIR="${HADOOP_LOG_DIR_ORIG_DFS}"
+    fi
+
+    if [ "${HADOOP_PID_DIR}X" != "X" ] && [ "${HADOOP_PID_DIR_ORIG_DFS}X" != "X" ]
+    then
+        export HADOOP_PID_DIR="${HADOOP_PID_DIR_ORIG_DFS}"
+    fi
+}
+
+function set_magpie_specific_vars
+{
+    myhostname=`hostname`
+    if [ "${HADOOP_CONF_DIR}X" != "X" ] && [ "${HADOOP_CONF_DIR_ORIG_DFS}X" != "X" ]
+    then
+        export HADOOP_CONF_DIR=$(echo "$HADOOP_CONF_DIR_ORIG_DFS" | sed "s/MAGPIEHOSTNAMESUBSTITUTION/$myhostname/g")
+    fi
+
+    if [ "${HADOOP_LOG_DIR}X" != "X" ] && [ "${HADOOP_LOG_DIR_ORIG_DFS}X" != "X" ]
+    then
+        export HADOOP_LOG_DIR=$(echo "$HADOOP_LOG_DIR_ORIG_DFS" | sed "s/MAGPIEHOSTNAMESUBSTITUTION/$myhostname/g")
+    fi
+
+    if [ "${HADOOP_PID_DIR}X" != "X" ] && [ "${HADOOP_PID_DIR_ORIG_DFS}X" != "X" ]
+    then
+        export HADOOP_PID_DIR=$(echo "$HADOOP_PID_DIR_ORIG_DFS" | sed "s/MAGPIEHOSTNAMESUBSTITUTION/$myhostname/g")
+    fi
+}
+
 this="${BASH_SOURCE-$0}"
 bin=$(cd -P -- "$(dirname -- "${this}")" >/dev/null && pwd -P)
 
@@ -77,12 +114,27 @@ if [[ $# -ge 1 ]]; then
   esac
 fi
 
+if echo ${HADOOP_CONF_DIR_CONFIG_ORIG} | grep -q MAGPIEHOSTNAMESUBSTITUTION
+then
+    export HADOOP_CONF_DIR_ORIG_DFS="${HADOOP_CONF_DIR_CONFIG_ORIG}"
+fi
+
+if echo ${HADOOP_LOG_DIR_CONFIG_ORIG} | grep -q MAGPIEHOSTNAMESUBSTITUTION
+then
+    export HADOOP_LOG_DIR_ORIG_DFS="${HADOOP_LOG_DIR_CONFIG_ORIG}"
+fi
+
+if echo ${HADOOP_PID_DIR_CONFIG_ORIG} | grep -q MAGPIEHOSTNAMESUBSTITUTION
+then
+    export HADOOP_PID_DIR_ORIG_DFS="${HADOOP_PID_DIR_CONFIG_ORIG}"
+fi
 
 #Add other possible options
 nameStartOpt="$nameStartOpt $*"
 
 #---------------------------------------------------------
 # namenodes
+set_magpie_specific_vars
 
 NAMENODES=$("${HADOOP_HDFS_HOME}/bin/hdfs" getconf -namenodes 2>/dev/null)
 
@@ -91,6 +143,9 @@ if [[ -z "${NAMENODES}" ]]; then
 fi
 
 echo "Starting namenodes on [${NAMENODES}]"
+
+set_magpie_generic_vars
+
 hadoop_uservar_su hdfs namenode "${HADOOP_HDFS_HOME}/bin/hdfs" \
     --workers \
     --config "${HADOOP_CONF_DIR}" \
@@ -104,6 +159,9 @@ HADOOP_JUMBO_RETCOUNTER=$?
 # datanodes (using default workers file)
 
 echo "Starting datanodes"
+
+set_magpie_generic_vars
+
 hadoop_uservar_su hdfs datanode "${HADOOP_HDFS_HOME}/bin/hdfs" \
     --workers \
     --config "${HADOOP_CONF_DIR}" \
@@ -114,6 +172,8 @@ hadoop_uservar_su hdfs datanode "${HADOO
 #---------------------------------------------------------
 # secondary namenodes (if any)
 
+set_magpie_specific_vars
+
 SECONDARY_NAMENODES=$("${HADOOP_HDFS_HOME}/bin/hdfs" getconf -secondarynamenodes 2>/dev/null)
 
 if [[ -n "${SECONDARY_NAMENODES}" ]]; then
@@ -131,6 +191,8 @@ if [[ -n "${SECONDARY_NAMENODES}" ]]; th
 
     echo "Starting secondary namenodes [${SECONDARY_NAMENODES}]"
 
+    set_magpie_generic_vars
+
     hadoop_uservar_su hdfs secondarynamenode "${HADOOP_HDFS_HOME}/bin/hdfs" \
       --workers \
       --config "${HADOOP_CONF_DIR}" \
@@ -144,11 +206,15 @@ fi
 #---------------------------------------------------------
 # quorumjournal nodes (if any)
 
+set_magpie_specific_vars
+
 JOURNAL_NODES=$("${HADOOP_HDFS_HOME}/bin/hdfs" getconf -journalNodes 2>&-)
 
 if [[ "${#JOURNAL_NODES}" != 0 ]]; then
   echo "Starting journal nodes [${JOURNAL_NODES}]"
 
+  set_magpie_specific_vars
+
   hadoop_uservar_su hdfs journalnode "${HADOOP_HDFS_HOME}/bin/hdfs" \
     --workers \
     --config "${HADOOP_CONF_DIR}" \
@@ -160,10 +226,15 @@ fi
 
 #---------------------------------------------------------
 # ZK Failover controllers, if auto-HA is enabled
+
+set_magpie_specific_vars
+
 AUTOHA_ENABLED=$("${HADOOP_HDFS_HOME}/bin/hdfs" getconf -confKey dfs.ha.automatic-failover.enabled | tr '[:upper:]' '[:lower:]')
 if [[ "${AUTOHA_ENABLED}" = "true" ]]; then
   echo "Starting ZK Failover Controllers on NN hosts [${NAMENODES}]"
 
+  set_magpie_generic_vars
+
   hadoop_uservar_su hdfs zkfc "${HADOOP_HDFS_HOME}/bin/hdfs" \
     --workers \
     --config "${HADOOP_CONF_DIR}" \
diff -pruN hadoop-3.1.0-orig/sbin/start-yarn.sh hadoop-3.1.0/sbin/start-yarn.sh
--- hadoop-3.1.0-orig/sbin/start-yarn.sh	2018-03-29 17:13:57.000000000 -0700
+++ hadoop-3.1.0/sbin/start-yarn.sh	2018-07-12 17:12:33.827248000 -0700
@@ -26,6 +26,43 @@ function hadoop_usage
 
 MYNAME="${BASH_SOURCE-$0}"
 
+function set_magpie_generic_vars
+{
+    if [ "${HADOOP_CONF_DIR}X" != "X" ] && [ "${HADOOP_CONF_DIR_ORIG_YARN}X" != "X" ]
+    then
+        export HADOOP_CONF_DIR="${HADOOP_CONF_DIR_ORIG_YARN}"
+    fi
+
+    if [ "${HADOOP_LOG_DIR}X" != "X" ] && [ "${HADOOP_LOG_DIR_ORIG_YARN}X" != "X" ]
+    then
+        export HADOOP_LOG_DIR="${HADOOP_LOG_DIR_ORIG_YARN}"
+    fi
+
+    if [ "${HADOOP_PID_DIR}X" != "X" ] && [ "${HADOOP_PID_DIR_ORIG_YARN}X" != "X" ]
+    then
+        export HADOOP_PID_DIR="${HADOOP_PID_DIR_ORIG_YARN}"
+    fi
+}
+
+function set_magpie_specific_vars
+{
+    myhostname=`hostname`
+    if [ "${HADOOP_CONF_DIR}X" != "X" ] && [ "${HADOOP_CONF_DIR_ORIG_YARN}X" != "X" ]
+    then
+        export HADOOP_CONF_DIR=$(echo "$HADOOP_CONF_DIR_ORIG_YARN" | sed "s/MAGPIEHOSTNAMESUBSTITUTION/$myhostname/g")
+    fi
+
+    if [ "${HADOOP_LOG_DIR}X" != "X" ] && [ "${HADOOP_LOG_DIR_ORIG_YARN}X" != "X" ]
+    then
+        export HADOOP_LOG_DIR=$(echo "$HADOOP_LOG_DIR_ORIG_YARN" | sed "s/MAGPIEHOSTNAMESUBSTITUTION/$myhostname/g")
+    fi
+
+    if [ "${HADOOP_PID_DIR}X" != "X" ] && [ "${HADOOP_PID_DIR_ORIG_YARN}X" != "X" ]
+    then
+        export HADOOP_PID_DIR=$(echo "$HADOOP_PID_DIR_ORIG_YARN" | sed "s/MAGPIEHOSTNAMESUBSTITUTION/$myhostname/g")
+    fi
+}
+
 bin=$(cd -P -- "$(dirname -- "${MYNAME}")" >/dev/null && pwd -P)
 
 # let's locate libexec...
@@ -47,16 +84,34 @@ fi
 
 HADOOP_JUMBO_RETCOUNTER=0
 
+if echo ${HADOOP_CONF_DIR_CONFIG_ORIG} | grep -q MAGPIEHOSTNAMESUBSTITUTION
+then
+    export HADOOP_CONF_DIR_ORIG_YARN="${HADOOP_CONF_DIR_CONFIG_ORIG}"
+fi
+
+if echo ${HADOOP_LOG_DIR_CONFIG_ORIG} | grep -q MAGPIEHOSTNAMESUBSTITUTION
+then
+    export HADOOP_LOG_DIR_ORIG_YARN="${HADOOP_LOG_DIR_CONFIG_ORIG}"
+fi
+
+if echo ${HADOOP_PID_DIR_CONFIG_ORIG} | grep -q MAGPIEHOSTNAMESUBSTITUTION
+then
+    export HADOOP_PID_DIR_ORIG_YARN="${HADOOP_PID_DIR_CONFIG_ORIG}"
+fi
+
 # start resourceManager
+set_magpie_specific_vars
 HARM=$("${HADOOP_HDFS_HOME}/bin/hdfs" getconf -confKey yarn.resourcemanager.ha.enabled 2>&-)
 if [[ ${HARM} = "false" ]]; then
   echo "Starting resourcemanager"
+  set_magpie_generic_vars
   hadoop_uservar_su yarn resourcemanager "${HADOOP_YARN_HOME}/bin/yarn" \
       --config "${HADOOP_CONF_DIR}" \
       --daemon start \
       resourcemanager
   (( HADOOP_JUMBO_RETCOUNTER=HADOOP_JUMBO_RETCOUNTER + $? ))
 else
+  set_magpie_specific_vars
   logicals=$("${HADOOP_HDFS_HOME}/bin/hdfs" getconf -confKey yarn.resourcemanager.ha.rm-ids 2>&-)
   logicals=${logicals//,/ }
   for id in ${logicals}
@@ -65,6 +120,7 @@ else
       RMHOSTS="${RMHOSTS} ${rmhost}"
   done
   echo "Starting resourcemanagers on [${RMHOSTS}]"
+  set_magpie_generic_vars
   hadoop_uservar_su yarn resourcemanager "${HADOOP_YARN_HOME}/bin/yarn" \
       --config "${HADOOP_CONF_DIR}" \
       --daemon start \
@@ -76,6 +132,7 @@ fi
 
 # start nodemanager
 echo "Starting nodemanagers"
+set_magpie_generic_vars
 hadoop_uservar_su yarn nodemanager "${HADOOP_YARN_HOME}/bin/yarn" \
     --config "${HADOOP_CONF_DIR}" \
     --workers \
@@ -85,8 +142,10 @@ hadoop_uservar_su yarn nodemanager "${HA
 
 
 # start proxyserver
+set_magpie_specific_vars
 PROXYSERVER=$("${HADOOP_HDFS_HOME}/bin/hdfs" getconf -confKey  yarn.web-proxy.address 2>&- | cut -f1 -d:)
 if [[ -n ${PROXYSERVER} ]]; then
+ set_magpie_generic_vars
  hadoop_uservar_su yarn proxyserver "${HADOOP_YARN_HOME}/bin/yarn" \
       --config "${HADOOP_CONF_DIR}" \
       --workers \
diff -pruN hadoop-3.1.0-orig/sbin/stop-dfs.sh hadoop-3.1.0/sbin/stop-dfs.sh
--- hadoop-3.1.0-orig/sbin/stop-dfs.sh	2018-03-29 17:03:01.000000000 -0700
+++ hadoop-3.1.0/sbin/stop-dfs.sh	2018-07-12 17:15:07.039123000 -0700
@@ -28,6 +28,43 @@ function hadoop_usage
   echo "Usage: stop-dfs.sh"
 }
 
+function set_magpie_generic_vars
+{
+    if [ "${HADOOP_CONF_DIR}X" != "X" ] && [ "${HADOOP_CONF_DIR_ORIG_DFS}X" != "X" ]
+    then
+        export HADOOP_CONF_DIR="${HADOOP_CONF_DIR_ORIG_DFS}"
+    fi
+
+    if [ "${HADOOP_LOG_DIR}X" != "X" ] && [ "${HADOOP_LOG_DIR_ORIG_DFS}X" != "X" ]
+    then
+        export HADOOP_LOG_DIR="${HADOOP_LOG_DIR_ORIG_DFS}"
+    fi
+
+    if [ "${HADOOP_PID_DIR}X" != "X" ] && [ "${HADOOP_PID_DIR_ORIG_DFS}X" != "X" ]
+    then
+        export HADOOP_PID_DIR="${HADOOP_PID_DIR_ORIG_DFS}"
+    fi
+}
+
+function set_magpie_specific_vars
+{
+    myhostname=`hostname`
+    if [ "${HADOOP_CONF_DIR}X" != "X" ] && [ "${HADOOP_CONF_DIR_ORIG_DFS}X" != "X" ]
+    then
+        export HADOOP_CONF_DIR=$(echo "$HADOOP_CONF_DIR_ORIG_DFS" | sed "s/MAGPIEHOSTNAMESUBSTITUTION/$myhostname/g")
+    fi
+
+    if [ "${HADOOP_LOG_DIR}X" != "X" ] && [ "${HADOOP_LOG_DIR_ORIG_DFS}X" != "X" ]
+    then
+        export HADOOP_LOG_DIR=$(echo "$HADOOP_LOG_DIR_ORIG_DFS" | sed "s/MAGPIEHOSTNAMESUBSTITUTION/$myhostname/g")
+    fi
+
+    if [ "${HADOOP_PID_DIR}X" != "X" ] && [ "${HADOOP_PID_DIR_ORIG_DFS}X" != "X" ]
+    then
+        export HADOOP_PID_DIR=$(echo "$HADOOP_PID_DIR_ORIG_DFS" | sed "s/MAGPIEHOSTNAMESUBSTITUTION/$myhostname/g")
+    fi
+}
+
 this="${BASH_SOURCE-$0}"
 bin=$(cd -P -- "$(dirname -- "${this}")" >/dev/null && pwd -P)
 
@@ -48,9 +85,26 @@ else
   exit 1
 fi
 
+if echo ${HADOOP_CONF_DIR_CONFIG_ORIG} | grep -q MAGPIEHOSTNAMESUBSTITUTION
+then
+    export HADOOP_CONF_DIR_ORIG_DFS="${HADOOP_CONF_DIR_CONFIG_ORIG}"
+fi
+
+if echo ${HADOOP_LOG_DIR_CONFIG_ORIG} | grep -q MAGPIEHOSTNAMESUBSTITUTION
+then
+    export HADOOP_LOG_DIR_ORIG_DFS="${HADOOP_LOG_DIR_CONFIG_ORIG}"
+fi
+
+if echo ${HADOOP_PID_DIR_CONFIG_ORIG} | grep -q MAGPIEHOSTNAMESUBSTITUTION
+then
+    export HADOOP_PID_DIR_ORIG_DFS="${HADOOP_PID_DIR_CONFIG_ORIG}"
+fi
+
 #---------------------------------------------------------
 # namenodes
 
+set_magpie_specific_vars
+
 NAMENODES=$("${HADOOP_HDFS_HOME}/bin/hdfs" getconf -namenodes 2>/dev/null)
 
 if [[ -z "${NAMENODES}" ]]; then
@@ -59,6 +113,8 @@ fi
 
 echo "Stopping namenodes on [${NAMENODES}]"
 
+  set_magpie_generic_vars
+
   hadoop_uservar_su hdfs namenode "${HADOOP_HDFS_HOME}/bin/hdfs" \
     --workers \
     --config "${HADOOP_CONF_DIR}" \
@@ -71,6 +127,8 @@ echo "Stopping namenodes on [${NAMENODES
 
 echo "Stopping datanodes"
 
+set_magpie_generic_vars
+
 hadoop_uservar_su hdfs datanode "${HADOOP_HDFS_HOME}/bin/hdfs" \
   --workers \
   --config "${HADOOP_CONF_DIR}" \
@@ -80,6 +138,8 @@ hadoop_uservar_su hdfs datanode "${HADOO
 #---------------------------------------------------------
 # secondary namenodes (if any)
 
+set_magpie_specific_vars
+
 SECONDARY_NAMENODES=$("${HADOOP_HDFS_HOME}/bin/hdfs" getconf -secondarynamenodes 2>/dev/null)
 
 if [[ "${SECONDARY_NAMENODES}" == "0.0.0.0" ]]; then
@@ -89,6 +149,8 @@ fi
 if [[ -n "${SECONDARY_NAMENODES}" ]]; then
   echo "Stopping secondary namenodes [${SECONDARY_NAMENODES}]"
 
+  set_magpie_generic_vars
+
   hadoop_uservar_su hdfs secondarynamenode "${HADOOP_HDFS_HOME}/bin/hdfs" \
     --workers \
     --config "${HADOOP_CONF_DIR}" \
@@ -100,11 +162,15 @@ fi
 #---------------------------------------------------------
 # quorumjournal nodes (if any)
 
+set_magpie_specific_vars
+
 JOURNAL_NODES=$("${HADOOP_HDFS_HOME}/bin/hdfs" getconf -journalNodes 2>&-)
 
 if [[ "${#JOURNAL_NODES}" != 0 ]]; then
   echo "Stopping journal nodes [${JOURNAL_NODES}]"
 
+  set_magpie_specific_vars
+
   hadoop_uservar_su hdfs journalnode "${HADOOP_HDFS_HOME}/bin/hdfs" \
     --workers \
     --config "${HADOOP_CONF_DIR}" \
@@ -115,10 +181,15 @@ fi
 
 #---------------------------------------------------------
 # ZK Failover controllers, if auto-HA is enabled
+
+set_magpie_specific_vars
+
 AUTOHA_ENABLED=$("${HADOOP_HDFS_HOME}/bin/hdfs" getconf -confKey dfs.ha.automatic-failover.enabled | tr '[:upper:]' '[:lower:]')
 if [[ "${AUTOHA_ENABLED}" = "true" ]]; then
   echo "Stopping ZK Failover Controllers on NN hosts [${NAMENODES}]"
 
+  set_magpie_generic_vars
+
   hadoop_uservar_su hdfs zkfc "${HADOOP_HDFS_HOME}/bin/hdfs" \
     --workers \
     --config "${HADOOP_CONF_DIR}" \
diff -pruN hadoop-3.1.0-orig/sbin/stop-yarn.sh hadoop-3.1.0/sbin/stop-yarn.sh
--- hadoop-3.1.0-orig/sbin/stop-yarn.sh	2018-03-29 17:13:57.000000000 -0700
+++ hadoop-3.1.0/sbin/stop-yarn.sh	2018-07-12 17:12:33.844254000 -0700
@@ -26,6 +26,43 @@ function hadoop_usage
 
 MYNAME="${BASH_SOURCE-$0}"
 
+function set_magpie_generic_vars
+{
+    if [ "${HADOOP_CONF_DIR}X" != "X" ] && [ "${HADOOP_CONF_DIR_ORIG_YARN}X" != "X" ]
+    then
+        export HADOOP_CONF_DIR="${HADOOP_CONF_DIR_ORIG_YARN}"
+    fi
+
+    if [ "${HADOOP_LOG_DIR}X" != "X" ] && [ "${HADOOP_LOG_DIR_ORIG_YARN}X" != "X" ]
+    then
+        export HADOOP_LOG_DIR="${HADOOP_LOG_DIR_ORIG_YARN}"
+    fi
+
+    if [ "${HADOOP_PID_DIR}X" != "X" ] && [ "${HADOOP_PID_DIR_ORIG_YARN}X" != "X" ]
+    then
+        export HADOOP_PID_DIR="${HADOOP_PID_DIR_ORIG_YARN}"
+    fi
+}
+
+function set_magpie_specific_vars
+{
+    myhostname=`hostname`
+    if [ "${HADOOP_CONF_DIR}X" != "X" ] && [ "${HADOOP_CONF_DIR_ORIG_YARN}X" != "X" ]
+    then
+        export HADOOP_CONF_DIR=$(echo "$HADOOP_CONF_DIR_ORIG_YARN" | sed "s/MAGPIEHOSTNAMESUBSTITUTION/$myhostname/g")
+    fi
+
+    if [ "${HADOOP_LOG_DIR}X" != "X" ] && [ "${HADOOP_LOG_DIR_ORIG_YARN}X" != "X" ]
+    then
+        export HADOOP_LOG_DIR=$(echo "$HADOOP_LOG_DIR_ORIG_YARN" | sed "s/MAGPIEHOSTNAMESUBSTITUTION/$myhostname/g")
+    fi
+
+    if [ "${HADOOP_PID_DIR}X" != "X" ] && [ "${HADOOP_PID_DIR_ORIG_YARN}X" != "X" ]
+    then
+        export HADOOP_PID_DIR=$(echo "$HADOOP_PID_DIR_ORIG_YARN" | sed "s/MAGPIEHOSTNAMESUBSTITUTION/$myhostname/g")
+    fi
+}
+
 bin=$(cd -P -- "$(dirname -- "${MYNAME}")" >/dev/null && pwd -P)
 
 # let's locate libexec...
@@ -45,8 +82,24 @@ else
   exit 1
 fi
 
+if echo ${HADOOP_CONF_DIR_CONFIG_ORIG} | grep -q MAGPIEHOSTNAMESUBSTITUTION
+then
+    export HADOOP_CONF_DIR_ORIG_YARN="${HADOOP_CONF_DIR_CONFIG_ORIG}"
+fi
+
+if echo ${HADOOP_LOG_DIR_CONFIG_ORIG} | grep -q MAGPIEHOSTNAMESUBSTITUTION
+then
+    export HADOOP_LOG_DIR_ORIG_YARN="${HADOOP_LOG_DIR_CONFIG_ORIG}"
+fi
+
+if echo ${HADOOP_PID_DIR_CONFIG_ORIG} | grep -q MAGPIEHOSTNAMESUBSTITUTION
+then
+    export HADOOP_PID_DIR_ORIG_YARN="${HADOOP_PID_DIR_CONFIG_ORIG}"
+fi
+
 # stop nodemanager
 echo "Stopping nodemanagers"
+set_magpie_generic_vars
 hadoop_uservar_su yarn nodemanager "${HADOOP_YARN_HOME}/bin/yarn" \
     --config "${HADOOP_CONF_DIR}" \
     --workers \
@@ -54,14 +107,17 @@ hadoop_uservar_su yarn nodemanager "${HA
     nodemanager
 
 # stop resourceManager
+set_magpie_specific_vars
 HARM=$("${HADOOP_HDFS_HOME}/bin/hdfs" getconf -confKey yarn.resourcemanager.ha.enabled 2>&-)
 if [[ ${HARM} = "false" ]]; then
   echo "Stopping resourcemanager"
+  set_magpie_generic_vars
   hadoop_uservar_su yarn resourcemanager "${HADOOP_YARN_HOME}/bin/yarn" \
       --config "${HADOOP_CONF_DIR}" \
       --daemon stop \
       resourcemanager
 else
+  set_magpie_specific_vars
   logicals=$("${HADOOP_HDFS_HOME}/bin/hdfs" getconf -confKey yarn.resourcemanager.ha.rm-ids 2>&-)
   logicals=${logicals//,/ }
   for id in ${logicals}
@@ -70,6 +126,7 @@ else
       RMHOSTS="${RMHOSTS} ${rmhost}"
   done
   echo "Stopping resourcemanagers on [${RMHOSTS}]"
+  set_magpie_generic_vars
   hadoop_uservar_su yarn resourcemanager "${HADOOP_YARN_HOME}/bin/yarn" \
       --config "${HADOOP_CONF_DIR}" \
       --daemon stop \
@@ -79,9 +136,11 @@ else
 fi
 
 # stop proxyserver
+set_magpie_specific_vars
 PROXYSERVER=$("${HADOOP_HDFS_HOME}/bin/hdfs" getconf -confKey  yarn.web-proxy.address 2>&- | cut -f1 -d:)
 if [[ -n ${PROXYSERVER} ]]; then
   echo "Stopping proxy server [${PROXYSERVER}]"
+  set_magpie_generic_vars
   hadoop_uservar_su yarn proxyserver "${HADOOP_YARN_HOME}/bin/yarn" \
       --config "${HADOOP_CONF_DIR}" \
       --workers \
