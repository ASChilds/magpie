#!/bin/bash
#############################################################################
#  Copyright (C) 2013-2015 Lawrence Livermore National Security, LLC.
#  Produced at Lawrence Livermore National Laboratory (cf, DISCLAIMER).
#  Written by Albert Chu <chu11@llnl.gov>
#  LLNL-CODE-644248
#  
#  This file is part of Magpie, scripts for running Hadoop on
#  traditional HPC systems.  For details, see https://github.com/llnl/magpie.
#  
#  Magpie is free software; you can redistribute it and/or modify it
#  under the terms of the GNU General Public License as published by
#  the Free Software Foundation; either version 2 of the License, or
#  (at your option) any later version.
#  
#  Magpie is distributed in the hope that it will be useful, but
#  WITHOUT ANY WARRANTY; without even the implied warranty of
#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
#  General Public License for more details.
#  
#  You should have received a copy of the GNU General Public License
#  along with Magpie.  If not, see <http://www.gnu.org/licenses/>.
#############################################################################

# Export common functions
#
# This is used by scripts, don't edit this
#
# Unlike 'magpie-lib-misc-external', these functions are common but
# live inside the Magpie system.  They largely require environment
# variables, functions, etc. to have been setup.

source ${MAGPIE_SCRIPTS_HOME}/magpie-exports-submission-type
source ${MAGPIE_SCRIPTS_HOME}/magpie-exports-core
source ${MAGPIE_SCRIPTS_HOME}/magpie-exports-local-dirs-conversion
source ${MAGPIE_SCRIPTS_HOME}/magpie-lib-misc-external

Magpie_am_I_master () {
    local myhostname=`hostname`

    Magpie_make_all_local_dirs_node_specific

    for project in HADOOP HBASE SPARK STORM KAFKA ZEPPELIN TACHYON ZOOKEEPER
    do
        local setupvar="${project}_SETUP"
        local confdirvar="${project}_CONF_DIR"
        if [ "${!setupvar}" == "yes" ] && [ -f "${!confdirvar}/masters" ]
        then
            if grep -q -E "^${myhostname}$" ${!confdirvar}/masters
            then 
                return 0
            fi
        fi
    done

    return 1
}

Magpie_am_I_a_hadoop_node () {
    local myhostname=`hostname`

    Magpie_make_hadoop_local_dirs_node_specific

    if grep -q -E "^${myhostname}$" ${HADOOP_CONF_DIR}/slaves \
        || grep -q -E "^${myhostname}$" ${HADOOP_CONF_DIR}/masters \
        || grep -s -q -E "^${myhostname}$" ${HADOOP_CONF_DIR}/namenode_hdfs_federation
    then 
        if grep -q -E "^${myhostname}$" ${HADOOP_CONF_DIR}/masters
        then
            hadoopnoderank=0
        elif grep -s -q -E "^${myhostname}$" ${HADOOP_CONF_DIR}/namenode_hdfs_federation
        then
            local subrank=`grep -n -E "^${myhostname}$" ${HADOOP_CONF_DIR}/namenode_hdfs_federation | awk --field-separator=':' '{print $1}'`
            hadoopnoderank="0${subrank}"
        else
            hadoopnoderank=`grep -n -E "^${myhostname}$" ${HADOOP_CONF_DIR}/slaves | awk --field-separator=':' '{print $1}'`
        fi
        return 0
    fi
    return 1
}

Magpie_am_I_a_hadoop_namenode () {
    local myhostname=`hostname`

    Magpie_make_hadoop_local_dirs_node_specific

    if grep -q -E "^${myhostname}$" ${HADOOP_CONF_DIR}/masters \
        || grep -s -q -E "^${myhostname}$" ${HADOOP_CONF_DIR}/namenode_hdfs_federation
    then 
        if grep -q -E "^${myhostname}$" ${HADOOP_CONF_DIR}/masters
        then
            hadoopnoderank=0
        else
            local subrank=`grep -n -E "^${myhostname}$" ${HADOOP_CONF_DIR}/namenode_hdfs_federation | awk --field-separator=':' '{print $1}'`
            hadoopnoderank="0${subrank}"
        fi
        return 0
    fi
    return 1
}

# For the common pattern
Magpie_am_I_a_project_node () {
    local myhostname=`hostname`
    local confdir=$1
    local noderankvar=$2
    local slavesfile=$3
    local mastersfile=$4
     
    Magpie_make_spark_local_dirs_node_specific

    if grep -q -E "^${myhostname}$" ${confdir}/${slavesfile:-"slaves"} \
        || grep -q -E "^${myhostname}$" ${confdir}/${mastersfile:-"masters"}
    then 
        if grep -q -E "^${myhostname}$" ${confdir}/${mastersfile:-"masters"}
        then
            local noderank=0
        else
            local noderank=`grep -n -E "^${myhostname}$" ${confdir}/${slavesfile:-"slaves"} | awk --field-separator=':' '{print $1}'`
        fi
        eval ${noderankvar}="${noderank}"
        return 0
    fi
    return 1
}

Magpie_am_I_a_hbase_node () {
    Magpie_am_I_a_project_node ${HBASE_CONF_DIR} "hbasenoderank" "regionservers"
    return $?
}

Magpie_am_I_a_phoenix_node () {
    Magpie_am_I_a_hbase_node
    return $?
}

Magpie_am_I_a_spark_node () {
    Magpie_am_I_a_project_node ${SPARK_CONF_DIR} "sparknoderank"
    return $?
}

Magpie_am_I_a_kafka_node () {
    Magpie_am_I_a_project_node ${KAFKA_CONF_DIR} "kafkanoderank"
    return $?
}

Magpie_am_I_a_storm_node () {
    Magpie_am_I_a_project_node ${STORM_CONF_DIR} "stormnoderank" "workers"
    return $?
}

Magpie_am_I_a_tachyon_node () {
    Magpie_am_I_a_project_node ${TACHYON_CONF_DIR} "tachyonnoderank"
    return $?
}

Magpie_am_I_a_zeppelin_node() {
    local myhostname=`hostname`

    Magpie_make_zeppelin_local_dirs_node_specific

    if grep -q -E "^${myhostname}$" ${ZEPPELIN_CONF_DIR}/masters
    then
        zeppelinnoderank=`grep -n -E "^${myhostname}$" ${ZEPPELIN_CONF_DIR}/masters | awk --field-separator=':' '{print $1}'`
        return 0
    fi
    return 1
}

Magpie_am_I_a_zookeeper_node () {
    local myhostname=`hostname`

    Magpie_make_zookeeper_local_dirs_node_specific

    # XXX this comment not true, come back
    # Do not consider the 'master' node, it is only for launching
    # Zookeeper, not an actual Zookeeper node
    if grep -q -E "^${myhostname}$" ${ZOOKEEPER_CONF_DIR}/masters \
        || grep -q -E "^${myhostname}$" ${ZOOKEEPER_CONF_DIR}/slaves
    then 
        if grep -q -E "^${myhostname}$" ${ZOOKEEPER_CONF_DIR}/slaves
        then
            zookeepernoderank=`grep -n -E "^${myhostname}$" ${ZOOKEEPER_CONF_DIR}/slaves | awk --field-separator=':' '{print $1}'`
        fi
        return 0
    fi
    return 1
}

Magpie_calculate_stop_timeouts () {
    local magpieshutdowntimeseconds=`expr ${MAGPIE_SHUTDOWN_TIME} \* 60`

    if [ "${MAGPIE_POST_JOB_RUN}X" != "X" ]
    then
        # Minimum 5 minutes or 1/3rd of time for MAGPIE_POST_JOB_RUN
        local magpiepostrunallocate=`expr ${magpieshutdowntimeseconds} \/ 3`
        if [ "${magpiepostrunallocate}" -lt 300 ]
        then
            magpiepostrunallocate=300
        fi

        magpieshutdowntimeseconds=`expr ${magpieshutdowntimeseconds} - ${magpiepostrunallocate}` 
    fi

    if [ "${HBASE_SETUP}" == "yes" ]
    then
        # Need to give Hbase more time b/c of compaction.  We'll say
        # Hbase always gets 50% of the time, Half for slave timeout and half for
        # compaction .  Input checks ensure
        # magpieshutdowntimeseconds >= 1200 

        local hbase_time=`expr ${magpieshutdowntimeseconds} \/ 2`
        hbase_slave_timeout=`expr ${hbase_time} \/ 2`
        magpieshutdowntimeseconds=${hbase_time}
    fi

    local stoptimeoutdivisor=1

    if [ "${HADOOP_SETUP}" == "yes" ]
    then
        if [ ${HADOOP_SETUP_TYPE}  == "MR1" ] \
            || [ ${HADOOP_SETUP_TYPE}  == "MR2" ]
        then
            # Need to split timeout time between namenode, datanodes,
            # secondary namenode, jobtracker/resource manager,
            # tasktracker/nodemanagers, jobhistory server, & saveNameSpace
            # time
            stoptimeoutdivisor=`expr ${stoptimeoutdivisor} + 7`
        else
            # Need to split timeout time between namenode, datanodes,
            # secondary namenode, jobhistory server, & saveNameSpace time
            stoptimeoutdivisor=`expr ${stoptimeoutdivisor} + 5`
        fi    
    
        # + 2 for scratch extra time in scripts and what not
        stoptimeoutdivisor=`expr ${stoptimeoutdivisor} + 2`
    fi

    if [ "${SPARK_SETUP}" == "yes" ]
    then
        # +2 for extra misc shutdown time
        stoptimeoutdivisor=`expr ${stoptimeoutdivisor} + 2`
    fi

    if [ "${STORM_SETUP}" == "yes" ]
    then
        # +2 for extra misc shutdown time
        stoptimeoutdivisor=`expr ${stoptimeoutdivisor} + 2`
    fi

    if [ "${ZEPPELIN_SETUP}" == "yes" ]
    then
        # +2 for extra misc shutdown time
        stoptimeoutdivisor=`expr ${stoptimeoutdivisor} + 2`
    fi

    if [ "${KAFKA_SETUP}" == "yes" ]
    then
        # +2 for extra misc shutdown time
        stoptimeoutdivisor=`expr ${stoptimeoutdivisor} + 2`
    fi

    if [ "${PHOENIX_SETUP}" == "yes" ]
    then
        # +2 for extra misc shutdown time
        stoptimeoutdivisor=`expr ${stoptimeoutdivisor} + 2`
    fi

    if [ "${ZOOKEEPER_SETUP}" == "yes" ]
    then
        # +2 for extra misc shutdown time
        stoptimeoutdivisor=`expr ${stoptimeoutdivisor} + 2`
    fi

    local stoptimeout=`expr ${magpieshutdowntimeseconds} \/ ${stoptimeoutdivisor}`
        
    if [ "${stoptimeout}" -lt 5 ]
    then
        stoptimeout=5
    fi

    hadoopstoptimeout=${stoptimeout}
}

# Count how many big data systems we're using that can run jobs
# Pig as a wrapper around Hadoop, so it doesn't count
Magpie_calculate_canrunjobscount () {
    canrunjobscount=0

    if [ "${HADOOP_SETUP}" == "yes" ] && ( [ "${HADOOP_SETUP_TYPE}" == "MR1" ] || [ "${HADOOP_SETUP_TYPE}" == "MR2" ] )
    then
        canrunjobscount=`expr ${canrunjobscount} + 1`
    fi

    if [ "${HBASE_SETUP}" == "yes" ]
    then
        canrunjobscount=`expr ${canrunjobscount} + 1`
    fi
 
    if [ "${SPARK_SETUP}" == "yes" ] && [ "${SPARK_USE_YARN}" != "yes" ]
    then
        canrunjobscount=`expr ${canrunjobscount} + 1`
    fi

    if [ "${STORM_SETUP}" == "yes" ]
    then
        canrunjobscount=`expr ${canrunjobscount} + 1`
    fi

    # Could be zero in weird test scenarios
    if [ "${canrunjobscount}" == "0" ]
    then
        canrunjobscount=1
    fi
}

Magpie_calculate_threadstouse () {
    proccount=`cat /proc/cpuinfo | grep processor | wc -l`

    # Sets canrunjobscount
    Magpie_calculate_canrunjobscount
 
    # If only one system to run jobs, estimate 1.5X cores
    # If > 1, split cores evenly amongst job running stuff

    if [ "${canrunjobscount}" == "1" ]
    then
        threadstouse=`expr ${proccount} + ${proccount} \/ 2`
    else
        threadstouse=`expr ${proccount} \/ ${canrunjobscount}`

        if [ "${threadstouse}" == "0" ]
        then
            threadstouse="1"
        fi
    fi
}

Magpie_calculate_memorytouse () {
    local memtotal=`cat /proc/meminfo | grep MemTotal | awk '{print $2}'`
    local memtotalgig=`echo "(${memtotal} / 1048576)" | bc -l | xargs printf "%1.0f"`
    
    # Sets canrunjobscount
    Magpie_calculate_canrunjobscount

    # We start w/ 80% of system memory 
    memorytouse=`echo "${memtotalgig} * .8" | bc -l | xargs printf "%1.0f"`
    memorytouse=`expr $memorytouse \/ ${canrunjobscount}`

    memorytouse=`echo "${memorytouse} * 1024" | bc -l | xargs printf "%1.0f"`
}

Magpie_find_conffile () {
    local magpieconffiledir=${MAGPIE_SCRIPTS_HOME}/conf
    local project=$1
    local conffiledir=$2
    local conffiledesired=$3
    local __returnvar=$4
    local conffilefound=""
    
    if [ "${conffiledir}X" != "X" ]
    then
        if [ -f "${conffiledir}/${conffiledesired}" ]
        then
            conffilefound="${conffiledir}/${conffiledesired}"
        fi
    fi

    if [ "${conffilefound}X" == "X" ]
    then
        conffilefound="${magpieconffiledir}/${conffiledesired}"
    fi

    if [ ! -f ${conffilefound} ]
    then
        echo "Missing ${project} configuration file ${conffiledesired}"
        exit 1
    fi

    eval $__returnvar="${conffilefound}"
}
