#!/bin/bash
#############################################################################
#  Copyright (C) 2013 Lawrence Livermore National Security, LLC.
#  Produced at Lawrence Livermore National Laboratory (cf, DISCLAIMER).
#  Written by Albert Chu <chu11@llnl.gov>
#  LLNL-CODE-644248
#  
#  This file is part of Magpie, scripts for running Hadoop on
#  traditional HPC systems.  For details, see <URL>.
#  
#  Magpie is free software; you can redistribute it and/or modify it
#  under the terms of the GNU General Public License as published by
#  the Free Software Foundation; either version 2 of the License, or
#  (at your option) any later version.
#  
#  Magpie is distributed in the hope that it will be useful, but
#  WITHOUT ANY WARRANTY; without even the implied warranty of
#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
#  General Public License for more details.
#  
#  You should have received a copy of the GNU General Public License
#  along with Magpie.  If not, see <http://www.gnu.org/licenses/>.
#############################################################################

# This script sets up configuration files for jobs.  For the most
# part, it shouldn't be editted.  See magpie.sbatch for configuration
# details.

source ${MAGPIE_SCRIPTS_HOME}/magpie-submission-convert
source ${MAGPIE_SCRIPTS_HOME}/magpie-common-exports

if [ "${HADOOP_SETUP}" != "yes" ]
then
    exit 0
fi

# hadoopnoderank set if succeed
if ! Magpie_am_I_a_hadoop_node
then
    exit 0
fi

extrahadoopclasses=""
extrahadoopopts=""

if [ ${HADOOP_SETUP_TYPE} == "MR1" ] || [ ${HADOOP_SETUP_TYPE} == "HDFS1" ]
then
    # For some reason Hadoop 1.0 needs FQDN hosts in hosts-include
    cp ${HADOOP_CONF_DIR}/slaves_fqdn ${HADOOP_CONF_DIR}/hosts-include
else
    cp ${HADOOP_CONF_DIR}/slaves ${HADOOP_CONF_DIR}/hosts-include
fi
hostsincludefile="${HADOOP_CONF_DIR}/hosts-include"
hostsincludefilesubst=`echo "${hostsincludefile}" | sed "s/\\//\\\\\\\\\//g"`

# By default leave exclude file empty
touch ${HADOOP_CONF_DIR}/hosts-exclude
hostsexcludefile="${HADOOP_CONF_DIR}/hosts-exclude"
hostsexcludefilesubst=`echo "${hostsexcludefile}" | sed "s/\\//\\\\\\\\\//g"`

#
# Calculate values for various config file variables, based on
# recommendtions, rules of thumb, or based on what user input.
#

# Recommendation from Cloudera, parallel copies sqrt(number of nodes), floor of ten
if [ "${HADOOP_PARALLEL_COPIES}X" != "X" ]
then
    parallelcopies=${HADOOP_PARALLEL_COPIES}
else 
    parallelcopies=$(echo "sqrt ( ${HADOOP_SLAVE_COUNT} )" | bc -l | xargs printf "%1.0f")
    if [ "${parallelcopies}" -lt "10" ]
    then
	parallelcopies=10
    fi
fi

# Recommendation from Cloudera, 10% of nodes w/ floor of ten, ceiling 200
# My experience this is low b/c of high core counts, so bump higher to 50% 
namenodehandlercount=$(echo "${HADOOP_SLAVE_COUNT} * .5" | bc -l | xargs printf "%1.0f")
if [ "${namenodehandlercount}" -lt "10" ]
then
    namenodehandlercount=10
fi

if [ "${namenodehandlercount}" -gt "200" ]
then
    namenodehandlercount=200
fi

# General rule of thumb is half namenode handler count, so * .25 instead of * .5
datanodehandlercount=$(echo "${HADOOP_SLAVE_COUNT} * .25" | bc -l | xargs printf "%1.0f")
if [ "${datanodehandlercount}" -lt "10" ]
then
    datanodehandlercount=10
fi

if [ "${datanodehandlercount}" -gt "200" ]
then
    datanodehandlercount=200
fi

# Per description, about 4% of nodes but w/ floor of 10
jobtrackerhandlercount=$(echo "${HADOOP_SLAVE_COUNT} * .04" | bc -l | xargs printf "%1.0f")
if [ "${jobtrackerhandlercount}" -lt "10" ]
then
    jobtrackerhandlercount=10
fi

# Per descrption, about square root number of nodes
submitfilereplication=$(echo "sqrt ( ${HADOOP_SLAVE_COUNT} )" | bc -l | xargs printf "%1.0f")

# Optimal depends on file system
if [ "${HADOOP_FILESYSTEM_MODE}" == "hdfs" ]
then
    iobuffersize=65536
elif [ "${HADOOP_FILESYSTEM_MODE}" == "hdfsoverlustre" ] \
    || [ "${HADOOP_FILESYSTEM_MODE}" == "hdfsovernetworkfs" ]
then
    # Default block size is 1M in Lustre
    # XXX: If not default, can get from lctl or similar?
    # If other networkFS, just assume like Lustre
    iobuffersize=1048576
elif [ "${HADOOP_FILESYSTEM_MODE}" == "rawnetworkfs" ] \
    || [ "${HADOOP_FILESYSTEM_MODE}" == "magpienetworkfs" ]
then
    # Assuming Lustre, so copy above 1M
    iobuffersize=1048576
elif [ "${HADOOP_FILESYSTEM_MODE}" == "intellustre" ]
then
    # Assuming Lustre, so copy above 1M
    iobuffersize=1048576
fi 

javahomesubst=`echo "${JAVA_HOME}" | sed "s/\\//\\\\\\\\\//g"`

memtotal=`cat /proc/meminfo | grep MemTotal | awk '{print $2}'`
memtotalgig=$(echo "(${memtotal} / 1048576)" | bc -l | xargs printf "%1.0f")
proccounttmp=`cat /proc/cpuinfo | grep processor | wc -l`
proccount=`expr ${proccounttmp} + ${proccounttmp} \/ 2`

if [ "${HADOOP_MAX_TASKS_PER_NODE}X" != "X" ]
then
    maxtaskspernode=${HADOOP_MAX_TASKS_PER_NODE}
else
    maxtaskspernode=${proccount}
fi

if [ "${YARN_RESOURCE_MEMORY}X" != "X" ]
then
    yarnresourcememory=${YARN_RESOURCE_MEMORY}
else
    # 80% of system memory seems like a good estimate
    tmp1=$(echo "${memtotalgig} * .8" | bc -l | xargs printf "%1.0f")
    yarnresourcememory=$(echo "${tmp1} * 1024" | bc -l | xargs printf "%1.0f")
fi

if [ "${HADOOP_CHILD_HEAPSIZE}X" != "X" ]
then
    allchildheapsize=${HADOOP_CHILD_HEAPSIZE}
else
# achu: We round down to nearest 512M
    tmp1=$(echo "${yarnresourcememory} / ${maxtaskspernode}" | bc -l | xargs printf "%1.2f")
    tmp2=$(echo "${tmp1} / 512" | bc -l | xargs printf "%1.0f")
    allchildheapsize=$(echo "${tmp2} * 512" | bc -l | xargs printf "%1.0f")
    if [ "${allchildheapsize}" -lt "512" ]
    then
	allchildheapsize=512
    fi
fi

if [ "${HADOOP_CHILD_MAP_HEAPSIZE}X" != "X" ]
then
    mapchildheapsize=${HADOOP_CHILD_MAP_HEAPSIZE}
else
    mapchildheapsize=${allchildheapsize}
fi

if [ "${HADOOP_CHILD_REDUCE_HEAPSIZE}X" != "X" ]
then
    reducechildheapsize=${HADOOP_CHILD_REDUCE_HEAPSIZE}
else
    if [ "${HADOOP_SETUP_TYPE}" == "MR2" ]
    then
	reducechildheapsize=`expr ${mapchildheapsize} \* 2`
    else
	reducechildheapsize=${allchildheapsize}
    fi
fi

if [ "${HADOOP_CHILD_MAP_CONTAINER_BUFFER}X" != "X" ]
then
    mapcontainerbuffer=${HADOOP_CHILD_MAP_CONTAINER_BUFFER}
else
    # Estimate 256M per G
    numgig=`expr ${mapchildheapsize} / 1024`
    if [ "${numgig}" == "0" ]
    then
	numgig=1
    fi
    mapcontainerbuffer=`expr ${numgig} \* 256`
fi

if [ "${HADOOP_CHILD_REDUCE_CONTAINER_BUFFER}X" != "X" ]
then
    reducecontainerbuffer=${HADOOP_CHILD_REDUCE_CONTAINER_BUFFER}
else
    # Estimate 256M per G
    numgig=`expr ${reducechildheapsize} / 1024`
    if [ "${numgig}" == "0" ]
    then
	numgig=1
    fi
    reducecontainerbuffer=`expr ${numgig} \* 256`
fi

# Cloudera recommends 256 for io.sort.mb.  Cloudera blog suggests
# io.sort.factor * 10 ~= io.sort.mb.

if [ "${HADOOP_IO_SORT_MB}X" != "X" ]
then
    iosortmb=${HADOOP_IO_SORT_MB}
else
    # 128M per gig
    numgig=`expr ${mapchildheapsize} / 1024`
    if [ "${numgig}" == "0" ]
    then
	numgig=1
    fi
    iosortmb=`expr ${numgig} \* 128`
fi

if [ "${HADOOP_IO_SORT_FACTOR}X" != "X" ]
then
    iosortfactor=${HADOOP_IO_SORT_FACTOR}
else
    iosortfactor=`expr ${iosortmb} \/ 10`
fi

mapcontainermb=`expr ${mapchildheapsize} + ${mapcontainerbuffer}`
reducecontainermb=`expr ${reducechildheapsize} + ${reducecontainerbuffer}`

yarnmincontainer=1024
if [ ${mapcontainermb} -lt ${yarnmincontainer} ]
then
    yarnmincontainer=${mapcontainermb}
fi

if [ ${reducecontainermb} -lt ${yarnmincontainer} ]
then
    yarnmincontainer=${reducecontainermb}
fi

yarnmaxcontainer=8192
if [ ${mapcontainermb} -gt ${yarnmaxcontainer} ]
then
    yarnmaxcontainer=${mapcontainermb}
fi

if [ ${reducecontainermb} -gt ${yarnmaxcontainer} ]
then
    yarnmaxcontainer=${reducecontainermb}
fi

if [ "${HADOOP_MAPREDUCE_SLOWSTART}X" != "X" ]
then
    mapredslowstart=${HADOOP_MAPREDUCE_SLOWSTART}
else
    mapredslowstart="0.05"
fi

if [ "${HADOOP_DEFAULT_MAP_TASKS}X" != "X" ]
then
    defaultmaptasks=${HADOOP_DEFAULT_MAP_TASKS}
else
    defaultmaptasks=`expr ${maxtaskspernode} \* ${HADOOP_SLAVE_COUNT}`
fi

if [ "${HADOOP_DEFAULT_REDUCE_TASKS}X" != "X" ]
then
    defaultreducetasks=${HADOOP_DEFAULT_REDUCE_TASKS}
else
    defaultreducetasks=${HADOOP_SLAVE_COUNT}
fi

if [ "${HADOOP_MAX_MAP_TASKS}X" != "X" ]
then
    maxmaptasks=${HADOOP_MAX_MAP_TASKS}
else
    maxmaptasks=${maxtaskspernode}
fi

if [ "${HADOOP_MAX_REDUCE_TASKS}X" != "X" ]
then
    maxreducetasks=${HADOOP_MAX_REDUCE_TASKS}
else
    maxreducetasks=${maxtaskspernode}
fi

if [ "${HADOOP_HDFS_BLOCKSIZE}X" != "X" ]
then
    hdfsblocksize=${HADOOP_HDFS_BLOCKSIZE}
else
    # 64M is Hadoop default, widely considered bad choice, we'll use 128M as default
    hdfsblocksize=134217728
fi

if [ "${HADOOP_HDFS_REPLICATION}X" != "X" ]
then
    hdfsreplication=${HADOOP_HDFS_REPLICATION}
else
    hdfsreplication=3
fi

if [ "${HADOOP_RAWNETWORKFS_BLOCKSIZE}X" != "X" ] \
    && [ "${HADOOP_FILESYSTEM_MODE}" == "rawnetworkfs" ]
then
    rawnetworkfsblocksize=${HADOOP_RAWNETWORKFS_BLOCKSIZE}
else
    # 32M is Hadoop default for local
    rawnetworkfsblocksize=33554432
fi

if [ "${HADOOP_INTELLUSTRE_BLOCKSIZE}X" != "X" ]
then
    intellustreblocksize=${HADOOP_INTELLUSTRE_BLOCKSIZE}
else
    intellustreblocksize=536870912
fi

if [ "${HADOOP_INTELLUSTRE_STRIPESIZE}X" != "X" ]
then
    intellustrestripesize=${HADOOP_INTELLUSTRE_STRIPESIZE}
else
    intellustrestripesize=134217728
fi

if [ "${HADOOP_INTELLUSTRE_STRIPECOUNT}X" != "X" ]
then
    intellustrestripecount=${HADOOP_INTELLUSTRE_STRIPECOUNT}
else
    intellustrestripecount=-1
fi

if [ "${HADOOP_FILESYSTEM_MODE}" == "intellustre" ] && [ "${HADOOP_INTELLUSTRE_SHUFFLE}X" != "X" ]
then
    if [ "${HADOOP_INTELLUSTRE_SHUFFLE}" == "yes" ]
    then
	mapoutputcollectorclass="org.apache.hadoop.mapred.SharedFsPlugins\$MapOutputBuffer"
	reduceshuffleconsumerplugin="org.apache.hadoop.mapred.SharedFsPlugins\$Shuffle"
    else
	mapoutputcollectorclass="org.apache.hadoop.mapred.MapTask\$MapOutputBuffer"
	reduceshuffleconsumerplugin="org.apache.hadoop.mapreduce.task.reduce.Shuffle"
    fi
else
    mapoutputcollectorclass="org.apache.hadoop.mapred.MapTask\$MapOutputBuffer"
    reduceshuffleconsumerplugin="org.apache.hadoop.mapreduce.task.reduce.Shuffle"
fi

if [ "${HADOOP_MAGPIENETWORKFS_BLOCKSIZE}X" != "X" ] \
    && [ "${HADOOP_FILESYSTEM_MODE}" == "magpienetworkfs" ]
then
    magpienetworkfsblocksize=${HADOOP_MAGPIENETWORKFS_BLOCKSIZE}
else
    # 32M is Hadoop default for local
    magpienetworkfsblocksize=33554432
fi

if [ "${HADOOP_DAEMON_HEAP_MAX}X" != "X" ]
then
    hadoopdaemonheapmax="${HADOOP_DAEMON_HEAP_MAX}"
else
    hadoopdaemonheapmax="1000"
fi 

if [ "${HADOOP_COMPRESSION}X" != "X" ]
then
    if [ "${HADOOP_COMPRESSION}" == "yes" ]
    then
	compression=true
    else
	compression=false
    fi
else
    compression=false
fi

openfiles=`ulimit -n`
if [ "${openfiles}" != "unlimited" ]
then
    openfileshardlimit=`ulimit -H -n`

    # we estimate 4096 per 100 nodes, minimum 8192, max 65536.
    # Obviously depends on many factors such as core count, but it's a
    # reasonble and safe over-estimate calculated based on experience.
    openfilesslavecount=`expr ${HADOOP_SLAVE_COUNT} \/ 100`
    openfilescount=`expr ${openfilesslavecount} \* 4096`
    if [ "${openfilescount}" -lt "8192" ]
    then
	openfilescount=8192
    fi
    if [ "${openfilescount}" -gt "65536" ]
    then
	openfilescount=65536
    fi
    
    if [ "${openfileshardlimit}" != "unlimited" ]
    then
        if [ ${openfilescount} -gt ${openfileshardlimit} ]
        then
            openfilescount=${openfileshardlimit}
        fi
    fi
else
    openfilescount="unlimited"
fi

userprocesses=`ulimit -u`
if [ "${userprocesses}" != "unlimited" ]
then
    userprocesseshardlimit=`ulimit -H -u`

    # we estimate 2048 per 100 nodes, minimum 4096, max 32768.
    userprocessesslavecount=`expr ${HADOOP_SLAVE_COUNT} \/ 100`
    userprocessescount=`expr ${userprocessesslavecount} \* 2048`
    if [ "${userprocessescount}" -lt "4096" ]
    then
	userprocessescount=4096
    fi
    if [ "${userprocessescount}" -gt "32768" ]
    then
	userprocessescount=32768
    fi
    
    if [ "${userprocesseshardlimit}" != "unlimited" ]
    then
        if [ ${userprocessescount} -gt ${userprocesseshardlimit} ]
        then
            userprocessescount=${userprocesseshardlimit}
        fi
    fi
else
    userprocessescount="unlimited"
fi

#
# Setup Hadoop file system
#

pathcount=0

# sets hadooptmpdir and fsdefault
Magpie_calculate_hadoop_filesystem_paths

if [ "${HADOOP_FILESYSTEM_MODE}" == "hdfs" ]
then
    hadooptmpdirsubst=`echo "${hadooptmpdir}" | sed "s/\\//\\\\\\\\\//g"`
    fsdefaultsubst=`echo ${fsdefault} | sed "s/\\//\\\\\\\\\//g"`

    # Assume if path doesn't exist must format
    if [ -d "${hadooptmpdir}" ]
    then
	format=0
    else
	format=1
    fi

    IFSORIG=${IFS}
    IFS=","
    datanodedirtmp=""
    for hdfspath in ${HADOOP_HDFS_PATH}
    do
	if [ ! -d "${hdfspath}" ]
	then
	    mkdir -p ${hdfspath}
	    if [ $? -ne 0 ] ; then
		echo "mkdir failed making ${hdfspath}"
		exit 1
	    fi
	fi

	if [ ${HADOOP_SETUP_TYPE} == "MR1" ] || [ ${HADOOP_SETUP_TYPE} == "HDFS1" ]
	then
	    if [ "${datanodedirtmp}X" == "X" ]
	    then
		datanodedirtmp="${hdfspath}/dfs/data"
	    else
		datanodedirtmp="${datanodedirtmp},${hdfspath}/dfs/data"
	    fi
	elif [ ${HADOOP_SETUP_TYPE}  == "MR2" ] || [ ${HADOOP_SETUP_TYPE} == "HDFS2" ]
	then
	    if [ "${datanodedirtmp}X" == "X" ]
	    then
		datanodedirtmp="file://${hdfspath}/dfs/data"
	    else
		datanodedirtmp="${datanodedirtmp},file://${hdfspath}/dfs/data"
	    fi
	fi
	pathcount=$((pathcount+1))
    done
    IFS=${IFSORIG}
    datanodedir=`echo "${datanodedirtmp}" | sed "s/\\//\\\\\\\\\//g"`
elif [ "${HADOOP_FILESYSTEM_MODE}" == "hdfsoverlustre" ]
then
    hadooptmpdirsubst=`echo "${hadooptmpdir}" | sed "s/\\//\\\\\\\\\//g"`
    fsdefaultsubst=`echo ${fsdefault} | sed "s/\\//\\\\\\\\\//g"`
    
    # Assume if path doesn't exist must format
    if [ -d "${hadooptmpdir}" ]
    then
	format=0
    else
	format=1
	mkdir -p ${hadooptmpdir}
	if [ $? -ne 0 ] ; then
	    echo "mkdir failed making ${hadooptmpdir}"
	    exit 1
	fi
	/usr/bin/lfs setstripe --size ${hdfsblocksize} --count 1 ${hadooptmpdir}
    fi
    
    if [ ${HADOOP_SETUP_TYPE} == "MR1" ] || [ ${HADOOP_SETUP_TYPE} == "HDFS1" ]
    then
	datanodedir="$\{hadoop.tmp.dir\}\/dfs\/data"
    elif [ ${HADOOP_SETUP_TYPE}  == "MR2" ] || [ ${HADOOP_SETUP_TYPE} == "HDFS2" ]
    then
	datanodedir="file:\/\/\$\{hadoop.tmp.dir\}\/dfs\/data"
    fi
    
    pathcount=1
elif [ "${HADOOP_FILESYSTEM_MODE}" == "hdfsovernetworkfs" ]
then
    hadooptmpdirsubst=`echo "${hadooptmpdir}" | sed "s/\\//\\\\\\\\\//g"`
    fsdefaultsubst=`echo${fsdefault}_ | sed "s/\\//\\\\\\\\\//g"`
    
    # Assume if path doesn't exist must format
    if [ -d "${hadooptmpdir}" ]
    then
	format=0
    else
	format=1
	mkdir -p ${hadooptmpdir}
	if [ $? -ne 0 ] ; then
	    echo "mkdir failed making ${hadooptmpdir}"
	    exit 1
	fi
    fi
    
    if [ ${HADOOP_SETUP_TYPE} == "MR1" ] || [ ${HADOOP_SETUP_TYPE} == "HDFS1" ]
    then
	datanodedir="$\{hadoop.tmp.dir\}\/dfs\/data"
    elif [ ${HADOOP_SETUP_TYPE}  == "MR2" ] || [ ${HADOOP_SETUP_TYPE} == "HDFS2" ]
    then
	datanodedir="file:\/\/\$\{hadoop.tmp.dir\}\/dfs\/data"
    fi
    
    pathcount=1
elif [ "${HADOOP_FILESYSTEM_MODE}" == "rawnetworkfs" ]
then
    hadooptmpdirsubst=`echo "${hadooptmpdir}" | sed "s/\\//\\\\\\\\\//g"`
    fsdefaultsubst=`echo ${fsdefault} | sed "s/\\//\\\\\\\\\//g"`
    
    if [ ! -d "${HADOOP_RAWNETWORKFS_PATH}" ]
    then
        mkdir -p ${HADOOP_RAWNETWORKFS_PATH}
        if [ $? -ne 0 ] ; then
	    echo "mkdir failed making ${HADOOP_RAWNETWORKFS_PATH}"
	    exit 1
        fi
    fi
	
    if [ ! -d "${hadooptmpdir}" ]
    then
        mkdir -p ${hadooptmpdir}
        if [ $? -ne 0 ] ; then
	    echo "mkdir failed making ${hadooptmpdir}"
	    exit 1
        fi
    fi
    
    format=0
    pathcount=1
elif [ "${HADOOP_FILESYSTEM_MODE}" == "intellustre" ]
then
    intellustrerootdir=`echo "${HADOOP_INTELLUSTRE_PATH}" | sed "s/\\//\\\\\\\\\//g"`
    hadooptmpdirsubst=`echo "${hadooptmpdir}" | sed "s/\\//\\\\\\\\\//g"`
    fsdefaultsubst=`echo ${fsdefault} | sed "s/\\//\\\\\\\\\//g"`

    if [ ! -d "${hadooptmpdir}" ]
    then
        mkdir -p ${hadooptmpdir}
        if [ $? -ne 0 ] ; then
            echo "mkdir failed making ${hadooptmpdir}"
            exit 1
        fi
    fi

    format=0

    pathcount=1
elif [ "${HADOOP_FILESYSTEM_MODE}" == "magpienetworkfs" ]
then
    magpienetworkfsbase=`echo "${HADOOP_MAGPIENETWORKFS_PATH}" | sed "s/\\//\\\\\\\\\//g"`
    hadooptmpdirsubst=`echo "${hadooptmpdir}" | sed "s/\\//\\\\\\\\\//g"`
    fsdefaultsubst=`echo ${fsdefault} | sed "s/\\//\\\\\\\\\//g"`
    
    if [ ! -d "${HADOOP_MAGPIENETWORKFS_PATH}" ]
    then
        mkdir -p ${HADOOP_MAGPIENETWORKFS_PATH}
        if [ $? -ne 0 ] ; then
	    echo "mkdir failed making ${HADOOP_MAGPIENETWORKFS_PATH}"
	    exit 1
        fi
    fi
    
    if [ ! -d "${hadooptmpdir}" ]
    then
        mkdir -p ${hadooptmpdir}
        if [ $? -ne 0 ] ; then
	    echo "mkdir failed making ${hadooptmpdir}"
	    exit 1
        fi
    fi

    format=0
    pathcount=1
else
    echo "Illegal HADOOP_FILESYSTEM_MODE \"${HADOOP_FILESYSTEM_MODE}\" specified"
    exit 1
fi

if ([ "${HADOOP_FILESYSTEM_MODE}" == "hdfsoverlustre" ] \
    || [ "${HADOOP_FILESYSTEM_MODE}" == "hdfsovernetworkfs" ] \
    || [ "${HADOOP_FILESYSTEM_MODE}" == "rawnetworkfs" ] \
    || [ "${HADOOP_FILESYSTEM_MODE}" == "intellustre" ] \
    || [ "${HADOOP_FILESYSTEM_MODE}" == "magpienetworkfs" ]) \
    && [ "${HADOOP_LOCALSTORE}X" != "X" ]
then
    IFSORIG=${IFS}
    IFS=","
    mapredlocalstoredir=""
    yarnlocalstoredir=""
    for localstorefile in ${HADOOP_LOCALSTORE}
    do
	localstoredirtmp=${localstorefile}
	mapredlocalstoredirtmp=`echo "${localstoredirtmp}/mapred/local" | sed "s/\\//\\\\\\\\\//g"`
	yarnlocalstoredirtmp=`echo "${localstoredirtmp}/yarn-nm" | sed "s/\\//\\\\\\\\\//g"`
	
	if [ -d "${localstoredirtmp}" ]
	then
	    mkdir -p ${localstoredirtmp}
	    if [ $? -ne 0 ] ; then
		echo "mkdir failed making ${localstoredirtmp}"
		exit 1
	    fi
	fi

	if [ "${mapredlocalstoredir}X" == "X" ]
	then
	    mapredlocalstoredir="${mapredlocalstoredirtmp}"
	    yarnlocalstoredir="${yarnlocalstoredirtmp}"
	else
	    mapredlocalstoredir="${mapredlocalstoredir},${mapredlocalstoredirtmp}"
	    yarnlocalstoredir="${yarnlocalstoredir},${yarnlocalstoredirtmp}"
	fi
    done
    IFS=${IFSORIG}
elif [ "${HADOOP_FILESYSTEM_MODE}" == "hdfs" ] \
    && [ ${pathcount} -gt 1 ] 
then
    IFSORIG=${IFS}
    IFS=","
    mapredlocalstoredir=""
    yarnlocalstoredir=""
    for localstorefile in ${HADOOP_HDFS_PATH}
    do
	localstoredirtmp=${localstorefile}
	mapredlocalstoredirtmp=`echo "${localstoredirtmp}/mapred/local" | sed "s/\\//\\\\\\\\\//g"`
	yarnlocalstoredirtmp=`echo "${localstoredirtmp}/yarn-nm" | sed "s/\\//\\\\\\\\\//g"`
	
	if [ "${mapredlocalstoredir}X" == "X" ]
	then
	    mapredlocalstoredir="${mapredlocalstoredirtmp}"
	    yarnlocalstoredir="${yarnlocalstoredirtmp}"
	else
	    mapredlocalstoredir="${mapredlocalstoredir},${mapredlocalstoredirtmp}"
	    yarnlocalstoredir="${yarnlocalstoredir},${yarnlocalstoredirtmp}"
	fi
    done
    IFS=${IFSORIG}
else
    mapredlocalstoredir="\$\{hadoop.tmp.dir\}\/mapred\/local"
    yarnlocalstoredir="\$\{hadoop.tmp.dir\}\/yarn-nm"
fi

# Sets hadoopstoptimeout
Magpie_calculate_stop_timeouts

#
# Hadoop UDA Setup
#

if [ "${HADOOP_UDA_SETUP}" == "yes" ] \
    && [ "${HADOOP_SETUP_TYPE}"  == "MR2" ]
then
    if [ "${extrahadoopclasses}X" == "X" ]
    then
	extrahadoopclasses="${HADOOP_UDA_JAR}"
    else
	extrahadoopclasses="${extrahadoopclasses}:${HADOOP_UDA_JAR}"
    fi

    if [ "${HADOOP_UDA_LIBPATH}X" != "X" ]
    then
	if [ "${extrahadoopopts}X" == "X" ]
	then
	    extrahadoopopts="-Djava.library.path=${HADOOP_UDA_LIBPATH}"
	else
	    extrahadoopopts="${extrahadoopopts} -Djava.library.path=${HADOOP_UDA_LIBPATH}"
	fi

	if [ "${extrayarnlibrarypath}X" == "X" ]
	then
	    extrayarnlibrarypath="${HADOOP_UDA_LIBPATH}"
	else
	    extrayarnlibrarypath="${extrayarnlibrarypath}:${HADOOP_UDA_LIBPATH}"
	fi
    fi
fi

if [ "${HADOOP_UDA_RDMA_BUFFER_SIZE}X" != "X" ]
then
    rdmabufsize=${HADOOP_UDA_RDMA_BUFFER_SIZE}
else
    rdmabufsize=1024
fi

#
# Get config files for setup
#

if [ "${HADOOP_CONF_FILES}X" == "X" ]
then
    hadoopconffiledir=${MAGPIE_SCRIPTS_HOME}/conf
else
    hadoopconffiledir=${HADOOP_CONF_FILES}
fi

if [ ${HADOOP_SETUP_TYPE} == "MR1" ] || [ ${HADOOP_SETUP_TYPE} == "HDFS1" ]
then
    coresitexml=${hadoopconffiledir}/core-site-1.0.xml
    mapredsitexml=${hadoopconffiledir}/mapred-site-1.0.xml
    hadoopenvsh=${hadoopconffiledir}/hadoop-env-1.0.sh
    hdfssitexml=${hadoopconffiledir}/hdfs-site-1.0.xml
elif [ ${HADOOP_SETUP_TYPE}  == "MR2" ] || [ ${HADOOP_SETUP_TYPE} == "HDFS2" ]
then
    coresitexml=${hadoopconffiledir}/core-site-2.0.xml
    mapredsitexml=${hadoopconffiledir}/mapred-site-2.0.xml
    hadoopenvsh=${hadoopconffiledir}/hadoop-env-2.0.sh
    mapredenvsh=${hadoopconffiledir}/mapred-env-2.0.sh
    yarnsitexml=${hadoopconffiledir}/yarn-site-2.0.xml
    yarnenvsh=${hadoopconffiledir}/yarn-env-2.0.sh
    hdfssitexml=${hadoopconffiledir}/hdfs-site-2.0.xml
else
    echo "Illegal HADOOP_SETUP_TYPE \"${HADOOP_SETUP_TYPE} \" specified"
    exit 1
fi

#
# Setup Hadoop configuration files and environment files
#

if [ ${HADOOP_SETUP_TYPE}  == "MR1" ] \
    || [ ${HADOOP_SETUP_TYPE}  == "MR2" ] \
    || [ ${HADOOP_SETUP_TYPE}  == "HDFS1" ] \
    || [ ${HADOOP_SETUP_TYPE}  == "HDFS2" ]
then
    sed -e "s/HADOOPTMPDIR/${hadooptmpdirsubst}/g" \
        -e "s/FSDEFAULT/${fsdefaultsubst}/g" \
	-e "s/IOBUFFERSIZE/${iobuffersize}/g" \
	-e "s/LOCALBLOCKSIZE/${rawnetworkfsblocksize}/g" \
	-e "s/INTELLUSTREROOTDIR/${intellustrerootdir}/g" \
	-e "s/INTELLUSTREBLOCKSIZE/${intellustreblocksize}/g" \
	-e "s/INTELLUSTRESTRIPESIZE/${intellustrestripesize}/g" \
	-e "s/INTELLUSTRESTRIPECOUNT/${intellustrestripecount}/g" \
	-e "s/MAGPIENETWORKFSBASEDIR/${magpienetworkfsbase}/g" \
	-e "s/MAGPIENETWORKFSBLOCKSIZE/${magpienetworkfsblocksize}/g" \
	$coresitexml > ${HADOOP_CONF_DIR}/core-site.xml

    hadoopconfdirsubst=`echo "${HADOOP_CONF_DIR}" | sed "s/\\//\\\\\\\\\//g"`
    hadooplogdirsubst=`echo "${HADOOP_LOG_DIR}" | sed "s/\\//\\\\\\\\\//g"`
    hadoophomesubst=`echo "${HADOOP_HOME}" | sed "s/\\//\\\\\\\\\//g"`
    hadoopcommonhomesubst=`echo "${HADOOP_COMMON_HOME}" | sed "s/\\//\\\\\\\\\//g"`
    hadoopmapredhomesubst=`echo "${HADOOP_MAPRED_HOME}" | sed "s/\\//\\\\\\\\\//g"`
    hadoophdfshomesubst=`echo "${HADOOP_HDFS_HOME}" | sed "s/\\//\\\\\\\\\//g"`

    extrahadoopclassessubst=`echo "${extrahadoopclasses}" | sed "s/\\//\\\\\\\\\//g"`
    extrahadoopoptssubst=`echo "${extrahadoopopts}" | sed "s/\\//\\\\\\\\\//g"`

    sed -e "s/HADOOP_JAVA_HOME/${javahomesubst}/g" \
	-e "s/HADOOP_DAEMON_HEAP_MAX/${hadoopdaemonheapmax}/g" \
	-e "s/HADOOPTIMEOUTSECONDS/${hadoopstoptimeout}/g" \
	-e "s/HADOOPCONFDIR/${hadoopconfdirsubst}/g" \
	-e "s/HADOOPLOGDIR/${hadooplogdirsubst}/g" \
	-e "s/HADOOPHOME/${hadoophomesubst}/g" \
	-e "s/HADOOPCOMMONHOME/${hadoopcommonhomesubst}/g" \
	-e "s/HADOOPMAPREDHOME/${hadoopmapredhomesubst}/g" \
	-e "s/HADOOPHDFSHOME/${hadoophdfshomesubst}/g" \
	-e "s/EXTRAHADOOPCLASSES/${extrahadoopclassessubst}/g" \
	-e "s/EXTRAHADOOPOPTS/${extrahadoopoptssubst}/g" \
	$hadoopenvsh > ${HADOOP_CONF_DIR}/hadoop-env.sh

    if [ "${MAGPIE_REMOTE_CMD:-ssh}" != "ssh" ]
    then
	echo "export HADOOP_SSH_CMD=\"${MAGPIE_REMOTE_CMD}\"" >> ${HADOOP_CONF_DIR}/hadoop-env.sh
    fi
    if [ "${MAGPIE_REMOTE_CMD_OPTS}X" != "X" ]
    then
	echo "export HADOOP_SSH_OPTS=\"${MAGPIE_REMOTE_CMD_OPTS}\"" >> ${HADOOP_CONF_DIR}/hadoop-env.sh
    fi

    if [ "${HADOOP_ENVIRONMENT_EXTRA_PATH}X" != "X" ] && [ -f ${HADOOP_ENVIRONMENT_EXTRA_PATH} ]
    then
	cat ${HADOOP_ENVIRONMENT_EXTRA_PATH} >> ${HADOOP_CONF_DIR}/hadoop-env.sh
    else
	echo "ulimit -n ${openfilescount}" >> ${HADOOP_CONF_DIR}/hadoop-env.sh
	echo "ulimit -u ${userprocessescount}" >> ${HADOOP_CONF_DIR}/hadoop-env.sh
    fi

    cat ${hadoopconffiledir}/hadoop.log4j.properties > ${HADOOP_CONF_DIR}/log4j.properties
fi

if [ ${HADOOP_SETUP_TYPE} == "MR1" ] \
    || [ ${HADOOP_SETUP_TYPE}  == "MR2" ]
then
    if [ "${HADOOP_UDA_SETUP}" == "yes" ]
    then
        #
        # A few settings depend on version
        #
	if echo ${HADOOP_VERSION} | grep -q -E "2\.[0-1]\.[0-9]"
	then 
	    shuffleproviderservices="uda.shuffle"
	else
	    shuffleproviderservices="uda_shuffle"
	fi
	reduceshuffleconsumerplugin="com.mellanox.hadoop.mapred.UdaShuffleConsumerPlugin"
    fi

    sed -e "s/HADOOP_MASTER_HOST/${HADOOP_MASTER_NODE}/g" \
	-e "s/MRPRALLELCOPIES/${parallelcopies}/g" \
	-e "s/JOBTRACKERHANDLERCOUNT/${jobtrackerhandlercount}/g" \
	-e "s/MRSLOWSTART/${mapredslowstart}/g" \
	-e "s/ALLCHILDHEAPSIZE/${allchildheapsize}/g" \
	-e "s/MAPCHILDHEAPSIZE/${mapchildheapsize}/g" \
	-e "s/MAPCONTAINERMB/${mapcontainermb}/g" \
	-e "s/REDUCECHILDHEAPSIZE/${reducechildheapsize}/g" \
	-e "s/REDUCECONTAINERMB/${reducecontainermb}/g" \
	-e "s/DEFAULTMAPTASKS/${defaultmaptasks}/g" \
	-e "s/DEFAULTREDUCETASKS/${defaultreducetasks}/" \
	-e "s/MAXMAPTASKS/${maxmaptasks}/g" \
	-e "s/MAXREDUCETASKS/${maxreducetasks}/g" \
	-e "s/LOCALSTOREDIR/${mapredlocalstoredir}/g" \
	-e "s/IOSORTFACTOR/${iosortfactor}/g" \
	-e "s/IOSORTMB/${iosortmb}/g" \
	-e "s/HADOOPCOMPRESSION/${compression}/g" \
	-e "s/HADOOPHOSTSINCLUDEFILENAME/${hostsincludefilesubst}/g" \
	-e "s/HADOOPHOSTSEXCLUDEFILENAME/${hostsexcludefilesubst}/g" \
	-e "s/SUBMITFILEREPLICATION/${submitfilereplication}/g" \
    	-e "s/MAPOUTPUTCOLLECTORCLASS/${mapoutputcollectorclass}/g" \
    	-e "s/REDUCESHUFFLECONSUMERPLUGIN/${reduceshuffleconsumerplugin}/g" \
	-e "s/SHUFFLEPROVIDERSERVICES/${shuffleproviderservices}/g" \
	-e "s/RDMABUFSIZE/${rdmabufsize}/g" \
	$mapredsitexml > ${HADOOP_CONF_DIR}/mapred-site.xml
fi

if [ ${HADOOP_SETUP_TYPE}  == "MR2" ]
then
    #
    # A few settings depend on version
    #
    if echo ${HADOOP_VERSION} | grep -q -E "2\.[0-1]\.[0-9]"
    then 
	if [ "${HADOOP_UDA_SETUP}" == "yes" ]
	then
	    yarnauxservices="mapreduce.shuffle,uda.shuffle"
	    yarnauxudashuffle="yarn.nodemanager.aux-services.uda.shuffle.class"
	else
	    yarnauxservices="mapreduce.shuffle"
	fi
	yarnauxmapreduceshuffle="yarn.nodemanager.aux-services.mapreduce.shuffle.class"
    else
	if [ "${HADOOP_UDA_SETUP}" == "yes" ]
	then
	    yarnauxservices="mapreduce_shuffle,uda_shuffle"
	    yarnauxudashuffle="yarn.nodemanager.aux-services.uda_shuffle.class"
	else
	    yarnauxservices="mapreduce_shuffle"
	fi
	yarnauxmapreduceshuffle="yarn.nodemanager.aux-services.mapreduce_shuffle.class"
    fi

    sed -e "s/HADOOP_MASTER_HOST/${HADOOP_MASTER_NODE}/g" \
	-e "s/YARNMINCONTAINER/${yarnmincontainer}/g" \
	-e "s/YARNMAXCONTAINER/${yarnmaxcontainer}/g" \
	-e "s/YARNRESOURCEMEMORY/${yarnresourcememory}/g" \
	-e "s/LOCALSTOREDIR/${yarnlocalstoredir}/g" \
	-e "s/YARNAUXSERVICES/${yarnauxservices}/g" \
	-e "s/YARNAUXMAPREDUCESHUFFLE/${yarnauxmapreduceshuffle}/g" \
	-e "s/YARNAUXUDASHUFFLE/${yarnauxudashuffle}/g" \
	$yarnsitexml > ${HADOOP_CONF_DIR}/yarn-site.xml
    
    hadoopmapredlogdirsubst=`echo "${HADOOP_LOG_DIR}" | sed "s/\\//\\\\\\\\\//g"`
    hadoopconfdirsubst=`echo "${HADOOP_CONF_DIR}" | sed "s/\\//\\\\\\\\\//g"`
    hadoophomesubst=`echo "${HADOOP_HOME}" | sed "s/\\//\\\\\\\\\//g"`
    hadoopcommonhomesubst=`echo "${HADOOP_COMMON_HOME}" | sed "s/\\//\\\\\\\\\//g"`
    hadoopmapredhomesubst=`echo "${HADOOP_MAPRED_HOME}" | sed "s/\\//\\\\\\\\\//g"`

    sed -e "s/HADOOP_JAVA_HOME/${javahomesubst}/g" \
	-e "s/HADOOP_DAEMON_HEAP_MAX/${hadoopdaemonheapmax}/g" \
	-e "s/HADOOPTIMEOUTSECONDS/${hadoopstoptimeout}/g" \
	-e "s/HADOOPMAPREDLOGDIR/${hadoopmapredlogdirsubst}/g" \
	-e "s/HADOOPCONFDIR/${hadoopconfdirsubst}/g" \
	-e "s/HADOOPHOME/${hadoophomesubst}/g" \
	-e "s/HADOOPCOMMONHOME/${hadoopcommonhomesubst}/g" \
	-e "s/HADOOPMAPREDHOME/${hadoopmapredhomesubst}/g" \
	$mapredenvsh > ${HADOOP_CONF_DIR}/mapred-env.sh

    yarnconfdirsubst=`echo "${YARN_CONF_DIR}" | sed "s/\\//\\\\\\\\\//g"`
    yarnlogdirsubst=`echo "${YARN_LOG_DIR}" | sed "s/\\//\\\\\\\\\//g"`
    yarncommonhomesubst=`echo "${YARN_COMMON_HOME}" | sed "s/\\//\\\\\\\\\//g"`
    hadoopyarnhomesubst=`echo "${HADOOP_YARN_HOME}" | sed "s/\\//\\\\\\\\\//g"`

    extrayarnlibrarypathsubst=`echo "${extrayarnlibrarypath}" | sed "s/\\//\\\\\\\\\//g"`

    sed -e "s/HADOOP_JAVA_HOME/${javahomesubst}/g" \
	-e "s/HADOOP_DAEMON_HEAP_MAX/${hadoopdaemonheapmax}/g" \
	-e "s/HADOOPTIMEOUTSECONDS/${hadoopstoptimeout}/g" \
	-e "s/YARNUSERNAME/${HADOOP_YARN_USER}/g" \
	-e "s/YARNCONFDIR/${yarnconfdirsubst}/g" \
	-e "s/YARNLOGDIR/${yarnlogdirsubst}/g" \
	-e "s/YARNCOMMONHOME/${yarncommonhomesubst}/g" \
	-e "s/HADOOPYARNHOME/${hadoopyarnhomesubst}/g" \
	-e "s/EXTRAYARNLIBRARYPATH/${extrayarnlibrarypathsubst}/g" \
	$yarnenvsh > ${HADOOP_CONF_DIR}/yarn-env.sh

    if [ "${MAGPIE_REMOTE_CMD:-ssh}" != "ssh" ]
    then
	echo "export YARN_SSH_CMD=\"$MAGPIE_REMOTE_CMD\"" >> ${HADOOP_CONF_DIR}/yarn-env.sh
    fi
    if [ "${MAGPIE_REMOTE_CMD_OPTS}X" != "X" ]
    then
	echo "export YARN_SSH_OPTS=\"${MAGPIE_REMOTE_CMD_OPTS}\"" >> ${HADOOP_CONF_DIR}/yarn-env.sh
    fi

    if [ "${HADOOP_ENVIRONMENT_EXTRA_PATH}X" != "X" ] && [ -f ${HADOOP_ENVIRONMENT_EXTRA_PATH} ]
    then
	cat ${HADOOP_ENVIRONMENT_EXTRA_PATH} >> ${HADOOP_CONF_DIR}/yarn-env.sh
    else
	echo "ulimit -n ${openfilescount}" >> ${HADOOP_CONF_DIR}/yarn-env.sh
	echo "ulimit -u ${userprocessescount}" >> ${HADOOP_CONF_DIR}/yarn-env.sh
    fi
fi

if [ "${HADOOP_FILESYSTEM_MODE}" == "hdfs" ] \
    || [ "${HADOOP_FILESYSTEM_MODE}" == "hdfsoverlustre" ] \
    || [ "${HADOOP_FILESYSTEM_MODE}" == "hdfsovernetworkfs" ]
then
    sed -e "s/HADOOP_MASTER_HOST/${HADOOP_MASTER_NODE}/g" \
	-e "s/HDFSBLOCKSIZE/${hdfsblocksize}/g" \
	-e "s/HDFSREPLICATION/${hdfsreplication}/g" \
	-e "s/HDFSNAMENODEHANDLERCLOUNT/${namenodehandlercount}/g" \
	-e "s/HDFSDATANODEHANDLERCLOUNT/${datanodehandlercount}/g" \
	-e "s/IOBUFFERSIZE/${iobuffersize}/g" \
	-e "s/HADOOPHOSTSINCLUDEFILENAME/${hostsincludefilesubst}/g" \
	-e "s/HADOOPHOSTSEXCLUDEFILENAME/${hostsexcludefilesubst}/g" \
	-e "s/DFSDATANODEDIR/${datanodedir}/g" \
	$hdfssitexml > ${HADOOP_CONF_DIR}/hdfs-site.xml
fi

#
# Perform format if necessary
#

if [ "${format}" -eq "1" ]
then
    # Only master will format the node
    if Magpie_am_I_master
    then
	cd $HADOOP_HOME
	echo 'Y' | bin/hadoop namenode -format
    else
	# If this is the first time running, make everyone else wait
	# until the format is complete
	sleep 30
    fi
fi

exit 0
